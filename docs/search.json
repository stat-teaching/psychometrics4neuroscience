[
  {
    "objectID": "slides/05-git-github/05-git-github.html#git-and-github-1",
    "href": "slides/05-git-github/05-git-github.html#git-and-github-1",
    "title": "Git and Github",
    "section": "Git and Github",
    "text": "Git and Github\n\nThe basic idea is to track changes within a folder, assign a message and eventually a tag to a specific version obtaining a version hystory. The version history is completely navigable, you can go back to a previous version of the code.\nThe are advanced features like branches for creating an independent version of the project to test new features and then merge into the main streamline.\nThe entire (local) Git project can be hosted on Github to improve collaboration. Other people or collaborators can clone the repository and push their changes to the project."
  },
  {
    "objectID": "slides/05-git-github/05-git-github.html#veeeery-basic-git-workflow-1",
    "href": "slides/05-git-github/05-git-github.html#veeeery-basic-git-workflow-1",
    "title": "Git and Github",
    "section": "Veeeery basic Git workflow",
    "text": "Veeeery basic Git workflow\nAfter installing Git, you can start a new repository opening a terminal on a folder and typing git init. The folder is now a git project you can notice by the hidden .git folder.\ncd ~/some/folder\ngit init\nThen you can add files to the staging area. Basically these files are ready to be committed i.e.¬†‚Äúwritten‚Äù in the Git history.\ngit add file1.txt\n# git add . # add everyting\nFinally you can commit the modified version of the file using git commit -m message\ngit commit -m \"my first amazing commit\"\nyou can see the Git hystory with all your commits:\ngit log"
  },
  {
    "objectID": "slides/05-git-github/05-git-github.html#github",
    "href": "slides/05-git-github/05-git-github.html#github",
    "title": "Git and Github",
    "section": "Github",
    "text": "Github\nImagine to put everyting into a server with nice viewing options and advanced features. Github is just an hosting service for your git folder.\nYou can create an empty repository on Github named git-test. Now my repo has the path git@github.com:filippogambarota/git-test.git.\ngit remote add origin git@github.com:filippogambarota/git-test.git\ngit push\nNow our local repository is linked with the remote repository. Every time we do git push our local commits will be uploaded.\nIf you worked on the repository from another machine or a colleague add some changes, you can do git pull and your local machine will be updated.\n\nThe repository git-test is online and can be seen here filippogambarota/git-test."
  },
  {
    "objectID": "slides/05-git-github/05-git-github.html#github-1",
    "href": "slides/05-git-github/05-git-github.html#github-1",
    "title": "Git and Github",
    "section": "Github",
    "text": "Github\nAn now let‚Äôs see on Github the result:"
  },
  {
    "objectID": "slides/05-git-github/05-git-github.html#more-about-git-and-github",
    "href": "slides/05-git-github/05-git-github.html#more-about-git-and-github",
    "title": "Git and Github",
    "section": "More about Git and Github",
    "text": "More about Git and Github\nThere are a lot of resources online:\n\nThe Open Science Manual - Zandonella and Massidda - Git and Github chapters.\nhttps://agripongit.vincenttunru.com/\nhttps://git-scm.com/docs/gittutorial"
  },
  {
    "objectID": "slides/05-git-github/05-git-github.html#open-science-framework-1",
    "href": "slides/05-git-github/05-git-github.html#open-science-framework-1",
    "title": "Git and Github",
    "section": "Open Science Framework",
    "text": "Open Science Framework\n\nOSF is a free, open platform to support your research and enable collaboration.\n\nIs a great tool to upload and share materials with others and collaborate on a project. Similarly to Github you can track the changes made to a project.\nThe great addition is having a DOI thus the project is persistently online and can be cited.\nIt is now common practice to create a OSF project supporting a research paper and put the link within the paper containing supplementary materials, raw data, scripts etc."
  },
  {
    "objectID": "slides/05-git-github/05-git-github.html#open-science-framework-2",
    "href": "slides/05-git-github/05-git-github.html#open-science-framework-2",
    "title": "Git and Github",
    "section": "Open Science Framework",
    "text": "Open Science Framework\nIt‚Äôs very easy to create a new project, then you simply need to add files and share it.\n\nThe project can be accessed here (depending on the visibility) https://osf.io/yf9tg/."
  },
  {
    "objectID": "slides/05-git-github/05-git-github.html#open-science-framework-3",
    "href": "slides/05-git-github/05-git-github.html#open-science-framework-3",
    "title": "Git and Github",
    "section": "Open Science Framework",
    "text": "Open Science Framework\nOSF and Github\nAn interesting feature is linking a Github repository to OSF. Now all changes made on Github (easier to manage) are mirrored into OSF. You can easily work in Github for the coding part and use OSF to upload other data or information and to assign a DOI to the project.\nPreprints\nOSF is also linked to a popular service for preprints called PsyArXiv https://psyarxiv.com/ thus you can link a preprint to an OSF project."
  },
  {
    "objectID": "slides/05-git-github/05-git-github.html#more-on-osf",
    "href": "slides/05-git-github/05-git-github.html#more-on-osf",
    "title": "Git and Github",
    "section": "More on OSF",
    "text": "More on OSF\n\nhttps://help.osf.io/article/342-getting-started-on-the-osf\nhttps://arca-dpss.github.io/manual-open-science/osf-chapter.html"
  },
  {
    "objectID": "slides/05-git-github/05-git-github.html#more-on-reproducibility",
    "href": "slides/05-git-github/05-git-github.html#more-on-reproducibility",
    "title": "Git and Github",
    "section": "More on reproducibility",
    "text": "More on reproducibility\nIn general, I highly suggest the online book The Open Science Manual https://arca-dpss.github.io/manual-open-science/ written by my friend Claudio Zandonella and Davide Massidda where these and other topics are explained in details:"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#reproducibility-starter-pack-1",
    "href": "slides/03-modern-r/03-modern-r.html#reproducibility-starter-pack-1",
    "title": "Modern R",
    "section": "Reproducibility starter pack üë∑",
    "text": "Reproducibility starter pack üë∑\n\nA general purpose (or flexible enough) programming language such as   or  \nA literate programming framework to integrate code and text\nA version control system to track projects\nAn online repository for future-proof sharing"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#disclaimers",
    "href": "slides/03-modern-r/03-modern-r.html#disclaimers",
    "title": "Modern R",
    "section": "Disclaimers",
    "text": "Disclaimers\n\nThe best tool is the tool that does the job.\n\n\nBut there are some features that makes a tool better in terms of reproducibility, reducing the probability of errors and improve your coding skills.\nThere is nothing bad about using SPSS, Jasp or Jamovi. The real problem is that using a point-and-click software reduce the reproducibility. If you can use the scripting part, whatever the tool.\nA general suggestion is to invest some of your time learning/improving a programming language for data pre-processing, analysis and reporting (tables, figures, etc.)"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#r",
    "href": "slides/03-modern-r/03-modern-r.html#r",
    "title": "Modern R",
    "section": "R",
    "text": "R\n\nR is a free software environment for statistical computing and graphics.\n\n\n(TBH) It is not a proper general purpose programming language (such as C++ or Python).\nR packages allow to do almost everything (file manager, image processing, webscraping, sending emails, coffee üòÑ, etc.)\nIt is free and open-source\nThe community is wide, active thus solving problems is very easy\nForce you to learn scripting but the are R-based GUI software (e.g., JAMOVI)"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#r---cran",
    "href": "slides/03-modern-r/03-modern-r.html#r---cran",
    "title": "Modern R",
    "section": "R - CRAN",
    "text": "R - CRAN\nThe CRAN is the repository where package developers upload their packages and other users can install them.\n\n\n\n\n\n\n\n\nAs the saying goes: if something exist, there is an R package for doing it! üòÑ"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#r---pypl-index",
    "href": "slides/03-modern-r/03-modern-r.html#r---pypl-index",
    "title": "Modern R",
    "section": "R - PYPL Index",
    "text": "R - PYPL Index\n\nSource: https://pypl.github.io/PYPL.html"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#r---pypl-index-1",
    "href": "slides/03-modern-r/03-modern-r.html#r---pypl-index-1",
    "title": "Modern R",
    "section": "R - PYPL Index",
    "text": "R - PYPL Index\nThe popularity is on a different scale compared to Python but still increasing:\n\n\n\nSource: https://pypl.github.io/PYPL.html"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#r-or-python",
    "href": "slides/03-modern-r/03-modern-r.html#r-or-python",
    "title": "Modern R",
    "section": "R or Python?",
    "text": "R or Python?\n\nPython is a very general-purpose language more powerful for general tasks.\nI find python very useful for programming experiments, image processing, automatizing tasks and interacting with the operating system\nR is still a little bit superior in terms of data manipulation and visualization. Python is faster and more powerful for complex models (e.g., machine learning, etc.)"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#positron",
    "href": "slides/03-modern-r/03-modern-r.html#positron",
    "title": "Modern R",
    "section": "Positron",
    "text": "Positron\nSometimes Python is not so easy to setup. In addition is not as interactive as R (i.e., line by line evaluation). Posit (ex. R Studio) recently created Positron that is a new IDE working with R and Python at the same way."
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#modern-r",
    "href": "slides/03-modern-r/03-modern-r.html#modern-r",
    "title": "Modern R",
    "section": "Modern R",
    "text": "Modern R\n\nFor purist programmers, R is weird: arrays starts with 1, object-oriented programming is hidden, a lot of built-in vectorized functions, etc. The The R Inferno book is really funny showing the strange R-stuff.\nDespite the weirdness, R is widely used because it is intuitive (for non-programmers) and made for statistics and data manipulation\nR is a language and as in spoken languages you can elegant, rude, ambiguous, funny, etc.\nThere are some tips to improve the readability and reproducibility of your code"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#functional-programming",
    "href": "slides/03-modern-r/03-modern-r.html#functional-programming",
    "title": "Modern R",
    "section": "Functional Programming",
    "text": "Functional Programming\n\nIn computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions.\n\n\nDespite R can be used both with an imperative and object-oriented approach, the functional side is quite powerful.\nThe basic idea is to decompose your code into small, testable and re-usable functions"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#functional-programming-example",
    "href": "slides/03-modern-r/03-modern-r.html#functional-programming-example",
    "title": "Modern R",
    "section": "Functional Programming, example‚Ä¶",
    "text": "Functional Programming, example‚Ä¶\nWe have a dataset (mtcars) and we want to calculate the mean, median, standard deviation, minimum and maximum of each column and store the result in a table.\n\nhead(mtcars)\n\n#&gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n#&gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nstr(mtcars)\n\n#&gt; 'data.frame':    32 obs. of  11 variables:\n#&gt;  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n#&gt;  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n#&gt;  $ disp: num  160 160 108 258 360 ...\n#&gt;  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n#&gt;  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n#&gt;  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n#&gt;  $ qsec: num  16.5 17 18.6 19.4 17 ...\n#&gt;  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n#&gt;  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n#&gt;  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n#&gt;  $ carb: num  4 4 1 1 2 1 4 2 2 4 ..."
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#functional-programming-1",
    "href": "slides/03-modern-r/03-modern-r.html#functional-programming-1",
    "title": "Modern R",
    "section": "Functional Programming",
    "text": "Functional Programming\nThe standard (~imperative) option is using a for loop, iterating through columns, calculate the values and store into another data structure.\n\nncols &lt;- ncol(mtcars)\nmeans &lt;- medians &lt;- mins &lt;- maxs &lt;- rep(0, ncols)\n\nfor(i in 1:ncols){\n  means[i] &lt;- mean(mtcars[[i]])\n  medians[i] &lt;- median(mtcars[[i]])\n  mins[i] &lt;- min(mtcars[[i]])\n  maxs[i] &lt;- max(mtcars[[i]])\n}\n\nresults &lt;- data.frame(means, medians, mins, maxs)\nresults$col &lt;- names(mtcars)\n\nresults\n\n#&gt;         means medians   mins    maxs  col\n#&gt; 1   20.090625  19.200 10.400  33.900  mpg\n#&gt; 2    6.187500   6.000  4.000   8.000  cyl\n#&gt; 3  230.721875 196.300 71.100 472.000 disp\n#&gt; 4  146.687500 123.000 52.000 335.000   hp\n#&gt; 5    3.596563   3.695  2.760   4.930 drat\n#&gt; 6    3.217250   3.325  1.513   5.424   wt\n#&gt; 7   17.848750  17.710 14.500  22.900 qsec\n#&gt; 8    0.437500   0.000  0.000   1.000   vs\n#&gt; 9    0.406250   0.000  0.000   1.000   am\n#&gt; 10   3.687500   4.000  3.000   5.000 gear\n#&gt; 11   2.812500   2.000  1.000   8.000 carb"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#functional-programming-2",
    "href": "slides/03-modern-r/03-modern-r.html#functional-programming-2",
    "title": "Modern R",
    "section": "Functional Programming",
    "text": "Functional Programming\nThe main idea is to decompose the problem writing a function and loop over the columns of the dataframe:\n\nsumm &lt;- function(x){\n  data.frame(means = mean(x), \n             medians = median(x), \n             mins = min(x), \n             maxs = max(x))\n}\nncols &lt;- ncol(mtcars)\ndfs &lt;- vector(mode = \"list\", length = ncols)\n\nfor(i in 1:ncols){\n  dfs[[i]] &lt;- summ(mtcars[[i]])\n}"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#functional-programming-3",
    "href": "slides/03-modern-r/03-modern-r.html#functional-programming-3",
    "title": "Modern R",
    "section": "Functional Programming",
    "text": "Functional Programming\n\nresults &lt;- do.call(rbind, dfs)\nresults\n\n#&gt;         means medians   mins    maxs\n#&gt; 1   20.090625  19.200 10.400  33.900\n#&gt; 2    6.187500   6.000  4.000   8.000\n#&gt; 3  230.721875 196.300 71.100 472.000\n#&gt; 4  146.687500 123.000 52.000 335.000\n#&gt; 5    3.596563   3.695  2.760   4.930\n#&gt; 6    3.217250   3.325  1.513   5.424\n#&gt; 7   17.848750  17.710 14.500  22.900\n#&gt; 8    0.437500   0.000  0.000   1.000\n#&gt; 9    0.406250   0.000  0.000   1.000\n#&gt; 10   3.687500   4.000  3.000   5.000\n#&gt; 11   2.812500   2.000  1.000   8.000"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#functional-programming-4",
    "href": "slides/03-modern-r/03-modern-r.html#functional-programming-4",
    "title": "Modern R",
    "section": "Functional Programming",
    "text": "Functional Programming\nThe actual real functional way require using the built-in iteration tools *apply. In this way you avoid writing the verbose for loop.\n\nresults &lt;- lapply(mtcars, summ)\nresults &lt;- do.call(rbind, results)\nresults\n\n#&gt;           means medians   mins    maxs\n#&gt; mpg   20.090625  19.200 10.400  33.900\n#&gt; cyl    6.187500   6.000  4.000   8.000\n#&gt; disp 230.721875 196.300 71.100 472.000\n#&gt; hp   146.687500 123.000 52.000 335.000\n#&gt; drat   3.596563   3.695  2.760   4.930\n#&gt; wt     3.217250   3.325  1.513   5.424\n#&gt; qsec  17.848750  17.710 14.500  22.900\n#&gt; vs     0.437500   0.000  0.000   1.000\n#&gt; am     0.406250   0.000  0.000   1.000\n#&gt; gear   3.687500   4.000  3.000   5.000\n#&gt; carb   2.812500   2.000  1.000   8.000"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#functional-programming-apply",
    "href": "slides/03-modern-r/03-modern-r.html#functional-programming-apply",
    "title": "Modern R",
    "section": "Functional Programming, *apply",
    "text": "Functional Programming, *apply\n\nThe *apply family is one of the best tool in R. The idea is pretty simple: apply a function to each element of a list.\nThe powerful side is that in R everything can be considered as a list. A vector is a list of single elements, a dataframe is a list of columns etc.\nInternally, R is still using a for loop but the verbose part (preallocation, choosing the iterator, indexing) is encapsulated into the *apply function.\n\n\n\nmeans &lt;- rep(0, ncol(mtcars))\nfor(i in 1:length(means)){\n  means[i] &lt;- mean(mtcars[[i]])\n}\n\n# the same with sapply\nmeans &lt;- sapply(mtcars, mean)"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#for-loops-are-bad",
    "href": "slides/03-modern-r/03-modern-r.html#for-loops-are-bad",
    "title": "Modern R",
    "section": "for loops are bad?",
    "text": "for loops are bad?\nfor loops are the core of each operation in R (and in every programming language). For complex operation thery are more readable and effective compared to *apply. In R we need extra care for writing efficent for loops.\nExtremely slow, no preallocation:\n\nres &lt;- c()\nfor(i in 1:1000){\n  # do something\n  res[i] &lt;- x\n}\n\nVery fast, no difference compared to *apply\n\nres &lt;- rep(0, 1000)\nfor(i in 1:length(res)){\n  # do something\n  res[i] &lt;- x\n}"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#for-loops-are-bad-1",
    "href": "slides/03-modern-r/03-modern-r.html#for-loops-are-bad-1",
    "title": "Modern R",
    "section": "for loops are bad?",
    "text": "for loops are bad?\nWe can formally compare the for loop approaches using the microbenchmark package:\n\nno_prealloc &lt;- function(n = 100){\n    res &lt;- c()\n    for(i in 1:n) res[i] &lt;- rnorm(1)\n}\n\nprealloc &lt;- function(n = 100){\n    res &lt;- vector(mode = \"numeric\", length = n)\n    for(i in 1:n) res[i] &lt;- rnorm(1)\n}\n\nmicrobenchmark::microbenchmark(\n    no_prealloc = no_prealloc(1000),\n    prealloc = prealloc(1000)\n)\n\n#&gt; Unit: microseconds\n#&gt;         expr     min       lq      mean   median       uq      max neval cld\n#&gt;  no_prealloc 848.225 872.0795 1010.1748 884.2175 918.5970 4462.520   100  a \n#&gt;     prealloc 648.589 663.1315  711.0051 668.4970 681.6215 2821.114   100   b"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#for-loops-are-bad-2",
    "href": "slides/03-modern-r/03-modern-r.html#for-loops-are-bad-2",
    "title": "Modern R",
    "section": "for loops are bad?",
    "text": "for loops are bad?\nIn fact, when the for loop is written appropriately, the performance are the same (or even better) compared to *apply:\n\nlibrary(purrr) # for map\niter &lt;- 500\n\nmicrobenchmark::microbenchmark(\n    for_no_prealloc = no_prealloc(iter),\n    for_prealloc = prealloc(iter),\n    sapply = sapply(1:iter, function(x) rnorm(1)),\n    map_dbl = map_dbl(1:iter, function(x) rnorm(1)),\n    vapply = vapply(1:iter, function(x) rnorm(1), FUN.VALUE = double(1)),\n    times = 500\n) |&gt; summary()\n\n#&gt;              expr     min       lq     mean   median       uq      max neval\n#&gt; 1 for_no_prealloc 427.404 444.7960 479.5039 452.2905 463.0960 4068.079   500\n#&gt; 2    for_prealloc 321.355 330.5470 347.4882 334.3735 339.7890 4380.626   500\n#&gt; 3          sapply 588.496 607.2065 652.3096 618.3775 633.7065 5153.349   500\n#&gt; 4         map_dbl 610.588 630.8610 697.8509 645.0725 679.1320 4740.082   500\n#&gt; 5          vapply 579.730 591.2970 635.9248 598.4200 607.1715 4754.439   500\n#&gt;    cld\n#&gt; 1 a   \n#&gt; 2  b  \n#&gt; 3   cd\n#&gt; 4   c \n#&gt; 5    d"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#with-apply-you-can-do-crazy-stuff",
    "href": "slides/03-modern-r/03-modern-r.html#with-apply-you-can-do-crazy-stuff",
    "title": "Modern R",
    "section": "With *apply you can do crazy stuff!",
    "text": "With *apply you can do crazy stuff!\n\nfuns &lt;- list(mean = mean, sd = sd, min = min, max = max, median = median)\nsapply(funs, function(f) lapply(mtcars, function(x) f(x)))\n\n#&gt;      mean     sd        min   max   median\n#&gt; mpg  20.09062 6.026948  10.4  33.9  19.2  \n#&gt; cyl  6.1875   1.785922  4     8     6     \n#&gt; disp 230.7219 123.9387  71.1  472   196.3 \n#&gt; hp   146.6875 68.56287  52    335   123   \n#&gt; drat 3.596563 0.5346787 2.76  4.93  3.695 \n#&gt; wt   3.21725  0.9784574 1.513 5.424 3.325 \n#&gt; qsec 17.84875 1.786943  14.5  22.9  17.71 \n#&gt; vs   0.4375   0.5040161 0     1     0     \n#&gt; am   0.40625  0.4989909 0     1     0     \n#&gt; gear 3.6875   0.7378041 3     5     4     \n#&gt; carb 2.8125   1.6152    1     8     2"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#why-functional-programming",
    "href": "slides/03-modern-r/03-modern-r.html#why-functional-programming",
    "title": "Modern R",
    "section": "Why functional programming?",
    "text": "Why functional programming?\n\nWe can write less and reusable code that can be shared and used in multiple projects\nThe scripts are more compact, easy to modify and less error prone (imagine that you want to improve the summ function, you only need to change it once instead of touching the for loop)\nFunctions can be easily and consistently documented (see roxygen documentation) improving the reproducibility and readability of your code"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#more-about-functional-programming-in-r",
    "href": "slides/03-modern-r/03-modern-r.html#more-about-functional-programming-in-r",
    "title": "Modern R",
    "section": "More about functional programming in R",
    "text": "More about functional programming in R\n\nAdvanced R by Hadley Wickham, section on Functional Programming (https://adv-r.hadley.nz/fp.html)\nHands-On Programming with R by Garrett Grolemund https://rstudio-education.github.io/hopr/\nHadley Wickham: The Joy of Functional Programming (for Data Science)\nBruno Rodrigues Youtube Channel"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#a-small-example-1",
    "href": "slides/03-modern-r/03-modern-r.html#a-small-example-1",
    "title": "Modern R",
    "section": "A small example",
    "text": "A small example\nTake the dataset iris and do the following operations in the most readable and efficient way that you can.\n\nfit a linear model (choose the y and x that you want) for each Species on the full dataset\nfit a linear model (choose the y and x that you want) for each Species but resampling with replacement (bootstrapping, choose the number of iterations that you want) the rows within each group\n(choose the y and x that you want) for each Species doing a leave-one-out analysis within each group\norganize the three steps into separated datasets in a nice and readable format (nice column names, not strange characters, spaces, etc.)\nshow the results with a plot of your choice"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#a-more-advanced-approach-r-packages",
    "href": "slides/03-modern-r/03-modern-r.html#a-more-advanced-approach-r-packages",
    "title": "Modern R",
    "section": "A more advanced approach, R packages",
    "text": "A more advanced approach, R packages\nR packages are not only on CRAN. You can (pretty) easily create a package and put it on Github. For example, if you keep using some functions in your project, write a general version and put them into a package.\n\ngithub.com/filippogambarota/filor"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#a-more-advanced-approach-r-packages-1",
    "href": "slides/03-modern-r/03-modern-r.html#a-more-advanced-approach-r-packages-1",
    "title": "Modern R",
    "section": "A more advanced approach, R packages",
    "text": "A more advanced approach, R packages\nIf your functions are project-specific you can define them into your scripts or write some R scripts only with functions and source() them into the global environment.\nproject/\n‚îú‚îÄ R/\n‚îÇ  ‚îú‚îÄ utils.R\n‚îú‚îÄ analysis.R\nAnd inside utils.R you have some functions:\n\nmyfun &lt;- function(x) {\n  # something\n}\n\nThen you can load the function using source(\"R/utils.R) at the beginning of analysis.R:\n\nsource(\"R/utils.R\")"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#analysis-project-as-r-package",
    "href": "slides/03-modern-r/03-modern-r.html#analysis-project-as-r-package",
    "title": "Modern R",
    "section": "Analysis project as R package",
    "text": "Analysis project as R package\nThe R project structure is really interesting to organize a data analysis pipeline. In fact, you can use the project structure. Vuorre and Crump (2021) and Marwick, Boettiger, and Mullen (2018) describe in details the idea.\nThe general approach is:\n\nCreate an R Studio project .Rproj file\nCreate your directories, put scripts, data, etc.\nCreate an R/ folder and put your scripts with functions\nCreate a DESCRIPTION file using usethis::use_description(check_name = FALSE)\nThen you can load your functions without source and with devtools::load_all() (same as library())"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#the-tidy-approach",
    "href": "slides/03-modern-r/03-modern-r.html#the-tidy-approach",
    "title": "Modern R",
    "section": "The Tidy approach",
    "text": "The Tidy approach\nThe tidyverse is a series of high-quality R packages to do modern data science:\n\n\ndata manipulation (dplyr, tidyr)\nplotting (ggplot2)\nreporting (rmarkdown)\nstring manipulation (stringr)\nfunctionals (purrr)\n‚Ä¶"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#the-tidy-approach---pipes",
    "href": "slides/03-modern-r/03-modern-r.html#the-tidy-approach---pipes",
    "title": "Modern R",
    "section": "The Tidy approach - Pipes",
    "text": "The Tidy approach - Pipes\nOne of the great improvement from the tidyverse is the usage of the pipe %&gt;% now introduced in base R as |&gt;. You will se these symbols a lot when looking at modern R code.\n\nThe idea is very simple, the standard pattern to apply a function is function(argument). The pipe can reverse the pattern as argument |&gt; function(). Normally when we apply multiple functions progressively the pattern is this:\n\n\n\nx &lt;- rnorm(100)\nx &lt;- round(x, 3)\nx &lt;- abs(x)\nx &lt;- as.character(x)"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#the-tidy-approach---pipes-1",
    "href": "slides/03-modern-r/03-modern-r.html#the-tidy-approach---pipes-1",
    "title": "Modern R",
    "section": "The Tidy approach - Pipes",
    "text": "The Tidy approach - Pipes\nWhen using the pipe, we remove the redundand assignment &lt;- pattern:\n\nx &lt;- rnorm(100)\nx |&gt;\n  round(3) |&gt;\n  abs() |&gt;\n  as.character()\n\nThe pipe can be read as ‚Äúfrom x apply round, then abs, etc.‚Äù. The first argument of the piped function is assumed to be the result of the previus call."
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#more-about-the-tidy-approach",
    "href": "slides/03-modern-r/03-modern-r.html#more-about-the-tidy-approach",
    "title": "Modern R",
    "section": "More about the Tidy approach",
    "text": "More about the Tidy approach\nThe tidy approach contains tons of functions and packages. The overall philosophy can be deepen in the R for Data Science book.\n\nhttps://r4ds.hadley.nz/"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#ggplot2",
    "href": "slides/03-modern-r/03-modern-r.html#ggplot2",
    "title": "Modern R",
    "section": "ggplot2",
    "text": "ggplot2\nOnly an quick mention to ggplot2 https://ggplot2-book.org/ (part of the tidyverse) that is an amazing package for data visualization following the piping and tidy approach. Is the implementation of the grammar of graphics idea.\n\nlibrary(tidyverse)\n\niris |&gt;\n  mutate(wi = runif(n())) |&gt;\n  ggplot(aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point(aes(size = wi)) +\n  geom_smooth(method = \"lm\", se = FALSE)\n  guides(size = \"none\") +\n  theme_minimal(15)"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#ggplot2-1",
    "href": "slides/03-modern-r/03-modern-r.html#ggplot2-1",
    "title": "Modern R",
    "section": "ggplot2",
    "text": "ggplot2"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#base-r-version",
    "href": "slides/03-modern-r/03-modern-r.html#base-r-version",
    "title": "Modern R",
    "section": "Base R version",
    "text": "Base R version\nMore verbose, more hard coding, more steps and intermediate objects.\n\niris_l &lt;- split(iris, iris$Species)\nlms &lt;- lapply(iris_l, function(x) lm(Petal.Width ~ Sepal.Length, data = x))\n\nplot(iris$Sepal.Length, \n     iris$Petal.Width, \n     col = as.numeric(iris$Species), pch = 19)\n\nabline(lms[[1]], col = 1, lwd = 2)\nabline(lms[[2]], col = 2, lwd = 2)\nabline(lms[[3]], col = 3, lwd = 2)\n\nlegend(\"topleft\", legend = levels(iris$Species), fill = 1:3)"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#base-r-version-1",
    "href": "slides/03-modern-r/03-modern-r.html#base-r-version-1",
    "title": "Modern R",
    "section": "Base R version",
    "text": "Base R version"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#more-on-ggplot2",
    "href": "slides/03-modern-r/03-modern-r.html#more-on-ggplot2",
    "title": "Modern R",
    "section": "More on ggplot2",
    "text": "More on ggplot2\nThe ggplot2 book https://ggplot2-book.org/ is a great resource to produce high-quality, publication ready plots. Clearly, the advantage of producing the figures entirely writing code are immense in terms of reusability and reproducibility."
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse",
    "href": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse",
    "title": "Modern R",
    "section": "Something crazy in the tidyverse",
    "text": "Something crazy in the tidyverse\nWithout going into details, I want to show you a very interesting approach that you can do with the tidyverse functions.\nLet‚Äôs assume you want to do a leave-one-out analysis thus fitting the same models on a dataset, removing one observation at time.\nYou can do it in base R with a loop or other methods, but the see so-called many-models approach. See https://r4ds.had.co.nz/many-models.html and https://www.youtube.com/watch?v=rz3_FDVt9eg."
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse-1",
    "href": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse-1",
    "title": "Modern R",
    "section": "Something crazy in the tidyverse",
    "text": "Something crazy in the tidyverse\nLet‚Äôs define some functions:\n\nleave1out &lt;- function(data){\n  idx &lt;- 1:nrow(data)\n  ll &lt;- lapply(idx, function(i) data[-i, ])\n  names(ll) &lt;- paste0(\"no\", idx)\n  c(no0 = list(data), ll)\n}\n\nfit_model &lt;- function(data){\n  lm(Sepal.Length ~ Petal.Width, data = data)\n}"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse-2",
    "href": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse-2",
    "title": "Modern R",
    "section": "Something crazy in the tidyverse",
    "text": "Something crazy in the tidyverse\n\ndat &lt;- tibble(data = leave1out(iris[1:20, ]))\ndat |&gt; \n  mutate(removed = names(data)) |&gt; \n  head()\n\n#&gt; # A tibble: 6 √ó 2\n#&gt;   data          removed\n#&gt;   &lt;named list&gt;  &lt;chr&gt;  \n#&gt; 1 &lt;df [20 √ó 5]&gt; no0    \n#&gt; 2 &lt;df [19 √ó 5]&gt; no1    \n#&gt; 3 &lt;df [19 √ó 5]&gt; no2    \n#&gt; 4 &lt;df [19 √ó 5]&gt; no3    \n#&gt; 5 &lt;df [19 √ó 5]&gt; no4    \n#&gt; 6 &lt;df [19 √ó 5]&gt; no5"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse-3",
    "href": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse-3",
    "title": "Modern R",
    "section": "Something crazy in the tidyverse",
    "text": "Something crazy in the tidyverse\n\n\n#&gt; # A tibble: 6 √ó 4\n#&gt;   data          removed fit          results         \n#&gt;   &lt;named list&gt;  &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;    \n#&gt; 1 &lt;df [20 √ó 5]&gt; no0     &lt;lm&gt;         &lt;tibble [2 √ó 5]&gt;\n#&gt; 2 &lt;df [19 √ó 5]&gt; no1     &lt;lm&gt;         &lt;tibble [2 √ó 5]&gt;\n#&gt; 3 &lt;df [19 √ó 5]&gt; no2     &lt;lm&gt;         &lt;tibble [2 √ó 5]&gt;\n#&gt; 4 &lt;df [19 √ó 5]&gt; no3     &lt;lm&gt;         &lt;tibble [2 √ó 5]&gt;\n#&gt; 5 &lt;df [19 √ó 5]&gt; no4     &lt;lm&gt;         &lt;tibble [2 √ó 5]&gt;\n#&gt; 6 &lt;df [19 √ó 5]&gt; no5     &lt;lm&gt;         &lt;tibble [2 √ó 5]&gt;"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse-4",
    "href": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse-4",
    "title": "Modern R",
    "section": "Something crazy in the tidyverse",
    "text": "Something crazy in the tidyverse\n\ndat |&gt; \n  mutate(removed = names(data)) |&gt; \n  mutate(fit = map(data, fit_model),\n         results = map(fit, broom::tidy)) |&gt; \n  unnest(results) |&gt; \n  ggplot(aes(x = removed, y = estimate)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~term, scales = \"free\")"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse-5",
    "href": "slides/03-modern-r/03-modern-r.html#something-crazy-in-the-tidyverse-5",
    "title": "Modern R",
    "section": "Something crazy in the tidyverse",
    "text": "Something crazy in the tidyverse"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#quick-tables",
    "href": "slides/03-modern-r/03-modern-r.html#quick-tables",
    "title": "Modern R",
    "section": "Quick tables",
    "text": "Quick tables\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1501\n\n\n\n\nSepal.Length\n5.80 (5.10, 6.40)\n\n\nSepal.Width\n3.00 (2.80, 3.30)\n\n\nPetal.Length\n4.35 (1.60, 5.10)\n\n\nPetal.Width\n1.30 (0.30, 1.80)\n\n\nSpecies\n\n\n\n\n¬†¬†¬†¬†setosa\n50 (33%)\n\n\n¬†¬†¬†¬†versicolor\n50 (33%)\n\n\n¬†¬†¬†¬†virginica\n50 (33%)\n\n\n\n1 Median (IQR); n (%)"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#quick-tables-from-models",
    "href": "slides/03-modern-r/03-modern-r.html#quick-tables-from-models",
    "title": "Modern R",
    "section": "Quick tables from models",
    "text": "Quick tables from models\n\nfit &lt;- lm(Sepal.Length ~ Petal.Width, data = iris)\nsjPlot::tab_model(fit)\n\n\n\n\n¬†\nSepal.Length\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n4.78\n4.63¬†‚Äì¬†4.92\n&lt;0.001\n\n\nPetal Width\n0.89\n0.79¬†‚Äì¬†0.99\n&lt;0.001\n\n\nObservations\n150\n\n\nR2 / R2 adjusted\n0.669 / 0.667"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#quick-tables-from-models-1",
    "href": "slides/03-modern-r/03-modern-r.html#quick-tables-from-models-1",
    "title": "Modern R",
    "section": "Quick tables from models",
    "text": "Quick tables from models\n\ngtsummary::tbl_regression(fit)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nPetal.Width\n0.89\n0.79, 0.99\n&lt;0.001\n\n\n\n1 CI = Confidence Interval"
  },
  {
    "objectID": "slides/03-modern-r/03-modern-r.html#references",
    "href": "slides/03-modern-r/03-modern-r.html#references",
    "title": "Modern R",
    "section": "References",
    "text": "References\n\n\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. ‚ÄúPackaging Data Analytical Work Reproducibly Using r (and Friends).‚Äù The American Statistician 72 (January): 80‚Äì88. https://doi.org/10.1080/00031305.2017.1375986.\n\n\nVuorre, Matti, and Matthew J C Crump. 2021. ‚ÄúSharing and Organizing Research Products as r Packages.‚Äù Behavior Research Methods 53 (April): 792‚Äì802. https://doi.org/10.3758/s13428-020-01436-x."
  },
  {
    "objectID": "slides/00-course-intro/00-course-intro.html#about-me",
    "href": "slides/00-course-intro/00-course-intro.html#about-me",
    "title": "Introduzione al corso",
    "section": "About me",
    "text": "About me\n\nSono uno Psicologo Clinico e assegnista di ricerca in Psicometria al Dipartimento di Psicologia dello Sviluppo e della Socializzazione\nMi occupo di analisi dei dati in Psicologia, in particolare: meta-analysis, simulazioni monte carlo per la power analysis, replicabilit√† e multiverse analysis"
  },
  {
    "objectID": "slides/00-course-intro/00-course-intro.html#ricevimento",
    "href": "slides/00-course-intro/00-course-intro.html#ricevimento",
    "title": "Introduzione al corso",
    "section": "Ricevimento",
    "text": "Ricevimento\n\nLavoro al Dipartimento di Psicologia dello Sviluppo e della Socializzazione, Via Venezia 16, 35131 (edificio CLA). Il mio ufficio √® 05-025 (5¬∞ piano).\nIl mio orario di ricevimento √® Venerd√¨ dalle 10:00 alle 12:00. Possiamo vederci sia in presenza che su Zoom. Potete prenotarvi qui https://calendar.app.google/Go7B6GBKBeRPCpU27.\nPer ogni cosa potete scrivermi a filippo.gambarota@unipd.it"
  },
  {
    "objectID": "slides/00-course-intro/00-course-intro.html#argomenti",
    "href": "slides/00-course-intro/00-course-intro.html#argomenti",
    "title": "Introduzione al corso",
    "section": "Argomenti",
    "text": "Argomenti\n\nOpen science tools: R, Quarto, Git e Github\nReplicabilit√† e riproducibilit√† nelle neuroscienze cognitive\nMeta-analysis\nGeneralized Linear Mixed-Effects models\nSimulazioni Monte Carlo"
  },
  {
    "objectID": "slides/00-course-intro/00-course-intro.html#organizzazione-delle-lezioni",
    "href": "slides/00-course-intro/00-course-intro.html#organizzazione-delle-lezioni",
    "title": "Introduzione al corso",
    "section": "Organizzazione delle lezioni",
    "text": "Organizzazione delle lezioni\nCosa faremo?\n\nPresentazione dei concetti teorici principali\nEsempi con dati reali e simulati\nIntepretazione dei risultati in ottica applicativa\nTanto codice\n\nCosa non faremo?\n\ndimostrazioni\nmolto focus sulle formule"
  },
  {
    "objectID": "slides/00-course-intro/00-course-intro.html#materiale",
    "href": "slides/00-course-intro/00-course-intro.html#materiale",
    "title": "Introduzione al corso",
    "section": "Materiale",
    "text": "Materiale\nPer il materiale, trovate tutto su Github https://github.com/stat-teaching/psychometrics4neuroscience. Trovate i vari link anche su Moodle ma Github √® molto pi√π comodo per tenere e organizzare tutto (lo vedremo nel dettaglio).\nLa repository contiene il codice, le slide (sia in formato .qmd che compilate), i dataset e tutto il resto del materiale."
  },
  {
    "objectID": "slides/00-course-intro/00-course-intro.html#esami",
    "href": "slides/00-course-intro/00-course-intro.html#esami",
    "title": "Introduzione al corso",
    "section": "Esami",
    "text": "Esami\nFate sempre riferimento alla pagina ufficiale https://www.stat.unipd.it/studenti-iscritti/calendario-appelli-desame.\n\n16/06/2025 aula SC60 10:30-12:30\n07/07/2025 aula SC60 10:30-12:30\n01/09/2025 aula SC60 14:30-16:30"
  },
  {
    "objectID": "slides/00-course-intro/00-course-intro.html#lavori-di-gruppo",
    "href": "slides/00-course-intro/00-course-intro.html#lavori-di-gruppo",
    "title": "Introduzione al corso",
    "section": "Lavori di gruppo",
    "text": "Lavori di gruppo\nParleremo dei lavori di gruppo assieme al Prof.¬†Maffei il 28/04/2025 (salvo modifiche) dove vi presenteremo i lavori nel dettaglio.\nIn breve, vi sar√† richiesto partendo da un dataset e/o un problema di ricerca, analizzare e/o simulare dei dati, produrre un report in Quarto e fare una presentazione del lavoro fatto."
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#data-and-packages",
    "href": "slides-gpt/06-regular-expressions.html#data-and-packages",
    "title": "15 - Regex",
    "section": "Data and packages",
    "text": "Data and packages\n\nlibrary(tidyverse)\nlibrary(babynames)"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#data-and-packages-1",
    "href": "slides-gpt/06-regular-expressions.html#data-and-packages-1",
    "title": "15 - Regex",
    "section": "Data and packages",
    "text": "Data and packages\n\nfruit contains the names of 80 fruits.\nwords contains 980 common English words.\nsentences contains 720 short sentences."
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#pattern-basics",
    "href": "slides-gpt/06-regular-expressions.html#pattern-basics",
    "title": "15 - Regex",
    "section": "Pattern basics",
    "text": "Pattern basics\n\nstr_view(fruit, \"berry\")\n\n [6] ‚îÇ bil&lt;berry&gt;\n [7] ‚îÇ black&lt;berry&gt;\n[10] ‚îÇ blue&lt;berry&gt;\n[11] ‚îÇ boysen&lt;berry&gt;\n[19] ‚îÇ cloud&lt;berry&gt;\n[21] ‚îÇ cran&lt;berry&gt;\n[29] ‚îÇ elder&lt;berry&gt;\n[32] ‚îÇ goji &lt;berry&gt;\n[33] ‚îÇ goose&lt;berry&gt;\n[38] ‚îÇ huckle&lt;berry&gt;\n[50] ‚îÇ mul&lt;berry&gt;\n[70] ‚îÇ rasp&lt;berry&gt;\n[73] ‚îÇ salal &lt;berry&gt;\n[76] ‚îÇ straw&lt;berry&gt;\n\n\n\nstr_view(c(\"a\", \"ab\", \"ae\", \"bd\", \"ea\", \"eab\"), \"a.\")\n\n[2] ‚îÇ &lt;ab&gt;\n[3] ‚îÇ &lt;ae&gt;\n[6] ‚îÇ e&lt;ab&gt;\n\n\n\nstr_view(fruit, \"a...e\")\n\n [1] ‚îÇ &lt;apple&gt;\n [7] ‚îÇ bl&lt;ackbe&gt;rry\n[48] ‚îÇ mand&lt;arine&gt;\n[51] ‚îÇ nect&lt;arine&gt;\n[62] ‚îÇ pine&lt;apple&gt;\n[64] ‚îÇ pomegr&lt;anate&gt;\n[70] ‚îÇ r&lt;aspbe&gt;rry\n[73] ‚îÇ sal&lt;al be&gt;rry"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#pattern-basics-1",
    "href": "slides-gpt/06-regular-expressions.html#pattern-basics-1",
    "title": "15 - Regex",
    "section": "Pattern basics",
    "text": "Pattern basics\nQuantifiers control how many times a pattern can match:\n\n? makes a pattern optional (i.e.¬†it matches 0 or 1 times)\n+ lets a pattern repeat (i.e.¬†it matches at least once)\n* lets a pattern be optional or repeat (i.e.¬†it matches any number of times, including 0).\n\n\n# ab? matches an \"a\", optionally followed by a \"b\".\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab?\")\n\n[1] ‚îÇ &lt;a&gt;\n[2] ‚îÇ &lt;ab&gt;\n[3] ‚îÇ &lt;ab&gt;b\n\n# ab+ matches an \"a\", followed by at least one \"b\".\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab+\")\n\n[2] ‚îÇ &lt;ab&gt;\n[3] ‚îÇ &lt;abb&gt;\n\n# ab* matches an \"a\", followed by any number of \"b\"s.\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab*\")\n\n[1] ‚îÇ &lt;a&gt;\n[2] ‚îÇ &lt;ab&gt;\n[3] ‚îÇ &lt;abb&gt;"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#pattern-basics-2",
    "href": "slides-gpt/06-regular-expressions.html#pattern-basics-2",
    "title": "15 - Regex",
    "section": "Pattern basics",
    "text": "Pattern basics\n\nstr_view(words, \"[aeiou]x[aeiou]\")\n\n[284] ‚îÇ &lt;exa&gt;ct\n[285] ‚îÇ &lt;exa&gt;mple\n[288] ‚îÇ &lt;exe&gt;rcise\n[289] ‚îÇ &lt;exi&gt;st\n\nstr_view(words, \"[^aeiou]y[^aeiou]\")\n\n[836] ‚îÇ &lt;sys&gt;tem\n[901] ‚îÇ &lt;typ&gt;e"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#pattern-basics-3",
    "href": "slides-gpt/06-regular-expressions.html#pattern-basics-3",
    "title": "15 - Regex",
    "section": "Pattern basics",
    "text": "Pattern basics\n\nstr_view(fruit, \"apple|melon|nut\")\n\n [1] ‚îÇ &lt;apple&gt;\n[13] ‚îÇ canary &lt;melon&gt;\n[20] ‚îÇ coco&lt;nut&gt;\n[52] ‚îÇ &lt;nut&gt;\n[62] ‚îÇ pine&lt;apple&gt;\n[72] ‚îÇ rock &lt;melon&gt;\n[80] ‚îÇ water&lt;melon&gt;\n\nstr_view(fruit, \"aa|ee|ii|oo|uu\")\n\n [9] ‚îÇ bl&lt;oo&gt;d orange\n[33] ‚îÇ g&lt;oo&gt;seberry\n[47] ‚îÇ lych&lt;ee&gt;\n[66] ‚îÇ purple mangost&lt;ee&gt;n"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#detect-matches",
    "href": "slides-gpt/06-regular-expressions.html#detect-matches",
    "title": "15 - Regex",
    "section": "Detect matches",
    "text": "Detect matches\n\nstr_detect(c(\"a\", \"b\", \"c\"), \"[aeiou]\")\n\n[1]  TRUE FALSE FALSE"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#detect-matches-1",
    "href": "slides-gpt/06-regular-expressions.html#detect-matches-1",
    "title": "15 - Regex",
    "section": "Detect matches",
    "text": "Detect matches\n\nbabynames |&gt; \n  filter(str_detect(name, \"x\")) |&gt; \n  count(name, wt = n, sort = TRUE)\n\n# A tibble: 974 √ó 2\n   name            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 Alexander  665492\n 2 Alexis     399551\n 3 Alex       278705\n 4 Alexandra  232223\n 5 Max        148787\n 6 Alexa      123032\n 7 Maxine     112261\n 8 Alexandria  97679\n 9 Maxwell     90486\n10 Jaxon       71234\n# ‚Ñπ 964 more rows"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#detect-matches-2",
    "href": "slides-gpt/06-regular-expressions.html#detect-matches-2",
    "title": "15 - Regex",
    "section": "Detect matches",
    "text": "Detect matches\n\nbabynames |&gt; \n  group_by(year) |&gt; \n  summarize(prop_x = mean(str_detect(name, \"x\")))\n\n# A tibble: 138 √ó 2\n    year  prop_x\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1880 0.0065 \n 2  1881 0.00879\n 3  1882 0.00940\n 4  1883 0.00768\n 5  1884 0.00827\n 6  1885 0.00872\n 7  1886 0.00878\n 8  1887 0.00801\n 9  1888 0.00905\n10  1889 0.00888\n# ‚Ñπ 128 more rows"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#count-matches",
    "href": "slides-gpt/06-regular-expressions.html#count-matches",
    "title": "15 - Regex",
    "section": "Count matches",
    "text": "Count matches\n\nx &lt;- c(\"apple\", \"banana\", \"pear\")\nstr_count(x, \"p\")\n\n[1] 2 0 1"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#count-matches-1",
    "href": "slides-gpt/06-regular-expressions.html#count-matches-1",
    "title": "15 - Regex",
    "section": "Count matches",
    "text": "Count matches\n\nstr_count(\"abababa\", \"aba\")\n\n[1] 2\n\nstr_view(\"abababa\", \"aba\")\n\n[1] ‚îÇ &lt;aba&gt;b&lt;aba&gt;"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#count-matches-2",
    "href": "slides-gpt/06-regular-expressions.html#count-matches-2",
    "title": "15 - Regex",
    "section": "Count matches",
    "text": "Count matches\n\nbabynames |&gt; \n  count(name) |&gt; \n  mutate(\n    vowels = str_count(name, \"[aeiou]\"),\n    consonants = str_count(name, \"[^aeiou]\")\n  )\n\n# A tibble: 97,310 √ó 4\n   name          n vowels consonants\n   &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt;\n 1 Aaban        10      2          3\n 2 Aabha         5      2          3\n 3 Aabid         2      2          3\n 4 Aabir         1      2          3\n 5 Aabriella     5      4          5\n 6 Aada          1      2          2\n 7 Aadam        26      2          3\n 8 Aadan        11      2          3\n 9 Aadarsh      17      2          5\n10 Aaden        18      2          3\n# ‚Ñπ 97,300 more rows"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#count-matches-3",
    "href": "slides-gpt/06-regular-expressions.html#count-matches-3",
    "title": "15 - Regex",
    "section": "Count matches",
    "text": "Count matches\n\nbabynames |&gt; \n  count(name) |&gt; \n  mutate(\n    name = str_to_lower(name),\n    vowels = str_count(name, \"[aeiou]\"),\n    consonants = str_count(name, \"[^aeiou]\")\n  )\n\n# A tibble: 97,310 √ó 4\n   name          n vowels consonants\n   &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt;\n 1 aaban        10      3          2\n 2 aabha         5      3          2\n 3 aabid         2      3          2\n 4 aabir         1      3          2\n 5 aabriella     5      5          4\n 6 aada          1      3          1\n 7 aadam        26      3          2\n 8 aadan        11      3          2\n 9 aadarsh      17      3          4\n10 aaden        18      3          2\n# ‚Ñπ 97,300 more rows"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#replace-values",
    "href": "slides-gpt/06-regular-expressions.html#replace-values",
    "title": "15 - Regex",
    "section": "Replace values",
    "text": "Replace values\n\nx &lt;- c(\"apple\", \"pear\", \"banana\")\nstr_replace_all(x, \"[aeiou]\", \"-\")\n\n[1] \"-ppl-\"  \"p--r\"   \"b-n-n-\""
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#replace-values-1",
    "href": "slides-gpt/06-regular-expressions.html#replace-values-1",
    "title": "15 - Regex",
    "section": "Replace values",
    "text": "Replace values\n\nx &lt;- c(\"apple\", \"pear\", \"banana\")\nstr_remove_all(x, \"[aeiou]\")\n\n[1] \"ppl\" \"pr\"  \"bnn\""
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#anchors",
    "href": "slides-gpt/06-regular-expressions.html#anchors",
    "title": "15 - Regex",
    "section": "Anchors",
    "text": "Anchors\n\nstr_view(fruit, \"^a\")\n\n[1] ‚îÇ &lt;a&gt;pple\n[2] ‚îÇ &lt;a&gt;pricot\n[3] ‚îÇ &lt;a&gt;vocado\n\nstr_view(fruit, \"a$\")\n\n [4] ‚îÇ banan&lt;a&gt;\n[15] ‚îÇ cherimoy&lt;a&gt;\n[30] ‚îÇ feijo&lt;a&gt;\n[36] ‚îÇ guav&lt;a&gt;\n[56] ‚îÇ papay&lt;a&gt;\n[74] ‚îÇ satsum&lt;a&gt;"
  },
  {
    "objectID": "slides-gpt/06-regular-expressions.html#anchors-1",
    "href": "slides-gpt/06-regular-expressions.html#anchors-1",
    "title": "15 - Regex",
    "section": "Anchors",
    "text": "Anchors\n\nstr_view(fruit, \"apple\")\n\n [1] ‚îÇ &lt;apple&gt;\n[62] ‚îÇ pine&lt;apple&gt;\n\nstr_view(fruit, \"^apple$\")\n\n[1] ‚îÇ &lt;apple&gt;"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#introduction",
    "href": "slides-gpt/03-data-transformation.html#introduction",
    "title": "03 - Data Transformation",
    "section": "Introduction",
    "text": "Introduction\n\nlibrary(dplyr)\nlibrary(nycflights13)\nflights &lt;- nycflights13::flights"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#selecting-columns",
    "href": "slides-gpt/03-data-transformation.html#selecting-columns",
    "title": "03 - Data Transformation",
    "section": "Selecting Columns",
    "text": "Selecting Columns\n\nflights %&gt;% select(year, month, day, dep_delay, arr_delay)\n\n# A tibble: 336,776 √ó 5\n    year month   day dep_delay arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  2013     1     1         2        11\n 2  2013     1     1         4        20\n 3  2013     1     1         2        33\n 4  2013     1     1        -1       -18\n 5  2013     1     1        -6       -25\n 6  2013     1     1        -4        12\n 7  2013     1     1        -5        19\n 8  2013     1     1        -3       -14\n 9  2013     1     1        -3        -8\n10  2013     1     1        -2         8\n# ‚Ñπ 336,766 more rows"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#filtering-rows",
    "href": "slides-gpt/03-data-transformation.html#filtering-rows",
    "title": "03 - Data Transformation",
    "section": "Filtering Rows",
    "text": "Filtering Rows\n\nflights %&gt;% filter(month == 1, day == 1)\n\n# A tibble: 842 √ó 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ‚Ñπ 832 more rows\n# ‚Ñπ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#arranging-rows",
    "href": "slides-gpt/03-data-transformation.html#arranging-rows",
    "title": "03 - Data Transformation",
    "section": "Arranging Rows",
    "text": "Arranging Rows\n\nflights %&gt;% arrange(desc(dep_delay))\n\n# A tibble: 336,776 √ó 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ‚Ñπ 336,766 more rows\n# ‚Ñπ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#creating-new-variables",
    "href": "slides-gpt/03-data-transformation.html#creating-new-variables",
    "title": "03 - Data Transformation",
    "section": "Creating New Variables",
    "text": "Creating New Variables\n\nflights %&gt;% mutate(speed = distance / air_time * 60)\n\n# A tibble: 336,776 √ó 20\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ‚Ñπ 336,766 more rows\n# ‚Ñπ 12 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, speed &lt;dbl&gt;"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#renaming-columns",
    "href": "slides-gpt/03-data-transformation.html#renaming-columns",
    "title": "03 - Data Transformation",
    "section": "Renaming Columns",
    "text": "Renaming Columns\n\nflights %&gt;% rename(departure_delay = dep_delay, arrival_delay = arr_delay)\n\n# A tibble: 336,776 √ó 19\n    year month   day dep_time sched_dep_time departure_delay arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;           &lt;dbl&gt;    &lt;int&gt;\n 1  2013     1     1      517            515               2      830\n 2  2013     1     1      533            529               4      850\n 3  2013     1     1      542            540               2      923\n 4  2013     1     1      544            545              -1     1004\n 5  2013     1     1      554            600              -6      812\n 6  2013     1     1      554            558              -4      740\n 7  2013     1     1      555            600              -5      913\n 8  2013     1     1      557            600              -3      709\n 9  2013     1     1      557            600              -3      838\n10  2013     1     1      558            600              -2      753\n# ‚Ñπ 336,766 more rows\n# ‚Ñπ 12 more variables: sched_arr_time &lt;int&gt;, arrival_delay &lt;dbl&gt;,\n#   carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;,\n#   air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#relocating-columns",
    "href": "slides-gpt/03-data-transformation.html#relocating-columns",
    "title": "03 - Data Transformation",
    "section": "Relocating Columns",
    "text": "Relocating Columns\n\nflights %&gt;% relocate(dep_delay, arr_delay, .before = distance)\n\n# A tibble: 336,776 √ó 19\n    year month   day dep_time sched_dep_time arr_time sched_arr_time carrier\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt; &lt;chr&gt;  \n 1  2013     1     1      517            515      830            819 UA     \n 2  2013     1     1      533            529      850            830 UA     \n 3  2013     1     1      542            540      923            850 AA     \n 4  2013     1     1      544            545     1004           1022 B6     \n 5  2013     1     1      554            600      812            837 DL     \n 6  2013     1     1      554            558      740            728 UA     \n 7  2013     1     1      555            600      913            854 B6     \n 8  2013     1     1      557            600      709            723 EV     \n 9  2013     1     1      557            600      838            846 B6     \n10  2013     1     1      558            600      753            745 AA     \n# ‚Ñπ 336,766 more rows\n# ‚Ñπ 11 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;,\n#   air_time &lt;dbl&gt;, dep_delay &lt;dbl&gt;, arr_delay &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#summarizing-data",
    "href": "slides-gpt/03-data-transformation.html#summarizing-data",
    "title": "03 - Data Transformation",
    "section": "Summarizing Data",
    "text": "Summarizing Data\n\nflights %&gt;% group_by(carrier) %&gt;% summarize(avg_delay = mean(dep_delay, na.rm = TRUE))\n\n# A tibble: 16 √ó 2\n   carrier avg_delay\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 9E          16.7 \n 2 AA           8.59\n 3 AS           5.80\n 4 B6          13.0 \n 5 DL           9.26\n 6 EV          20.0 \n 7 F9          20.2 \n 8 FL          18.7 \n 9 HA           4.90\n10 MQ          10.6 \n11 OO          12.6 \n12 UA          12.1 \n13 US           3.78\n14 VX          12.9 \n15 WN          17.7 \n16 YV          19.0"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#counting-observations",
    "href": "slides-gpt/03-data-transformation.html#counting-observations",
    "title": "03 - Data Transformation",
    "section": "Counting Observations",
    "text": "Counting Observations\n\nflights %&gt;% count(carrier)\n\n# A tibble: 16 √ó 2\n   carrier     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 9E      18460\n 2 AA      32729\n 3 AS        714\n 4 B6      54635\n 5 DL      48110\n 6 EV      54173\n 7 F9        685\n 8 FL       3260\n 9 HA        342\n10 MQ      26397\n11 OO         32\n12 UA      58665\n13 US      20536\n14 VX       5162\n15 WN      12275\n16 YV        601"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#using-pipes",
    "href": "slides-gpt/03-data-transformation.html#using-pipes",
    "title": "03 - Data Transformation",
    "section": "Using Pipes",
    "text": "Using Pipes\n\nflights %&gt;% \n  filter(month == 1, day == 1) %&gt;% \n  select(year, month, day, carrier, flight)\n\n# A tibble: 842 √ó 5\n    year month   day carrier flight\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;int&gt;\n 1  2013     1     1 UA        1545\n 2  2013     1     1 UA        1714\n 3  2013     1     1 AA        1141\n 4  2013     1     1 B6         725\n 5  2013     1     1 DL         461\n 6  2013     1     1 UA        1696\n 7  2013     1     1 B6         507\n 8  2013     1     1 EV        5708\n 9  2013     1     1 B6          79\n10  2013     1     1 AA         301\n# ‚Ñπ 832 more rows"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#combining-multiple-transformations",
    "href": "slides-gpt/03-data-transformation.html#combining-multiple-transformations",
    "title": "03 - Data Transformation",
    "section": "Combining Multiple Transformations",
    "text": "Combining Multiple Transformations\n\nflights %&gt;% \n  group_by(carrier) %&gt;% \n  summarize(avg_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% \n  arrange(desc(avg_delay))\n\n# A tibble: 16 √ó 2\n   carrier avg_delay\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 F9          20.2 \n 2 EV          20.0 \n 3 YV          19.0 \n 4 FL          18.7 \n 5 WN          17.7 \n 6 9E          16.7 \n 7 B6          13.0 \n 8 VX          12.9 \n 9 OO          12.6 \n10 UA          12.1 \n11 MQ          10.6 \n12 DL           9.26\n13 AA           8.59\n14 AS           5.80\n15 HA           4.90\n16 US           3.78"
  },
  {
    "objectID": "slides-gpt/03-data-transformation.html#conclusion",
    "href": "slides-gpt/03-data-transformation.html#conclusion",
    "title": "03 - Data Transformation",
    "section": "Conclusion",
    "text": "Conclusion\n\nsummary(flights)\n\n      year          month             day           dep_time    sched_dep_time\n Min.   :2013   Min.   : 1.000   Min.   : 1.00   Min.   :   1   Min.   : 106  \n 1st Qu.:2013   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.: 907   1st Qu.: 906  \n Median :2013   Median : 7.000   Median :16.00   Median :1401   Median :1359  \n Mean   :2013   Mean   : 6.549   Mean   :15.71   Mean   :1349   Mean   :1344  \n 3rd Qu.:2013   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:1744   3rd Qu.:1729  \n Max.   :2013   Max.   :12.000   Max.   :31.00   Max.   :2400   Max.   :2359  \n                                                 NA's   :8255                 \n   dep_delay          arr_time    sched_arr_time   arr_delay       \n Min.   : -43.00   Min.   :   1   Min.   :   1   Min.   : -86.000  \n 1st Qu.:  -5.00   1st Qu.:1104   1st Qu.:1124   1st Qu.: -17.000  \n Median :  -2.00   Median :1535   Median :1556   Median :  -5.000  \n Mean   :  12.64   Mean   :1502   Mean   :1536   Mean   :   6.895  \n 3rd Qu.:  11.00   3rd Qu.:1940   3rd Qu.:1945   3rd Qu.:  14.000  \n Max.   :1301.00   Max.   :2400   Max.   :2359   Max.   :1272.000  \n NA's   :8255      NA's   :8713                  NA's   :9430      \n   carrier              flight       tailnum             origin         \n Length:336776      Min.   :   1   Length:336776      Length:336776     \n Class :character   1st Qu.: 553   Class :character   Class :character  \n Mode  :character   Median :1496   Mode  :character   Mode  :character  \n                    Mean   :1972                                        \n                    3rd Qu.:3465                                        \n                    Max.   :8500                                        \n                                                                        \n     dest              air_time        distance         hour      \n Length:336776      Min.   : 20.0   Min.   :  17   Min.   : 1.00  \n Class :character   1st Qu.: 82.0   1st Qu.: 502   1st Qu.: 9.00  \n Mode  :character   Median :129.0   Median : 872   Median :13.00  \n                    Mean   :150.7   Mean   :1040   Mean   :13.18  \n                    3rd Qu.:192.0   3rd Qu.:1389   3rd Qu.:17.00  \n                    Max.   :695.0   Max.   :4983   Max.   :23.00  \n                    NA's   :9430                                  \n     minute        time_hour                     \n Min.   : 0.00   Min.   :2013-01-01 05:00:00.00  \n 1st Qu.: 8.00   1st Qu.:2013-04-04 13:00:00.00  \n Median :29.00   Median :2013-07-03 10:00:00.00  \n Mean   :26.23   Mean   :2013-07-03 05:22:54.64  \n 3rd Qu.:44.00   3rd Qu.:2013-10-01 07:00:00.00  \n Max.   :59.00   Max.   :2013-12-31 23:00:00.00"
  },
  {
    "objectID": "scripts-lectures/2025-04-01.html",
    "href": "scripts-lectures/2025-04-01.html",
    "title": "2025-04-01",
    "section": "",
    "text": "library(here)\nlibrary(lme4)\nlibrary(tidyverse)\nlibrary(effects)\nlibrary(ggeffects)\nlibrary(sjPlot)\nlibrary(lattice)\n\n\n# loading data\ndat &lt;- readRDS(here(\"data/emoint.rds\"))\ndat &lt;- filter(dat, emotion_lbl != \"neutral\")\n\nSingle-subject model:\n\nfit &lt;- glm(acc ~ intensity * emotion_lbl, \n           data = dat, \n           subset = id == 1,\n           family = binomial(link = \"logit\"))\n\n# model without the interaction\nfit0 &lt;- glm(acc ~ intensity + emotion_lbl, \n           data = dat, \n           subset = id == 1,\n           family = binomial(link = \"logit\"))\n\nsummary(fit)\n\n\nCall:\nglm(formula = acc ~ intensity * emotion_lbl, family = binomial(link = \"logit\"), \n    data = dat, subset = id == 1)\n\nCoefficients:\n                                Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)                    -2.598651   1.347156  -1.929   0.0537 .\nintensity                       0.095568   0.038859   2.459   0.0139 *\nemotion_lbldisgust             -0.618369   1.514043  -0.408   0.6830  \nemotion_lblfear                 0.429429   1.603165   0.268   0.7888  \nemotion_lblhappiness           -0.275018   1.600348  -0.172   0.8636  \nemotion_lblsadness             -0.556507   1.737147  -0.320   0.7487  \nemotion_lblsuprise              2.267763   1.513293   1.499   0.1340  \nintensity:emotion_lbldisgust   -0.024760   0.041089  -0.603   0.5468  \nintensity:emotion_lblfear      -0.048318   0.041881  -1.154   0.2486  \nintensity:emotion_lblhappiness -0.007585   0.044519  -0.170   0.8647  \nintensity:emotion_lblsadness    0.018227   0.051148   0.356   0.7216  \nintensity:emotion_lblsuprise   -0.044934   0.042913  -1.047   0.2951  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 448.60  on 369  degrees of freedom\nResidual deviance: 265.38  on 358  degrees of freedom\nAIC: 289.38\n\nNumber of Fisher Scoring iterations: 7\n\nsummary(fit0)\n\n\nCall:\nglm(formula = acc ~ intensity + emotion_lbl, family = binomial(link = \"logit\"), \n    data = dat, subset = id == 1)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -1.824e+00  6.024e-01  -3.028  0.00246 ** \nintensity             7.148e-02  7.860e-03   9.094  &lt; 2e-16 ***\nemotion_lbldisgust   -1.425e+00  6.182e-01  -2.304  0.02121 *  \nemotion_lblfear      -1.539e+00  7.060e-01  -2.180  0.02926 *  \nemotion_lblhappiness -4.451e-01  6.421e-01  -0.693  0.48818    \nemotion_lblsadness   -2.137e-15  6.637e-01   0.000  1.00000    \nemotion_lblsuprise    8.825e-01  6.746e-01   1.308  0.19082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 448.60  on 369  degrees of freedom\nResidual deviance: 271.84  on 363  degrees of freedom\nAIC: 285.84\n\nNumber of Fisher Scoring iterations: 6\n\n# evaluating the single effects\ncar::Anova(fit)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: acc\n                      LR Chisq Df Pr(&gt;Chisq)    \nintensity              157.042  1  &lt; 2.2e-16 ***\nemotion_lbl             31.677  5  6.884e-06 ***\nintensity:emotion_lbl    6.462  5     0.2638    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(fit, fit0) # same as car::Anova()\n\nAnalysis of Deviance Table\n\nModel 1: acc ~ intensity * emotion_lbl\nModel 2: acc ~ intensity + emotion_lbl\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       358     265.38                     \n2       363     271.84 -5  -6.4619   0.2638\n\n# plotting the effects\nplot(allEffects(fit))\n\n\n\n\n\n\n\nplot(ggeffect(fit, terms = c(\"intensity\", \"emotion_lbl\")))\n\n\n\n\n\n\n\n# manually\neff &lt;- ggeffect(fit, terms = c(\"intensity\", \"emotion_lbl\"))\ndata.frame(eff)\n\n     x  predicted std.error   conf.low conf.high     group\n1   10 0.16206109 1.0134548 0.02584869 0.5850072     anger\n2   10 0.07523375 0.5710367 0.02587777 0.1994509   disgust\n3   10 0.15489361 0.7318202 0.04184388 0.4347781      fear\n4   10 0.11985130 0.6739778 0.03506656 0.3378552 happiness\n5   10 0.11740792 0.8052717 0.02671343 0.3920019   sadness\n6   10 0.54375108 0.5457759 0.29023397 0.7764599   suprise\n7   20 0.33463559 0.7357683 0.10627463 0.6802208     anger\n8   20 0.14174613 0.4586222 0.06298839 0.2886439   disgust\n9   20 0.22719363 0.6038331 0.08258613 0.4898177      fear\n10  20 0.24712418 0.5065491 0.10843502 0.4697392 happiness\n11  20 0.29333114 0.5615043 0.12134142 0.5550924   sadness\n12  20 0.66413889 0.4322265 0.45875524 0.8218514   suprise\n13  30 0.56669672 0.5980157 0.28829016 0.8085283     anger\n14  30 0.25109449 0.3609010 0.14183446 0.4048178   disgust\n15  30 0.32044495 0.4924107 0.15227929 0.5531434      fear\n16  30 0.44172120 0.3915644 0.26862075 0.6302460 happiness\n17  30 0.56431390 0.4509125 0.34862614 0.7581312   sadness\n18  30 0.76640719 0.3769221 0.61049188 0.8729046   suprise\n19  40 0.77277821 0.6898508 0.46803932 0.9293107     anger\n20  40 0.40499269 0.2929652 0.27709731 0.5472329   disgust\n21  40 0.43064266 0.4112451 0.25251283 0.6287355      fear\n22  40 0.65602957 0.3800502 0.47521173 0.8006776 happiness\n23  40 0.80164974 0.5590775 0.57465410 0.9236080   sadness\n24  40 0.84481172 0.4045034 0.71129104 0.9232458   suprise\n25  50 0.89841576 0.9466620 0.58037697 0.9826245     anger\n26  50 0.58014633 0.2776176 0.44503643 0.7042241   disgust\n27  50 0.54816730 0.3802329 0.36540650 0.7187988      fear\n28  50 0.82134401 0.4795189 0.64236416 0.9216746 happiness\n29  50 0.92653114 0.8018876 0.72370511 0.9837975   sadness\n30  50 0.90032269 0.5014756 0.77170101 0.9602158   suprise\n31  60 0.95833052 1.2721821 0.65520373 0.9964202     anger\n32  60 0.73719748 0.3224567 0.59855539 0.8407014   disgust\n33  60 0.66055460 0.4108916 0.46516502 0.8132244      fear\n34  60 0.91723130 0.6401745 0.75961878 0.9749137 happiness\n35  60 0.97521779 1.0930188 0.82204393 0.9970258   sadness\n36  60 0.93744757 0.6368946 0.81135906 0.9812097   suprise\n37  70 0.98355420 1.6256474 0.71195388 0.9993094     anger\n38  70 0.85062758 0.4081041 0.71903147 0.9268582   disgust\n39  70 0.75736091 0.4918202 0.54346873 0.8911182      fear\n40  70 0.96391547 0.8270993 0.84078153 0.9926540 happiness\n41  70 0.99192187 1.4027000 0.88707864 0.9994793   sadness\n42  70 0.96133902 0.7912609 0.84059387 0.9915436   suprise\n43  80 0.99361109 1.9922386 0.75805874 0.9998705     anger\n44  80 0.92038654 0.5145711 0.80831055 0.9694141   disgust\n45  80 0.83351729 0.6031108 0.60556023 0.9422876      fear\n46  80 0.98470730 1.0260350 0.89603987 0.9979255 happiness\n47  80 0.99739686 1.7209459 0.92926412 0.9999105   sadness\n48  80 0.97633565 0.9554345 0.86380034 0.9962880   suprise\n49  90 0.99753344 2.3658621 0.79664507 0.9999760     anger\n50  90 0.95913231 0.6314125 0.87193127 0.9877902   disgust\n51  90 0.88926557 0.7310256 0.65711799 0.9711409      fear\n52  90 0.99359849 1.2311731 0.93287683 0.9994234 happiness\n53  90 0.99916428 2.0437594 0.95608899 0.9999848   sadness\n54  90 0.98560220 1.1251304 0.88298049 0.9983924   suprise\n55 100 0.99905004 2.7436465 0.82930246 0.9999956     anger\n56 100 0.97944271 0.7538196 0.91577731 0.9952328   disgust\n57 100 0.92795947 0.8682480 0.70141067 0.9860399      fear\n58 100 0.99733432 1.4398651 0.95699760 0.9998410 happiness\n59 100 0.99973202 2.3692744 0.97289708 0.9999974   sadness\n60 100 0.99127256 1.2981848 0.89917783 0.9993092   suprise\n\ndd &lt;- expand.grid(\n    emotion_lbl = unique(dat$emotion_lbl),\n    intensity = unique(dat$intensity)\n)\n\n#predict(fit, newdata = dd, type = \"response\", se.fit = TRUE)\n\n# creating table\ntab_model(fit)\n\n\n\n\n\n\n\n\n\n\n¬†\nacc\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.07\n0.00¬†‚Äì¬†0.72\n0.054\n\n\nintensity\n1.10\n1.04¬†‚Äì¬†1.22\n0.014\n\n\nemotion lbl [disgust]\n0.54\n0.04¬†‚Äì¬†17.70\n0.683\n\n\nemotion lbl [fear]\n1.54\n0.08¬†‚Äì¬†56.59\n0.789\n\n\nemotion lbl [happiness]\n0.76\n0.04¬†‚Äì¬†27.73\n0.864\n\n\nemotion lbl [sadness]\n0.57\n0.02¬†‚Äì¬†24.64\n0.749\n\n\nemotion lbl [suprise]\n9.66\n0.66¬†‚Äì¬†320.51\n0.134\n\n\nintensity √ó emotion lbl\n[disgust]\n0.98\n0.88¬†‚Äì¬†1.04\n0.547\n\n\nintensity √ó emotion lbl\n[fear]\n0.95\n0.86¬†‚Äì¬†1.02\n0.249\n\n\nintensity √ó emotion lbl\n[happiness]\n0.99\n0.89¬†‚Äì¬†1.07\n0.865\n\n\nintensity √ó emotion lbl\n[sadness]\n1.02\n0.91¬†‚Äì¬†1.13\n0.722\n\n\nintensity √ó emotion lbl\n[suprise]\n0.96\n0.86¬†‚Äì¬†1.03\n0.295\n\n\nObservations\n370\n\n\nR2 Tjur\n0.465\n\n\n\n\n\n\n\nFit a model for each subject and plot the coefficients.\n\nfit_res &lt;- function(fit){\n    cc &lt;- summary(fit)$coefficients\n    cc &lt;- data.frame(cc)\n    ci &lt;- data.frame(confint(fit))\n    names(ci) &lt;- c(\"lower\", \"upper\")\n    names(cc) &lt;- c(\"b\", \"se\", \"z\", \"p\")\n    out &lt;- cbind(cc, ci)\n    out$param &lt;- rownames(out)\n    rownames(out) &lt;- NULL\n    return(out)\n}\n\nfit_fun &lt;- function(data){\n    glm(acc ~ intensity, family = binomial(link = \"logit\"), data = data)\n}\n\nfit_glm_res &lt;- function(data){\n    fit &lt;- fit_fun(data)\n    fit_res(fit)\n}\n\ndat |&gt; \n    group_by(id) |&gt; \n    nest() |&gt; \n    mutate(res = lapply(data, fit_glm_res)) |&gt; \n    unnest(res) |&gt; \n    ggplot(aes(x = id, y = b, ymin = lower, ymax = upper)) +\n    geom_pointrange() +\n    facet_wrap(~param, scales = \"free\")\n\n\n\n\n\n\n\n\n\n# centering on the minimum and rescaling to 0-10\ndat$intensity0 &lt;- (dat$intensity/10) - 1\n\nfit0 &lt;- glmer(acc ~ intensity0 + (1|id), data = dat, family = binomial(link = \"logit\"))\nfit &lt;- glmer(acc ~ intensity0 + (intensity0|id), data = dat, family = binomial(link = \"logit\"))\n\ncar::compareCoefs(fit0, fit, zvals = TRUE)\n\nCalls:\n1: glmer(formula = acc ~ intensity0 + (1 | id), data = dat, family = \n  binomial(link = \"logit\"))\n2: glmer(formula = acc ~ intensity0 + (intensity0 | id), data = dat, family \n  = binomial(link = \"logit\"))\n\n            Model 1 Model 2\n(Intercept) -1.0760 -1.0935\nSE           0.0457  0.0406\nz             -23.6   -26.9\n                           \nintensity0  0.32508 0.33490\nSE          0.00524 0.01148\nz              62.0    29.2\n                           \n\ndotplot(ranef(fit))\n\n$id\n\n\n\n\n\n\n\n\n# histograms of acc when intensity is 0\nhist(plogis(fixef(fit)[1] + ranef(fit)$id[[1]]))\n\n\n\n\n\n\n\nsummary(fit)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: acc ~ intensity0 + (intensity0 | id)\n   Data: dat\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n  30742.3   30783.2  -15366.2   30732.3     26265 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.2930 -0.7394  0.3993  0.7623  2.1703 \n\nRandom effects:\n Groups Name        Variance Std.Dev. Corr \n id     (Intercept) 0.069921 0.26443       \n        intensity0  0.007283 0.08534  -0.43\nNumber of obs: 26270, groups:  id, 71\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.09350    0.04058  -26.95   &lt;2e-16 ***\nintensity0   0.33490    0.01148   29.17   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr)\nintensity0 -0.545\n\n# independent slopes and intercepts\nfit2 &lt;- glmer(acc ~ intensity0 + (intensity0|id), data = dat, family = binomial(link = \"logit\"))\n\nsummary(fit2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: acc ~ intensity0 + (intensity0 | id)\n   Data: dat\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n  30742.3   30783.2  -15366.2   30732.3     26265 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.2930 -0.7394  0.3993  0.7623  2.1703 \n\nRandom effects:\n Groups Name        Variance Std.Dev. Corr \n id     (Intercept) 0.069921 0.26443       \n        intensity0  0.007283 0.08534  -0.43\nNumber of obs: 26270, groups:  id, 71\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.09350    0.04058  -26.95   &lt;2e-16 ***\nintensity0   0.33490    0.01148   29.17   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr)\nintensity0 -0.545"
  },
  {
    "objectID": "scripts-lectures/2025-03-28.html",
    "href": "scripts-lectures/2025-03-28.html",
    "title": "2025-03-28",
    "section": "",
    "text": "Preprocessing of the facial expression dataset (https://osf.io/download/tph5f/z).\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(here)\n\n# https://osf.io/zhtbj/?view_only=\n# download.file(\"https://osf.io/download/tph5f/\", \"data-raw/emoint.csv\")\n\ndat &lt;- read.csv(here(\"data-raw/emoint.csv\"))\n\n# conversion for responses\n# 1 = neutral\n# 2 = anger\n# 3 = disgust\n# 4 = fear\n# 5 = happiness\n# 6 = sadness\n# 7 = surprise\n\nresp_code &lt;- c(\n  \"1\" = \"neutral\",\n  \"2\" = \"anger\",\n  \"3\" = \"disgust\",\n  \"4\" = \"fear\",\n  \"5\" = \"happiness\",\n  \"6\" = \"sadness\",\n  \"7\" = \"suprise\"\n)\n\nemotion_code &lt;- c(\n  \"fear\" = \"fear\",\n  \"disop\" = \"disgust\",\n  \"discl\" = \"disgust\",\n  \"hap\" = \"happiness\",\n  \"sad\" = \"sadness\",\n  \"sur\" = \"suprise\",\n  \"angcl\" = \"anger\",\n  \"neutral\" = \"neutral\"\n)\n\ndat_clean &lt;- dat |&gt; \n  pivot_longer(4:ncol(dat), values_to = \"response\") |&gt; \n  separate(name, into = c(\"face\", \"emotion\", \"intensity\"), sep = \"_\")  |&gt;\n  # intensity as number\n  mutate(intensity = as.numeric(intensity)) |&gt; \n  # neutral as maximal intensity, avoid NA\n  mutate(intensity = ifelse(emotion == \"neutral\", 100, intensity))\n\nnames(dat_clean)[1:3] &lt;- c(\"id\", \"gender\", \"age\")\n\n# recoding response with labels\n# see https://adv-r.hadley.nz/subsetting.html?q=look#lookup-tables\n\ndat_clean$response_lbl &lt;- resp_code[dat_clean$response]\n\n# recoding the displayed emotion as the response\n\ndat_clean$emotion_lbl &lt;- emotion_code[dat_clean$emotion]\n\n# binary accuracy if emotion_lbl == response_lbl\n\ndat_clean$acc &lt;- as.integer(dat_clean$response_lbl == dat_clean$emotion_lbl)\n\nhead(dat_clean)\n\n# A tibble: 6 √ó 10\n     id gender age   face  emotion intensity response response_lbl emotion_lbl\n  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;      \n1     1 f      32    f1    fear           60        4 fear         fear       \n2     1 f      32    m3    disop          60        3 disgust      disgust    \n3     1 f      32    m3    hap            70        5 happiness    happiness  \n4     1 f      32    m1    hap           100        5 happiness    happiness  \n5     1 f      32    m4    disop          60        6 sadness      disgust    \n6     1 f      32    m1    fear           20        1 neutral      fear       \n# ‚Ñπ 1 more variable: acc &lt;int&gt;\n\nsaveRDS(dat_clean, here(\"data/emoint.rds\"))"
  },
  {
    "objectID": "materials/functional-programming-examples.html",
    "href": "materials/functional-programming-examples.html",
    "title": "Functional programming examples",
    "section": "",
    "text": "library(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr)\n\ndat &lt;- iris\n\n# functions\n\nremove_rownames &lt;- function(x){\n    rownames(x) &lt;- NULL\n    x\n}\n\nsum_lm &lt;- function(x, conf.level = 0.95){\n    xs &lt;- data.frame(summary(x)$coefficients)\n    xs$param &lt;- rownames(xs)\n    rownames(xs) &lt;- NULL\n    names(xs) &lt;- c(\"b\", \"se\", \"t\", \"pval\", \"param\")\n    cc &lt;- data.frame(confint(x, level = conf.level))\n    names(cc) &lt;- c(\"ci.lb\", \"ci.ub\")\n    rownames(cc) &lt;- NULL\n    cbind(\n        xs[, c(\"param\", \"b\", \"se\", \"t\", \"pval\")],\n        cc\n    )\n}\n\n# fit a linear model for each Species\n\ndatl &lt;- split(dat, dat$Species)\nfitl &lt;- lapply(datl, function(d) lm(Sepal.Length ~ Petal.Width + Petal.Length, data = d))\nresl &lt;- lapply(fitl, sum_lm)\n\nres_by_species &lt;- do.call(rbind, resl)\nres_by_species &lt;- remove_rownames(res_by_species)\nres_by_species$species &lt;- rep(names(resl), sapply(resl, nrow))\n\nggplot(res_by_species, aes(x = param, \n                           y = b, \n                           ymin = ci.lb, \n                           ymax = ci.ub,\n                           color = species)) +\n    geom_pointrange(position = position_dodge(width = 0.5))\n\n\n\n\n\n\n\n# alternative version using other packages\n\nresl &lt;- lapply(fitl, broom::tidy, conf.int = TRUE)\nres_by_species &lt;- dplyr::bind_rows(resl, .id = \"species\")\n\nres_by_species\n\n# A tibble: 9 √ó 8\n  species    term       estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 setosa     (Intercep‚Ä¶  4.25       0.411    10.3    1.13e-13   3.42       5.08 \n2 setosa     Petal.Wid‚Ä¶  0.712      0.487     1.46   1.51e- 1  -0.268      1.69 \n3 setosa     Petal.Len‚Ä¶  0.399      0.296     1.35   1.84e- 1  -0.196      0.994\n4 versicolor (Intercep‚Ä¶  2.38       0.449     5.30   3.04e- 6   1.48       3.28 \n5 versicolor Petal.Wid‚Ä¶ -0.320      0.402    -0.795  4.30e- 1  -1.13       0.489\n6 versicolor Petal.Len‚Ä¶  0.934      0.169     5.52   1.44e- 6   0.594      1.27 \n7 virginica  (Intercep‚Ä¶  1.05       0.514     2.05   4.63e- 2   0.0179     2.09 \n8 virginica  Petal.Wid‚Ä¶  0.00706    0.179     0.0394 9.69e- 1  -0.354      0.368\n9 virginica  Petal.Len‚Ä¶  0.995      0.0893   11.1    8.87e-15   0.815      1.17 \n\n# bootstrapping\n\nboot &lt;- function(data, B = 100){\n    res &lt;- vector(mode = \"list\", length = B)\n    n &lt;- nrow(data)\n    for(i in 1:B){\n        idx &lt;- sample(x = 1:n, size = n, replace = TRUE)\n        dataB &lt;- data[idx, ]\n        rownames(dataB) &lt;- NULL\n        res[[i]] &lt;- dataB\n    }\n    return(res)\n}\n\nfit_lm &lt;- function(data){\n    lm(Sepal.Length ~ Petal.Width + Petal.Length, data = data)\n}\n\nbootl &lt;- lapply(datl, boot, B = 100)\nfit_bootl &lt;- lapply(bootl, function(x) lapply(x, fit_lm))\nres_bootl &lt;- lapply(fit_bootl, function(x) lapply(x, sum_lm))\nres_bootl &lt;- lapply(res_bootl, function(x) do.call(rbind, x))\nres_boot_by_species &lt;- do.call(rbind, res_bootl)\nres_boot_by_species &lt;- remove_rownames(res_boot_by_species)\nres_boot_by_species$species &lt;- rep(names(res_bootl), sapply(res_bootl, nrow))\n\nggplot(res_boot_by_species, aes(x = param, y = b, fill = species)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n# using nested tibbles\n\ndat |&gt; \n    group_by(Species) |&gt; \n    nest() |&gt; \n    mutate(boot = map(data, boot)) |&gt; \n    select(-data) |&gt; \n    unnest(boot) |&gt; \n    mutate(fit = map(boot, fit_lm)) |&gt; \n    mutate(res = map(fit, sum_lm)) |&gt; \n    select(-boot, -fit) |&gt; \n    unnest(res)\n\n# A tibble: 900 √ó 8\n# Groups:   Species [3]\n   Species param            b    se      t     pval  ci.lb ci.ub\n   &lt;fct&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 setosa  (Intercept)  4.27  0.369 11.6   2.20e-15  3.53  5.01 \n 2 setosa  Petal.Width  1.57  0.596  2.64  1.12e- 2  0.376 2.77 \n 3 setosa  Petal.Length 0.280 0.265  1.06  2.96e- 1 -0.253 0.813\n 4 setosa  (Intercept)  3.62  0.460  7.87  4.02e-10  2.70  4.55 \n 5 setosa  Petal.Width  0.457 0.575  0.796 4.30e- 1 -0.699 1.61 \n 6 setosa  Petal.Length 0.847 0.353  2.40  2.05e- 2  0.136 1.56 \n 7 setosa  (Intercept)  4.17  0.473  8.81  1.62e-11  3.22  5.12 \n 8 setosa  Petal.Width  0.297 0.469  0.632 5.30e- 1 -0.648 1.24 \n 9 setosa  Petal.Length 0.591 0.348  1.70  9.63e- 2 -0.110 1.29 \n10 setosa  (Intercept)  4.22  0.431  9.78  6.55e-13  3.35  5.08 \n# ‚Ñπ 890 more rows"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Psychometrics4Neuroscience",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nscript\n\n\n\n\n\n\nBase R vs Tidyverse\n\n\n¬†\n\n\n\n\nFunctional programming examples\n\n\n¬†\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Psychometrics4Neuroscience",
    "section": "",
    "text": "Shimizu et al. (2024) analyzed the impact of the type and intensity of the facial expression on the detection accuracy.\n\nThis study examined the relationship between the intensity of emotional expressions in facial stimuli and receivers‚Äô decoding accuracy for six basic emotions: anger, disgust, fear, happiness, sadness, and surprise. A laboratory experiment was conducted using the forced‚Äêchoice method, in which the intensity of each stimulus was manipulated at every 10% interval using the morphing technique. To explore whether a linear relationship would be observed when the intensity was finely manipulated at 10% intervals, a hierarchical multiple regression analysis was performed. The mean percentage of correct responses for each stimulus was the dependent variable, and the linear, quadratic, and cubic terms of the stimulus intensity were the independent variables. The results showed that the linear model was not adopted as the final model for all facial expressions; that is, the effect of the squared term of intensity was significant for anger, disgust, fear, and sadness, while the effect of the cubic term of intensity was significant for happiness and surprise. Our findings indicate that a higher intensity of emotional expression does not yield higher decoding accuracy.\n\nData are available on OSF\n\nAssignment for 31/03/2025"
  },
  {
    "objectID": "examples.html#face-processing",
    "href": "examples.html#face-processing",
    "title": "Psychometrics4Neuroscience",
    "section": "",
    "text": "Shimizu et al. (2024) analyzed the impact of the type and intensity of the facial expression on the detection accuracy.\n\nThis study examined the relationship between the intensity of emotional expressions in facial stimuli and receivers‚Äô decoding accuracy for six basic emotions: anger, disgust, fear, happiness, sadness, and surprise. A laboratory experiment was conducted using the forced‚Äêchoice method, in which the intensity of each stimulus was manipulated at every 10% interval using the morphing technique. To explore whether a linear relationship would be observed when the intensity was finely manipulated at 10% intervals, a hierarchical multiple regression analysis was performed. The mean percentage of correct responses for each stimulus was the dependent variable, and the linear, quadratic, and cubic terms of the stimulus intensity were the independent variables. The results showed that the linear model was not adopted as the final model for all facial expressions; that is, the effect of the squared term of intensity was significant for anger, disgust, fear, and sadness, while the effect of the cubic term of intensity was significant for happiness and surprise. Our findings indicate that a higher intensity of emotional expression does not yield higher decoding accuracy.\n\nData are available on OSF\n\nAssignment for 31/03/2025"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting started",
    "section": "",
    "text": "Shared Notepad: online notepad to share the R code during the class.\nShared Drive: folder to share files and script if necessary\n\n\nQRcode always redirecting here:"
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Getting started",
    "section": "Quarto",
    "text": "Quarto\nQuarto is an an open-source scientific and technical publishing system. It is very easy to install and you can find instructions here https://quarto.org/docs/get-started/.\nTo check if Quarto is installed, you can open a terminal (e.g., Powershell or CMD in Windows) and type:\nquarto --version\nThis should return the version of Quarto without errors or strange messages."
  },
  {
    "objectID": "index.html#gitgithub",
    "href": "index.html#gitgithub",
    "title": "Getting started",
    "section": "Git/Github",
    "text": "Git/Github\nGit is a version-control system that you can easily install from here https://git-scm.com/. You can use Git from the command line depending on your operating system or you can use the Github Desktop software https://desktop.github.com/download/ to work with a GUI.\nGithub is the online version of Git and works as an online repository to share and store the code. In order to work with Git and Github you need to:\n\ncreate an account https://github.com/\nconfigure your local machine with an SSH key (with this you do not need to write the password every time). Configuring an SSH key is not so easy thus I suggest you to use the Github CLI tool https://cli.github.com/ where you can configure your account using SSH very easily. The instructions can be found here https://cli.github.com/manual/gh_auth_login and we need to use the gh auth login command.\n\nAlso here you can check the Git installation typing this command in the terminal:\ngit --version"
  },
  {
    "objectID": "index.html#cloning-the-repository",
    "href": "index.html#cloning-the-repository",
    "title": "Getting started",
    "section": "Cloning the repository",
    "text": "Cloning the repository\nNow that we have all the tools we can clone the repository of the course. You can navigate into the folder that you want in your machine and type:\ngit clone git@github.com:stat-teaching/psychometrics4neuroscience.git\nOr manually (but there will not be the Git-Github link) downloading the zip folder from https://github.com/stat-teaching/psychometrics4neuroscience.\nIf you cloned the repository you can navigate into the folder and type:\ngit status\nThis should return some info or messages not errors or messages related to non being in a Git repository.\nNow you have the repository of the course. I will update this repository during the course with new slides and materials.\nDo not modify the files and everytime you start working or using this repository run:\ngit pull\nAnd your local repository will be updated."
  },
  {
    "objectID": "index.html#creating-your-repository",
    "href": "index.html#creating-your-repository",
    "title": "Getting started",
    "section": "Creating your repository",
    "text": "Creating your repository\nFor the course exercises, notes and everything else, I suggest you to create a local folder to track with Git and Github. Follow these steps:\n\ncreate an R Project\ncreate an R/ folder\ninit the git repository with the command git init\nadd all files with git add .\ncommit the changes with git commit -m \"message\"\n\nThen you need to create and link the online repository. Go to Github and create a new empty repository. Copy the SSH address and run git remote add origin &lt;ssh link&gt;. Then run git push. In this way your local and online Git repositories will be linked.\nCheck these other resources to learn Git:\n\nhttps://rogerdudler.github.io/git-guide/\nhttps://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/"
  },
  {
    "objectID": "index.html#renv",
    "href": "index.html#renv",
    "title": "Getting started",
    "section": "renv",
    "text": "renv\nrenv is the equivalent of venv in Python. Basically allows to create a by-project library of R packages with a specific version. Sometimes packages change defaults, functions, etc. thus fixing them improve the reproducibility and remove unexpected results.\nFor a detailed guide about renv you can read the official documentation https://rstudio.github.io/renv/articles/renv.html.\nWhen opening the R project of the repository, renv should prompt you to install renv itself (if missing) and then to install the packages included into the renv.lock file.\nTo create new project with renv you can type:\nrenv::init()\nand following the instructions. renv will create all the required files and folders and switch to the local library."
  },
  {
    "objectID": "materials/baseR-vs-tidyverse.html",
    "href": "materials/baseR-vs-tidyverse.html",
    "title": "Base R vs Tidyverse",
    "section": "",
    "text": "In this exercise, we will use two datasets:\n\nThe iris dataset for complex operations on grouped data.\nThe mtcars dataset for reshaping between long and wide formats.\n\nThis will allow us to compare different data manipulation tasks using base R and the tidyverse."
  },
  {
    "objectID": "materials/baseR-vs-tidyverse.html#base-r-solution",
    "href": "materials/baseR-vs-tidyverse.html#base-r-solution",
    "title": "Base R vs Tidyverse",
    "section": "Base R Solution",
    "text": "Base R Solution\n\n# Load the iris dataset\ndata(iris)\n\n# Base R approach using tapply and aggregate\nmean_sd_base &lt;- aggregate(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~ Species, data = iris, \n                          FUN = function(x) c(mean = mean(x), sd = sd(x)))\n\n# Flatten the results\nmean_sd_base &lt;- do.call(data.frame, mean_sd_base)\n\n# Display the result\nmean_sd_base\n\n     Species Sepal.Length.mean Sepal.Length.sd Sepal.Width.mean Sepal.Width.sd\n1     setosa             5.006       0.3524897            3.428      0.3790644\n2 versicolor             5.936       0.5161711            2.770      0.3137983\n3  virginica             6.588       0.6358796            2.974      0.3224966\n  Petal.Length.mean Petal.Length.sd Petal.Width.mean Petal.Width.sd\n1             1.462       0.1736640            0.246      0.1053856\n2             4.260       0.4699110            1.326      0.1977527\n3             5.552       0.5518947            2.026      0.2746501"
  },
  {
    "objectID": "materials/baseR-vs-tidyverse.html#tidyverse-solution",
    "href": "materials/baseR-vs-tidyverse.html#tidyverse-solution",
    "title": "Base R vs Tidyverse",
    "section": "Tidyverse Solution",
    "text": "Tidyverse Solution\n\n# Load the tidyverse package\nlibrary(tidyverse)\n\n# Tidyverse approach using dplyr\nmean_sd_tidy &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(across(starts_with(\"Sepal\") | starts_with(\"Petal\"), \n                   list(mean = ~mean(.), sd = ~sd(.)), \n                   .names = \"{col}_{fn}\"))\n\n# Display the result\nmean_sd_tidy\n\n# A tibble: 3 √ó 9\n  Species    Sepal.Length_mean Sepal.Length_sd Sepal.Width_mean Sepal.Width_sd\n  &lt;fct&gt;                  &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 setosa                  5.01           0.352             3.43          0.379\n2 versicolor              5.94           0.516             2.77          0.314\n3 virginica               6.59           0.636             2.97          0.322\n# ‚Ñπ 4 more variables: Petal.Length_mean &lt;dbl&gt;, Petal.Length_sd &lt;dbl&gt;,\n#   Petal.Width_mean &lt;dbl&gt;, Petal.Width_sd &lt;dbl&gt;"
  },
  {
    "objectID": "materials/baseR-vs-tidyverse.html#base-r-solution-1",
    "href": "materials/baseR-vs-tidyverse.html#base-r-solution-1",
    "title": "Base R vs Tidyverse",
    "section": "Base R Solution",
    "text": "Base R Solution\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Add car names as a column instead of row names\nmtcars$car &lt;- rownames(mtcars)\n\n# Base R approach to long format\nmtcars_long_base &lt;- reshape(mtcars, idvar = \"car\", varying = names(mtcars)[1:11], \n                            v.names = \"value\", timevar = \"variable\", \n                            times = names(mtcars)[1:11], direction = \"long\")\n\n# Back to wide format\nmtcars_wide_base &lt;- reshape(mtcars_long_base, idvar = \"car\", timevar = \"variable\", \n                            direction = \"wide\")\n\n# Display results\nhead(mtcars_long_base)\n\n                                    car variable value\nMazda RX4.mpg                 Mazda RX4      mpg  21.0\nMazda RX4 Wag.mpg         Mazda RX4 Wag      mpg  21.0\nDatsun 710.mpg               Datsun 710      mpg  22.8\nHornet 4 Drive.mpg       Hornet 4 Drive      mpg  21.4\nHornet Sportabout.mpg Hornet Sportabout      mpg  18.7\nValiant.mpg                     Valiant      mpg  18.1\n\nhead(mtcars_wide_base)\n\n                                    car value.mpg value.cyl value.disp value.hp\nMazda RX4.mpg                 Mazda RX4      21.0         6        160      110\nMazda RX4 Wag.mpg         Mazda RX4 Wag      21.0         6        160      110\nDatsun 710.mpg               Datsun 710      22.8         4        108       93\nHornet 4 Drive.mpg       Hornet 4 Drive      21.4         6        258      110\nHornet Sportabout.mpg Hornet Sportabout      18.7         8        360      175\nValiant.mpg                     Valiant      18.1         6        225      105\n                      value.drat value.wt value.qsec value.vs value.am\nMazda RX4.mpg               3.90    2.620      16.46        0        1\nMazda RX4 Wag.mpg           3.90    2.875      17.02        0        1\nDatsun 710.mpg              3.85    2.320      18.61        1        1\nHornet 4 Drive.mpg          3.08    3.215      19.44        1        0\nHornet Sportabout.mpg       3.15    3.440      17.02        0        0\nValiant.mpg                 2.76    3.460      20.22        1        0\n                      value.gear value.carb\nMazda RX4.mpg                  4          4\nMazda RX4 Wag.mpg              4          4\nDatsun 710.mpg                 4          1\nHornet 4 Drive.mpg             3          1\nHornet Sportabout.mpg          3          2\nValiant.mpg                    3          1"
  },
  {
    "objectID": "materials/baseR-vs-tidyverse.html#tidyverse-solution-1",
    "href": "materials/baseR-vs-tidyverse.html#tidyverse-solution-1",
    "title": "Base R vs Tidyverse",
    "section": "Tidyverse Solution",
    "text": "Tidyverse Solution\n\n# Tidyverse approach to long format\nmtcars_long_tidy &lt;- mtcars %&gt;% \n  pivot_longer(cols = -car, names_to = \"variable\", values_to = \"value\")\n\n# Back to wide format\nmtcars_wide_tidy &lt;- mtcars_long_tidy %&gt;% \n  pivot_wider(names_from = variable, values_from = value)\n\n# Display results\nhead(mtcars_long_tidy)\n\n# A tibble: 6 √ó 3\n  car       variable  value\n  &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n1 Mazda RX4 mpg       21   \n2 Mazda RX4 cyl        6   \n3 Mazda RX4 disp     160   \n4 Mazda RX4 hp       110   \n5 Mazda RX4 drat       3.9 \n6 Mazda RX4 wt         2.62\n\nhead(mtcars_wide_tidy)\n\n# A tibble: 6 √ó 12\n  car            mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mazda RX4     21       6   160   110  3.9   2.62  16.5     0     1     4     4\n2 Mazda RX4 W‚Ä¶  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n3 Datsun 710    22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n4 Hornet 4 Dr‚Ä¶  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n5 Hornet Spor‚Ä¶  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n6 Valiant       18.1     6   225   105  2.76  3.46  20.2     1     0     3     1"
  },
  {
    "objectID": "materials/baseR-vs-tidyverse.html#pros-and-cons-of-base-r",
    "href": "materials/baseR-vs-tidyverse.html#pros-and-cons-of-base-r",
    "title": "Base R vs Tidyverse",
    "section": "Pros and Cons of Base R",
    "text": "Pros and Cons of Base R\n\nPros:\n\nFlexibility: Base R allows detailed control over transformations.\nNo external dependencies: No need to install additional packages.\nSuitable for simple tasks: If transformations are minimal, base R can be effective.\n\n\n\nCons:\n\nVerbose: Base R code for reshaping data is long and requires multiple parameters.\nLess intuitive: The syntax for reshape() can be confusing.\nMore manual work: Intermediate steps often need to be managed explicitly."
  },
  {
    "objectID": "materials/baseR-vs-tidyverse.html#pros-and-cons-of-tidyverse",
    "href": "materials/baseR-vs-tidyverse.html#pros-and-cons-of-tidyverse",
    "title": "Base R vs Tidyverse",
    "section": "Pros and Cons of Tidyverse",
    "text": "Pros and Cons of Tidyverse\n\nPros:\n\nConcise and readable: Functions like pivot_longer() and pivot_wider() are intuitive.\nStreamlined workflow: Tidyverse simplifies common operations like grouping and reshaping.\nBetter suited for modern data analysis: Works well with pipes and declarative transformations.\n\n\n\nCons:\n\nRequires package installation: Tidyverse needs additional dependencies.\nLearning curve: Users new to functional programming might need time to adapt.\nMay not cover every niche use case: Highly specific transformations might need workarounds."
  },
  {
    "objectID": "scripts-lectures.html",
    "href": "scripts-lectures.html",
    "title": "Psychometrics4Neuroscience",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\n2025-03-28\n\n\n\n\n2025-03-31\n\n\n\n\n2025-04-01\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "scripts-lectures/2025-03-31.html",
    "href": "scripts-lectures/2025-03-31.html",
    "title": "2025-03-31",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "scripts-lectures/2025-03-31.html#data",
    "href": "scripts-lectures/2025-03-31.html#data",
    "title": "2025-03-31",
    "section": "Data",
    "text": "Data\n\n# loading the cleaned data version\ndat &lt;- readRDS(here(\"data\", \"emoint.rds\"))"
  },
  {
    "objectID": "scripts-lectures/2025-03-31.html#participant-level",
    "href": "scripts-lectures/2025-03-31.html#participant-level",
    "title": "2025-03-31",
    "section": "Participant-level",
    "text": "Participant-level\nLet‚Äôs compute some statistics at the participant level:\nOverall accuracy of the experiment:\n\nchance_level &lt;- 1 / length(unique(dat$response))\n\ndat |&gt; \n    group_by(id) |&gt; \n    summarise(acc = mean(acc),\n              n_trials = n()) |&gt;\n    ggplot(aes(x = acc, y = id)) +\n    geom_point(aes(size = n_trials)) +\n    geom_vline(xintercept = chance_level, lwd = 1, lty = \"dashed\")\n\n\n\n\n\n\n\n\nAll the participants are clearly above the chance level. For each trial there was all the 7 options.\n\ndat |&gt; \n    filter(emotion_lbl != \"neutral\") |&gt; \n    group_by(id, emotion_lbl) |&gt; \n    summarise(acc = mean(acc),\n              n_trial = n()) |&gt; \n    ggplot(aes(x = emotion_lbl, y = acc, fill = emotion_lbl)) +\n    geom_point(aes(size = n_trial), position = position_jitter(width = 0.3, seed = 2025), alpha = 0.5, show.legend = FALSE) +\n    geom_boxplot(show.legend = FALSE)\n\n\n\n\n\n\n\n\n\ndat |&gt;\n    filter(emotion_lbl != \"neutral\") |&gt;  \n    group_by(id, intensity) |&gt; \n    summarise(acc = mean(acc)) |&gt; \n    ggplot(aes(x = intensity, y = acc)) +\n    geom_point() +\n    geom_line(aes(group = id), alpha = 0.5) +\n    geom_boxplot(aes(group = intensity))"
  },
  {
    "objectID": "scripts-lectures/2025-03-31.html#errors",
    "href": "scripts-lectures/2025-03-31.html#errors",
    "title": "2025-03-31",
    "section": "Errors",
    "text": "Errors\n\ndat |&gt; \n    group_by(emotion_lbl, response_lbl) |&gt; \n    count() |&gt; \n    group_by(emotion_lbl)  |&gt; \n    mutate(tot = sum(n)) |&gt; \n    ungroup() |&gt; \n    mutate(p = n / tot) |&gt; \n    mutate(is_correct = ifelse(emotion_lbl == response_lbl, 1, 0)) |&gt; \n    ggplot(aes(x = response_lbl, fill = factor(is_correct), y = p)) +\n    geom_col(position = position_dodge(), show.legend = FALSE) +\n    facet_wrap(~emotion_lbl, scales = \"free_y\") +\n    theme(\n        axis.text.x = element_text(angle = 90)\n    ) +\n        ylim(c(0,1)) +\n        scale_fill_manual(values = c(scales::alpha(\"black\", 0.5), \"firebrick\"))\n\n\n\n\n\n\n\n\nThis is the proportion of responses for each emotion label as a function of the intensity.\n\ndat |&gt; \n    filter(emotion_lbl != \"neutral\") |&gt; \n    group_by(emotion_lbl, response_lbl, intensity) |&gt; \n    count() |&gt; \n    group_by(emotion_lbl, intensity) |&gt; \n    mutate(tot = sum(n))  |&gt; \n    ungroup() |&gt; \n    mutate(p = n / tot) |&gt; \n    mutate(is_correct = ifelse(emotion_lbl == response_lbl, 1, 0)) |&gt; \n    ggplot(aes(x = intensity, y = p, color = response_lbl)) +\n    facet_wrap(~emotion_lbl) +\n    geom_line(aes(lty = factor(is_correct)), lwd = 1) +\n    scale_linetype_manual(values = c(\"dashed\", \"solid\"), guide = \"none\")"
  },
  {
    "objectID": "scripts-lectures/2025-03-31.html#variability-measures",
    "href": "scripts-lectures/2025-03-31.html#variability-measures",
    "title": "2025-03-31",
    "section": "Variability measures",
    "text": "Variability measures\nHere I‚Äôm calculating a variability measure (Shannon Entropy) to capture the variability of responses for a specific intensity level for each emotion. Higher entropy means a more uncertain response.\n\nentropy &lt;- function(p, relative = FALSE) {\n  p &lt;- p[p &gt; 0]  # Exclude zero probabilities to avoid log(0)\n  H &lt;- -sum(p * log2(p))\n  \n  if (relative) {\n    H &lt;- H / log2(length(p))\n  }\n  return(H)\n}\n\ndat |&gt; \n    filter(emotion_lbl != \"neutral\") |&gt; \n    group_by(emotion_lbl, response_lbl, intensity) |&gt; \n    count() |&gt; \n    group_by(emotion_lbl, intensity) |&gt; \n    mutate(tot = sum(n))  |&gt; \n    ungroup() |&gt; \n    mutate(p = n / tot) |&gt; \n    mutate(is_correct = ifelse(emotion_lbl == response_lbl, 1, 0)) |&gt; \n    group_by(emotion_lbl, intensity) |&gt; \n    summarise(entropy = entropy(p, relative = TRUE)) |&gt; \n    ggplot(aes(x = intensity, y = entropy)) +\n    geom_line(aes(color = emotion_lbl), lwd = 1)\n\n\n\n\n\n\n\n\n\nget_intersection &lt;- function(y1, y2, x){\n    x[which.min(abs(y1 - y2))]\n}\n\nget_intersection_point &lt;- function(data){\n    data &lt;- data |&gt; \n        select(intensity, response_lbl, p) |&gt; \n        pivot_wider(names_from = response_lbl, values_from = p)\n    datat &lt;- select(data, -intensity)\n    idx &lt;- combn(1:ncol(datat), 2) # variables combination\n    datat &lt;- mutate(datat, across(everything(), replace_na, 0)) # replace NA with 0\n    x &lt;- unique(data$intensity)\n\n    res &lt;- vector(mode = \"list\", length = ncol(idx))\n\n    for(i in 1:length(res)){\n        rc &lt;- idx[, i]\n        ii &lt;- get_intersection(datat[[rc[1]]], datat[[rc[2]]], x)\n        res[[i]] &lt;- data.frame(intensity = ii, emo1 = names(datat)[rc[1]], emo2 = names(datat)[rc[2]])\n    }\n    do.call(rbind, res)\n}\n\nint_point &lt;- dat |&gt; \n    filter(emotion_lbl != \"neutral\") |&gt; \n    group_by(emotion_lbl, response_lbl, intensity) |&gt; \n    count() |&gt; \n    group_by(emotion_lbl, intensity) |&gt; \n    mutate(tot = sum(n))  |&gt; \n    ungroup() |&gt; \n    mutate(p = n / tot) |&gt; \n    group_nest(emotion_lbl) |&gt; \n    mutate(int_point = map(data, get_intersection_point))"
  },
  {
    "objectID": "scripts-lectures/2025-03-31.html#face-specific-effects",
    "href": "scripts-lectures/2025-03-31.html#face-specific-effects",
    "title": "2025-03-31",
    "section": "Face-specific effects",
    "text": "Face-specific effects\nWe can also visualize some identity-specific effect to see if the pattern is the same for each face.\n\nby_face &lt;- dat |&gt; \n    filter(emotion_lbl != \"neutral\") |&gt; \n    group_by(face, emotion_lbl, response_lbl, intensity) |&gt; \n    count() |&gt; \n    group_by(emotion_lbl, intensity) |&gt; \n    mutate(tot = sum(n)) |&gt; \n    mutate(p = n/tot) |&gt; \n    group_by(emotion_lbl) |&gt; \n    nest() |&gt; \n    mutate(plt = map(data, function(x){\n        ggplot(x, aes(x = intensity, y = p, color = response_lbl)) +\n            geom_line() +\n            facet_wrap(~face)\n    }))\n\n\ndisgustfearhappinesssadnesssupriseanger"
  },
  {
    "objectID": "scripts-lectures/2025-03-31.html#overall-plot-try-to-reproduce",
    "href": "scripts-lectures/2025-03-31.html#overall-plot-try-to-reproduce",
    "title": "2025-03-31",
    "section": "Overall plot [try to reproduce :)]",
    "text": "Overall plot [try to reproduce :)]\n\ndat |&gt; \n    filter(emotion_lbl != \"neutral\") |&gt; \n    ggplot(aes(x = intensity, y = acc, color = emotion_lbl)) +\n    stat_smooth(aes(group = id), geom = \"line\", method = \"glm\", formula = y ~ x, method.args = list(family = binomial()), se = FALSE, alpha = 0.5) +\n    facet_wrap(~emotion_lbl) +\n    geom_point(position = position_jitter(height = 0.05),\n    alpha = 0.05, color = \"black\") +\n    xlab(\"Intensity\") +\n    ylab(\"Accuracy\") +\n    geom_smooth(aes(x = intensity, y = acc), method = \"glm\", formula = y ~ x, method.args = list(family = binomial()), se = FALSE, color = \"black\") +\n    theme_minimal(base_size = 20) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "slides-gpt.html",
    "href": "slides-gpt.html",
    "title": "Psychometrics4Neuroscience",
    "section": "",
    "text": "Here I included some slides and materials taken from textbooks. They are mainly summaries made with GPT and a little bit of manual cleaning, there is little or no effort from my side. They are only useful to extract the code examples summarizing the crucial parts of the chapters."
  },
  {
    "objectID": "slides-gpt.html#r4ds",
    "href": "slides-gpt.html#r4ds",
    "title": "Psychometrics4Neuroscience",
    "section": "R4DS",
    "text": "R4DS\nThis is an amazing book about the tidyverse in applied data manipulation and processing with a lot of examples and an overview of the most important R packages."
  },
  {
    "objectID": "slides-gpt/05-tidy-data.html#data",
    "href": "slides-gpt/05-tidy-data.html#data",
    "title": "05- Tidy Data",
    "section": "Data",
    "text": "Data\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "slides-gpt/05-tidy-data.html#tidy-data",
    "href": "slides-gpt/05-tidy-data.html#tidy-data",
    "title": "05- Tidy Data",
    "section": "Tidy data",
    "text": "Tidy data\nYou can represent the same underlying data in multiple ways. The example below shows the same data organized in three different ways. Each dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\ntable1\n\n# A tibble: 6 √ó 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\ntable2\n\n# A tibble: 12 √ó 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 √ó 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583"
  },
  {
    "objectID": "slides-gpt/05-tidy-data.html#principles-of-tidy-data",
    "href": "slides-gpt/05-tidy-data.html#principles-of-tidy-data",
    "title": "05- Tidy Data",
    "section": "Principles of tidy data",
    "text": "Principles of tidy data\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\ndplyr, ggplot2, and all the other packages in the tidyverse but also most of the modelling packages (lme4, brms, etc.) are designed to work with tidy data."
  },
  {
    "objectID": "slides-gpt/05-tidy-data.html#example",
    "href": "slides-gpt/05-tidy-data.html#example",
    "title": "05- Tidy Data",
    "section": "Example",
    "text": "Example\n\n# Compute rate per 10,000\ntable1 |&gt;\n  mutate(rate = cases / population * 10000)\n\n# Compute total cases per year\ntable1 |&gt; \n  group_by(year) |&gt; \n  summarize(total_cases = sum(cases))\n\n# Visualize changes over time\nggplot(table1, aes(x = year, y = cases)) +\n  geom_line(aes(group = country), color = \"grey50\") +\n  geom_point(aes(color = country, shape = country)) +\n  scale_x_continuous(breaks = c(1999, 2000)) # x-axis breaks at 1999 and 2000"
  },
  {
    "objectID": "slides-gpt/05-tidy-data.html#sec-billboard",
    "href": "slides-gpt/05-tidy-data.html#sec-billboard",
    "title": "05- Tidy Data",
    "section": "Data in column names",
    "text": "Data in column names\nThe billboard dataset records the billboard rank of songs in the year 2000:\n\nbillboard\n\n# A tibble: 317 √ó 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby‚Ä¶ 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The ‚Ä¶ 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D‚Ä¶ Kryp‚Ä¶ 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D‚Ä¶ Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb‚Ä¶ 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give‚Ä¶ 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc‚Ä¶ 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do‚Ä¶ 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try ‚Ä¶ 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo‚Ä¶ Open‚Ä¶ 2000-08-26      76    76    74    69    68    67    61    58\n# ‚Ñπ 307 more rows\n# ‚Ñπ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, ‚Ä¶\n\n\nIn this dataset, each observation is a song. The first three columns (artist, track and date.entered) are variables that describe the song. Then we have 76 columns (wk1-wk76) that describe the rank of the song in each week."
  },
  {
    "objectID": "slides-gpt/05-tidy-data.html#wide-to-long-format",
    "href": "slides-gpt/05-tidy-data.html#wide-to-long-format",
    "title": "05- Tidy Data",
    "section": "Wide to long format",
    "text": "Wide to long format\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n# A tibble: 24,092 √ó 5\n   artist track                   date.entered week   rank\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n# ‚Ñπ 24,082 more rows"
  },
  {
    "objectID": "slides-gpt/05-tidy-data.html#wide-to-long-format-1",
    "href": "slides-gpt/05-tidy-data.html#wide-to-long-format-1",
    "title": "05- Tidy Data",
    "section": "Wide to long format",
    "text": "Wide to long format\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n\n# A tibble: 5,307 √ó 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# ‚Ñπ 5,297 more rows"
  },
  {
    "objectID": "slides-gpt/05-tidy-data.html#wide-to-long-format-2",
    "href": "slides-gpt/05-tidy-data.html#wide-to-long-format-2",
    "title": "05- Tidy Data",
    "section": "Wide to long format",
    "text": "Wide to long format\n\nbillboard_longer &lt;- billboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt; \n  mutate(\n    week = parse_number(week)\n  )\nbillboard_longer\n\n# A tibble: 5,307 √ó 5\n   artist  track                   date.entered  week  rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26       1    87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26       2    82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26       3    72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26       4    77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26       5    87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26       6    94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26       7    99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02       1    91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02       2    87\n10 2Ge+her The Hardest Part Of ... 2000-09-02       3    92\n# ‚Ñπ 5,297 more rows"
  },
  {
    "objectID": "slides-gpt/05-tidy-data.html#long-to-wide",
    "href": "slides-gpt/05-tidy-data.html#long-to-wide",
    "title": "05- Tidy Data",
    "section": "Long to wide",
    "text": "Long to wide\n\ncms_patient_experience |&gt; \n  distinct(measure_cd, measure_title)\n\n# A tibble: 6 √ó 2\n  measure_cd   measure_title                                                    \n  &lt;chr&gt;        &lt;chr&gt;                                                            \n1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and Infor‚Ä¶\n2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate               \n3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider                 \n4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education               \n5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff           \n6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources"
  },
  {
    "objectID": "slides-gpt/05-tidy-data.html#long-to-wide-1",
    "href": "slides-gpt/05-tidy-data.html#long-to-wide-1",
    "title": "05- Tidy Data",
    "section": "Long to wide",
    "text": "Long to wide\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 500 √ó 9\n   org_pac_id org_nm           measure_title CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICA‚Ä¶ CAHPS for MI‚Ä¶          63          NA          NA\n 2 0446157747 USC CARE MEDICA‚Ä¶ CAHPS for MI‚Ä¶          NA          87          NA\n 3 0446157747 USC CARE MEDICA‚Ä¶ CAHPS for MI‚Ä¶          NA          NA          86\n 4 0446157747 USC CARE MEDICA‚Ä¶ CAHPS for MI‚Ä¶          NA          NA          NA\n 5 0446157747 USC CARE MEDICA‚Ä¶ CAHPS for MI‚Ä¶          NA          NA          NA\n 6 0446157747 USC CARE MEDICA‚Ä¶ CAHPS for MI‚Ä¶          NA          NA          NA\n 7 0446162697 ASSOCIATION OF ‚Ä¶ CAHPS for MI‚Ä¶          59          NA          NA\n 8 0446162697 ASSOCIATION OF ‚Ä¶ CAHPS for MI‚Ä¶          NA          85          NA\n 9 0446162697 ASSOCIATION OF ‚Ä¶ CAHPS for MI‚Ä¶          NA          NA          83\n10 0446162697 ASSOCIATION OF ‚Ä¶ CAHPS for MI‚Ä¶          NA          NA          NA\n# ‚Ñπ 490 more rows\n# ‚Ñπ 3 more variables: CAHPS_GRP_5 &lt;dbl&gt;, CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;"
  },
  {
    "objectID": "slides-gpt/05-tidy-data.html#long-to-wide-2",
    "href": "slides-gpt/05-tidy-data.html#long-to-wide-2",
    "title": "05- Tidy Data",
    "section": "Long to wide",
    "text": "Long to wide\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 95 √ó 8\n   org_pac_id org_nm CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5 CAHPS_GRP_8\n   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC C‚Ä¶          63          87          86          57          85\n 2 0446162697 ASSOC‚Ä¶          59          85          83          63          88\n 3 0547164295 BEAVE‚Ä¶          49          NA          75          44          73\n 4 0749333730 CAPE ‚Ä¶          67          84          85          65          82\n 5 0840104360 ALLIA‚Ä¶          66          87          87          64          87\n 6 0840109864 REX H‚Ä¶          73          87          84          67          91\n 7 0840513552 SCL H‚Ä¶          58          83          76          58          78\n 8 0941545784 GRITM‚Ä¶          46          86          81          54          NA\n 9 1052612785 COMMU‚Ä¶          65          84          80          58          87\n10 1254237779 OUR L‚Ä¶          61          NA          NA          65          NA\n# ‚Ñπ 85 more rows\n# ‚Ñπ 1 more variable: CAHPS_GRP_12 &lt;dbl&gt;"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Psychometrics4Neuroscience",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nscript\n\n\n\n\n\n\nIntroduzione al corso\n\n\n¬†\n\n\n\n\nExperimental Psychology and Neuroscience\n\n\n¬†\n\n\n\n\nModern R\n\n\n¬†\n\n\n\n\nLiterate programming with Quarto\n\n\n¬†\n\n\n\n\nGit and Github\n\n\n¬†\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#paradigmi-sperimentali",
    "href": "slides/01-psychological-research/01-psychological-research.html#paradigmi-sperimentali",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Paradigmi sperimentali",
    "text": "Paradigmi sperimentali\n\nCon il prof. Maffei avete affrontato come rilevare attivit√† celebrale ad esempio con fMRI o EEG/ERP.\nSolitamente (ma non sempre) queste rilevazioni sono eseguite mentre il soggetto sperimentale esegue un compito.\nOltre al dato neurofisiologico abbiamo quindi anche sempre un dato comportamentale che viene analizzato separatamente o in relazione a quello cerebrale.\nCi sono anche casi dove ai fini della ricerca √® rilevante solo il dato comportamentale."
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#paradigmi-sperimentali-1",
    "href": "slides/01-psychological-research/01-psychological-research.html#paradigmi-sperimentali-1",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Paradigmi sperimentali",
    "text": "Paradigmi sperimentali\nQuando si parla di paradigma sperimentale, si intende un insieme di stimoli (visivi, uditivi, tattili, etc.) che vengono presentati ai soggetti.\nSolitamente i paradigmi vengono studiati e programmati nel dettaglio controllando il numero dei trials, le proprieta fisiche (durata, dimensione, etc.) degli stimoli, l‚Äôorganizzazione temporale (durata esperimento, pause, ordine degli stimoli)."
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#change-detection-task",
    "href": "slides/01-psychological-research/01-psychological-research.html#change-detection-task",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Change-detection Task",
    "text": "Change-detection Task\nQuesto compito √® utilizzato per stimare la capacit√† della memoria visiva a breve termine. Possiamo vedere un esempio pratico a questo link https://run.pavlovia.org/demos/change_detection/.\nCon questo tipo di esperimenti √® possibile stimare la quantit√† di informazione che riusciamo a memorizzare.\n\nRouder et al. (2011)"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#facial-expressions",
    "href": "slides/01-psychological-research/01-psychological-research.html#facial-expressions",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Facial expressions",
    "text": "Facial expressions\nAltri ricercatori sono pi√π interessati ad utilizzare stimoli con una valenza sociale ed evoluzionistica come i volti. C‚Äô√® moltissima ricerca riguardo il modo in cui elaboriamo volti ed espressioni facciali. Un altro esempio qui http://run.pavlovia.org/demos/emotion_rating/.\n\nM√ºnkler et al. (2015)"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#unconscious-processing",
    "href": "slides/01-psychological-research/01-psychological-research.html#unconscious-processing",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Unconscious processing",
    "text": "Unconscious processing\nAltri (tipo me) si sono interessati a come elaboriamo informazioni (ad esempio visive) in modo non consapevole o subliminale. Trovate un esempio con volti qui https://www.youtube.com/watch?v=ln-uXcC2Y_8&t=6s."
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#altri-paradigmi-da-provare",
    "href": "slides/01-psychological-research/01-psychological-research.html#altri-paradigmi-da-provare",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Altri paradigmi da provare",
    "text": "Altri paradigmi da provare\nSe volete provare altri esperimenti potete andare su https://pavlovia.org/explore?sort=DEFAULT&search=demos. Alcuni interessanti sono:\n\nStroop https://run.pavlovia.org/demos/stroop/\nDigit Span https://run.pavlovia.org/demos/digit_span/\nSemantic Priming https://run.pavlovia.org/demos/semantic_priming/"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#quali-misure",
    "href": "slides/01-psychological-research/01-psychological-research.html#quali-misure",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Quali misure?",
    "text": "Quali misure?\nSolitamente a livello comportamentale siamo interessati a:\n\ntempi di reazione/risposta: i vari software permettono di rilevare con estrema precisione la velocit√† di risposta\naccuratezza nelle varie condizioni sperimentali\nstile di risposta\n‚Ä¶"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#tempi-di-reazione",
    "href": "slides/01-psychological-research/01-psychological-research.html#tempi-di-reazione",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Tempi di reazione",
    "text": "Tempi di reazione\nAd esempio possiamo ipotizzare che una condizione sperimentale pi√π difficile richieda maggiore tempo di risposta. Spesso si parla di qualche decina di millisecondi."
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#tempi-di-reazione-1",
    "href": "slides/01-psychological-research/01-psychological-research.html#tempi-di-reazione-1",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Tempi di reazione",
    "text": "Tempi di reazione\nE possiamo anche essere interessati a stimare le differenze individuali che in Psicologia sono consistenti."
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#esperimenti-complessi",
    "href": "slides/01-psychological-research/01-psychological-research.html#esperimenti-complessi",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Esperimenti complessi",
    "text": "Esperimenti complessi\nSpesso siamo interessati non tanto ad effetti principali ma a interazioni tra condizioni sperimentali. Ad esempio, se vogliamo studiare come elaboriamo le espressioni facciali possiamo indagare:\n\nsiamo pi√π veloci e/o accurati ad elaborare volti rispetto ad altri stimoli (effetto principale della categoria)\nsiamo pi√π veloci e/o accurati ad elaborare alcune espressioni facciali rispetto ad altre? (effetto principale dell‚Äôespressione facciale)\nsiamo pi√π veloci e/o accurati ad elaborare espressioni facciali di diversa intensit√† (effetto principale dell‚Äôespressione facciale)?"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#esperimenti-complessi-1",
    "href": "slides/01-psychological-research/01-psychological-research.html#esperimenti-complessi-1",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Esperimenti complessi",
    "text": "Esperimenti complessi\nMentre alcuni effetti sono noti in letteratura (alcune emozioni sono pi√π facili da riconoscere) possiamo essere interessati a vedere se la relazione tra intensit√† ed accuratezza cambia in funzione dell‚Äôemozione (interazione):\n\nShimizu et al. (2024)"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#esperimenti-complessi-2",
    "href": "slides/01-psychological-research/01-psychological-research.html#esperimenti-complessi-2",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Esperimenti complessi",
    "text": "Esperimenti complessi\nIn questo tipo di esperimenti ci sono diverse cose da considerare:\n\nquanti trial per ogni condizione?\nquanti soggetti mi servono?\nesperimenti pi√π lunghi portano a maggiore precisione di stima ma anche maggiore stanchezza\nquali stimoli utilizzo? il tipo di stimolo pu√≤ avere un effetto?\n‚Ä¶"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#altre-misure-in-psicologia",
    "href": "slides/01-psychological-research/01-psychological-research.html#altre-misure-in-psicologia",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Altre misure in Psicologia",
    "text": "Altre misure in Psicologia"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#altre-misure-in-psicologia-1",
    "href": "slides/01-psychological-research/01-psychological-research.html#altre-misure-in-psicologia-1",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Altre misure in Psicologia",
    "text": "Altre misure in Psicologia"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#ma-anche-variabili-pi√π-complesse",
    "href": "slides/01-psychological-research/01-psychological-research.html#ma-anche-variabili-pi√π-complesse",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Ma anche variabili pi√π complesse‚Ä¶",
    "text": "Ma anche variabili pi√π complesse‚Ä¶\nDati circolari (e.g., distribuzione Von Mises) per stimare la precisione di un processo cognitivo/percettivo. Cremers and Klugkist (2018) hanno pubblicato un tutorial per dati circolari in psicologia.\n\nZhang and Luck (2008)"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#mixture-models",
    "href": "slides/01-psychological-research/01-psychological-research.html#mixture-models",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Mixture-models",
    "text": "Mixture-models\nLa situazione precedente, pu√≤ essere modellata come una mistura di due processi cognitivamente diversi, una stima ed una risposta casuale."
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#procedure-adattive",
    "href": "slides/01-psychological-research/01-psychological-research.html#procedure-adattive",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Procedure adattive",
    "text": "Procedure adattive\nIn alcune discipline (e.g., psicofisica) si utilizzano delle procedure che adattano gli stimoli presentati in base alle risposte. Questo permette di stimare in modo efficiente dei parametri di interesse.\n\nLeek (2001)"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#procedure-adattive-1",
    "href": "slides/01-psychological-research/01-psychological-research.html#procedure-adattive-1",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Procedure adattive",
    "text": "Procedure adattive\nLa procedura, detta staircase adatta la difficolt√† dell‚Äôesperimento per tenere l‚Äôaccuratezza al 50% (o altri valori)"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#procedure-adattive-2",
    "href": "slides/01-psychological-research/01-psychological-research.html#procedure-adattive-2",
    "title": "Experimental Psychology and Neuroscience",
    "section": "Procedure adattive",
    "text": "Procedure adattive\nDove questa √® la relazione vera da stimare:"
  },
  {
    "objectID": "slides/01-psychological-research/01-psychological-research.html#references",
    "href": "slides/01-psychological-research/01-psychological-research.html#references",
    "title": "Experimental Psychology and Neuroscience",
    "section": "References",
    "text": "References\n\n\n\n\nCremers, Jolien, and Irene Klugkist. 2018. ‚ÄúOne Direction? A Tutorial for Circular Data Analysis Using r with Examples in Cognitive Psychology.‚Äù Frontiers in Psychology 9 (October): 2040. https://doi.org/10.3389/fpsyg.2018.02040.\n\n\nLeek, Marjorie R. 2001. ‚ÄúAdaptive Procedures in Psychophysical Research.‚Äù Perception & Psychophysics 63 (November): 1279‚Äì92. https://doi.org/10.3758/bf03194543.\n\n\nM√ºnkler, Paula, Marcus Rothkirch, Yasmin Dalati, Katharina Schmack, and Philipp Sterzer. 2015. ‚ÄúBiased Recognition of Facial Affect in Patients with Major Depressive Disorder Reflects Clinical State.‚Äù PloS One 10 (June): e0129863. https://doi.org/10.1371/journal.pone.0129863.\n\n\nRouder, Jeffrey N, Richard D Morey, Candice C Morey, and Nelson Cowan. 2011. ‚ÄúHow to Measure Working Memory Capacity in the Change Detection Paradigm.‚Äù Psychonomic Bulletin & Review 18 (April): 324‚Äì30. https://doi.org/10.3758/s13423-011-0055-3.\n\n\nShimizu, Yunoshin, Kazumi Ogawa, Masanori Kimura, Ken Fujiwara, and Nobuyuki Watanabe. 2024. ‚ÄúThe Influence of Emotional Facial Expression Intensity on Decoding Accuracy: High Intensity Does Not Yield High Accuracy.‚Äù The Japanese Psychological Research 66 (October): 521‚Äì40. https://doi.org/10.1111/jpr.12529.\n\n\nZhang, Weiwei, and Steven J Luck. 2008. ‚ÄúDiscrete fixed-resolution representations in visual working memory.‚Äù Nature 453 (May): 233‚Äì35. https://doi.org/10.1038/nature06860."
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#literate-programming-1",
    "href": "slides/04-quarto/04-quarto.html#literate-programming-1",
    "title": "Literate programming with Quarto",
    "section": "Literate Programming1",
    "text": "Literate Programming1\n\nDonald Knuth first defined literate programming as a script, notebook, or computational document that contains an explanation of the program logic in a natural language, interspersed with snippets of macros and source code, which can be compiled and rerun\n\nFor example jupyter notebooks, R Markdown and now Quarto are literate programming frameworks to integrate code and text.\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Literate_programming"
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#literate-programming-the-markup-language",
    "href": "slides/04-quarto/04-quarto.html#literate-programming-the-markup-language",
    "title": "Literate programming with Quarto",
    "section": "Literate Programming, the markup language",
    "text": "Literate Programming, the markup language\nBeyond the coding part, the markup language is the core element of a literate programming framework. The idea of a markup language is separating the result from what you actually write. Some examples are:\n\nLaTeX\nHTML\nMarkdown\nXML\n‚Ä¶"
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#latex",
    "href": "slides/04-quarto/04-quarto.html#latex",
    "title": "Literate programming with Quarto",
    "section": "LaTeX 1",
    "text": "LaTeX 1\n\nhttps://latexbase.com/"
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#html",
    "href": "slides/04-quarto/04-quarto.html#html",
    "title": "Literate programming with Quarto",
    "section": "HTML",
    "text": "HTML\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n\n&lt;h1&gt;My First Heading&lt;/h1&gt;\n\nLorem Ipsum √® un testo segnaposto utilizzato nel settore della tipografia e della stampa. Lorem Ipsum √® considerato il testo segnaposto standard sin dal sedicesimo secolo, quando un anonimo tipografo prese una cassetta di caratteri e li assembl√≤ per preparare un testo campione. √à sopravvissuto non solo a pi√π di cinque secoli, ma anche al passaggio alla videoimpaginazione, pervenendoci sostanzialmente inalterato. Fu reso popolare, negli anni ‚Äô60, con la diffusione dei fogli di caratteri trasferibili ‚ÄúLetraset‚Äù, che contenevano passaggi del Lorem Ipsum, e pi√π recentemente da software di impaginazione come Aldus PageMaker, che includeva versioni del Lorem Ipsum.\n\n&lt;h2&gt;My Second Heading&lt;/h2&gt;\n\nLorem Ipsum √® un testo segnaposto utilizzato nel settore della tipografia e della stampa. \n\nLorem Ipsum √® considerato il testo segnaposto standard sin dal sedicesimo secolo, quando un anonimo \n\ntipografo prese una cassetta di caratteri e li assembl√≤ per preparare un testo campione. \n\n√à sopravvissuto non solo a pi√π di cinque secoli, ma anche al passaggio alla videoimpaginazione, pervenendoci sostanzialmente inalterato. \n\nFu reso popolare, negli anni ‚Äô60, con la diffusione dei \n\nfogli di caratteri trasferibili ‚ÄúLetraset‚Äù, che contenevano passaggi del Lorem Ipsum\n\npi√π recentemente da software di impaginazione come Aldus PageMaker, che includeva versioni del Lorem Ipsum.\n\n&lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#markdown",
    "href": "slides/04-quarto/04-quarto.html#markdown",
    "title": "Literate programming with Quarto",
    "section": "Markdown1",
    "text": "Markdown1\n\n\nhttps://markdownlivepreview.com/"
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#markdown-1",
    "href": "slides/04-quarto/04-quarto.html#markdown-1",
    "title": "Literate programming with Quarto",
    "section": "Markdown",
    "text": "Markdown\nMarkdown is one of the most popular markup languages for several reasons:\n\neasy to write and read compared to Latex and HTML\neasy to convert from Markdown to basically every other format using pandoc\neasy to implement new features"
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#markdown-source-code",
    "href": "slides/04-quarto/04-quarto.html#markdown-source-code",
    "title": "Literate programming with Quarto",
    "section": "Markdown (source code)",
    "text": "Markdown (source code)\n## Markdown\n\nMarkdown is one of the most popular markup languages for several reasons:\n\n- easy to write and read compared to Latex and HTML\n- easy to convert from Markdown to basically every other format using `pandoc`\n- easy to implement new features\n\nAlso the source code can be used, compared to Latex or HTML, to take notes and read. Latex and HTML need to be compiled otherwise they are very hard to read."
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#whats-wrong-about-microsoft-word",
    "href": "slides/04-quarto/04-quarto.html#whats-wrong-about-microsoft-word",
    "title": "Literate programming with Quarto",
    "section": "What‚Äôs wrong about Microsoft Word?",
    "text": "What‚Äôs wrong about Microsoft Word?\nMS Word is a WYSIWYG (what you see is what you get editor) that force users to think about formatting, numbering, etc. Markup languages receive the content (plain text) and the rules and creates the final document."
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#whats-wrong-about-microsoft-word-1",
    "href": "slides/04-quarto/04-quarto.html#whats-wrong-about-microsoft-word-1",
    "title": "Literate programming with Quarto",
    "section": "What‚Äôs wrong about Microsoft Word?",
    "text": "What‚Äôs wrong about Microsoft Word?\nBeyond the pure writing process, there are other aspects related to research data.\n\n\nwriting math formulas\nreporting statistics in the text\nproducing tables\nproducing plots\n\n\nIn MS Word (or similar) we need to produce everything outside and then manually put figures and tables."
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#the-solution-quarto",
    "href": "slides/04-quarto/04-quarto.html#the-solution-quarto",
    "title": "Literate programming with Quarto",
    "section": "The solution‚Ä¶ Quarto",
    "text": "The solution‚Ä¶ Quarto\nQuarto (https://quarto.org/) is the evolution of R Markdown that integrate a programming language with the Markdown markup language. It is very simple but quite powerful."
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#basic-markdown",
    "href": "slides/04-quarto/04-quarto.html#basic-markdown",
    "title": "Literate programming with Quarto",
    "section": "Basic Markdown",
    "text": "Basic Markdown\nMarkdown can be learned in minutes. You can go to the following link https://quarto.org/docs/authoring/markdown-basics.html and try to understand the syntax."
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#more-about-quarto-and-r-markdown",
    "href": "slides/04-quarto/04-quarto.html#more-about-quarto-and-r-markdown",
    "title": "Literate programming with Quarto",
    "section": "More about Quarto and R Markdown",
    "text": "More about Quarto and R Markdown\nThe topic is extremely vast. You can do everything in Quarto, a website, thesis, your CV, etc.\n\nYihui Xie - R Markdown Cookbook https://bookdown.org/yihui/rmarkdown-cookbook/\nYihui Xie - R Markdown: The Definitive Guide https://bookdown.org/yihui/rmarkdown/\nQuarto documentation https://quarto.org/docs/guide/"
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#writing-papers-papaja",
    "href": "slides/04-quarto/04-quarto.html#writing-papers-papaja",
    "title": "Literate programming with Quarto",
    "section": "Writing papers, papaja",
    "text": "Writing papers, papaja\n\nhttps://github.com/crsh/papaja"
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#writing-papers-apaquarto",
    "href": "slides/04-quarto/04-quarto.html#writing-papers-apaquarto",
    "title": "Literate programming with Quarto",
    "section": "Writing papers, apaquarto",
    "text": "Writing papers, apaquarto\n\nhttps://github.com/wjschne/apaquarto"
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#collaborating-tbh-not-so-easy",
    "href": "slides/04-quarto/04-quarto.html#collaborating-tbh-not-so-easy",
    "title": "Literate programming with Quarto",
    "section": "Collaborating! (TBH not so easy)",
    "text": "Collaborating! (TBH not so easy)\nThe trackdown package can be used to collaborate on Rmd or qmd documents using Google Docs.\n\nhttps://github.com/ClaudioZandonella/trackdown"
  },
  {
    "objectID": "slides/04-quarto/04-quarto.html#collaborating-overleaf",
    "href": "slides/04-quarto/04-quarto.html#collaborating-overleaf",
    "title": "Literate programming with Quarto",
    "section": "Collaborating! Overleaf",
    "text": "Collaborating! Overleaf\nWith Overleaf you can collaborate on .tex documents but also .Rnw documents. No Rmd or qmd unfortunately. See an example document."
  }
]