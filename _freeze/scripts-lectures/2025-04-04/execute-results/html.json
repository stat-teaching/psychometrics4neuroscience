{
  "hash": "57eb1afe4c27ab5ae2171e9fd784a8c3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"2025-04-04\"\nformat: html\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(glmmTMB)\nlibrary(lmerTest)\n\n# fittare un modello specificato con formula per ogni id (|cluster)\n# model = lm/glm (o altro)\n# args = altri argomenti dentro la funzione (e.g., family = )\n# ad esempio, per stimare l'effetto di x1 + x2 per ogni cluster\n# y ~ x1 + x2 | cluster\n\nfit_by_cluster <- function(formula, data, model = NULL, args = NULL){\n    if(is.null(model)){\n        model <- lm\n    }\n    \n    parts <- lme4:::modelFormula(formula)\n    groups <- as.character(parts$groups)\n    datal <- split(data, data[[groups]])\n    \n    args$formula <- parts$model\n    \n    lapply(datal, function(x){\n        do.call(model, args = c(args, list(data = x)))\n    })\n}\n\n\ndat <- readRDS(here(\"data/emoint.rds\"))\ndat <- filter(dat, emotion_lbl != \"neutral\")\ndat$intensity0 <- (dat$intensity/10) - 1\n\nfit1 <- glmer(acc ~ intensity0 + (intensity0|id),\n             data = dat, \n             family = binomial(link = \"logit\"))\n\nfit10 <- glmer(acc ~ 1 + (1|id),\n              data = dat, \n              family = binomial(link = \"logit\"))\n\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: acc ~ intensity0 + (intensity0 | id)\n   Data: dat\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n  30742.3   30783.2  -15366.2   30732.3     26265 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.2930 -0.7394  0.3993  0.7623  2.1703 \n\nRandom effects:\n Groups Name        Variance Std.Dev. Corr \n id     (Intercept) 0.069921 0.26443       \n        intensity0  0.007283 0.08534  -0.43\nNumber of obs: 26270, groups:  id, 71\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.09351    0.04058  -26.94   <2e-16 ***\nintensity0   0.33490    0.01148   29.17   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr)\nintensity0 -0.545\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeff <- coefficients(fit1)$id\nhist(exp(coeff$intensity0))\n```\n\n::: {.cell-output-display}\n![](2025-04-04_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(coeff$intensity0)\n```\n\n::: {.cell-output-display}\n![](2025-04-04_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(fitted(fit1), residuals(fit1, type = \"response\"))\n```\n\n::: {.cell-output-display}\n![](2025-04-04_files/figure-html/unnamed-chunk-1-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# influence(fit1) # ci mette molto\n\ndat_agg <- dat |> \n    group_by(id, intensity0) |> \n    summarise(nc = sum(acc),\n              nf = n() - nc,\n              p = nc / n(),\n              n = n())\n\n# aggregated vs binary model\n\nfit2 <- glmer(cbind(nc, nf) ~ intensity0 + (intensity0|id),\n             data = dat_agg,\n             family = binomial(link = \"logit\"))\n\nfit20 <- glmer(cbind(nc, nf) ~ 1 + (1|id),\n              data = dat_agg,\n              family = binomial(link = \"logit\"))\n\ncar::compareCoefs(fit1, fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCalls:\n1: glmer(formula = acc ~ intensity0 + (intensity0 | id), data = dat, family \n  = binomial(link = \"logit\"))\n2: glmer(formula = cbind(nc, nf) ~ intensity0 + (intensity0 | id), data = \n  dat_agg, family = binomial(link = \"logit\"))\n\n            Model 1 Model 2\n(Intercept) -1.0935 -1.0935\nSE           0.0406  0.0406\n                           \nintensity0   0.3349  0.3349\nSE           0.0115  0.0115\n                           \n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit20, fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: dat_agg\nModels:\nfit20: cbind(nc, nf) ~ 1 + (1 | id)\nfit2: cbind(nc, nf) ~ intensity0 + (intensity0 | id)\n      npar     AIC   BIC  logLik -2*log(L)  Chisq Df Pr(>Chisq)    \nfit20    2 10100.1 10109 -5048.0   10096.1                         \nfit2     5  5356.2  5379 -2673.1    5346.2 4749.9  3  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit10, fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: dat\nModels:\nfit10: acc ~ 1 + (1 | id)\nfit1: acc ~ intensity0 + (intensity0 | id)\n      npar   AIC   BIC logLik -2*log(L)  Chisq Df Pr(>Chisq)    \nfit10    2 35486 35503 -17741     35482                         \nfit1     5 30742 30783 -15366     30732 4749.9  3  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# nella forma binomial, i residui sono leggermente meglio\n\nplot(fitted(fit2), residuals(fit2, type = \"response\"))\n```\n\n::: {.cell-output-display}\n![](2025-04-04_files/figure-html/unnamed-chunk-1-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# vediamo la variabilità nelle interazioni\n\nff <- fit_by_cluster(acc ~ emotion_lbl * intensity0 | id,\n               data = dat,\n               model = glm,\n               args = list(family = binomial))\n\nff |> \n    lapply(broom::tidy) |> \n    bind_rows(.id = \"id\") |> \n    filter(grepl(\":\", term)) |> \n    filter(abs(estimate) < 5) |> \n    ggplot(aes(x = estimate, y = id)) +\n    geom_point() +\n    facet_wrap(~term) \n```\n\n::: {.cell-output-display}\n![](2025-04-04_files/figure-html/unnamed-chunk-1-5.png){width=672}\n:::\n\n```{.r .cell-code}\nto_remove <- ff |> \n    lapply(broom::tidy) |> \n    bind_rows(.id = \"id\") |> \n    filter(grepl(\":\", term)) |> \n    filter(abs(estimate) > 5) |> \n    pull(id) |> \n    unique()\n```\n:::\n\n\n\n\nAbbiamo inoltre visto alcuni esempi di Simpson's Paradox (non nella sua forma più estrema) e come centrare le variabili in modo da separare l'effetto tra i cluster e dentro i cluster:\n\n- Esempio con esperimento simulato di sensibilità al contrasto [qmd](2025-04-04_extra/2025-04-04_esempio-simpson-paradox.qmd)\n",
    "supporting": [
      "2025-04-04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}