{
  "hash": "11aabf2ce92787dd2dd44a229d6a86f8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Generalized Linear Mixed-Effects Models\"\nbibliography: \"https://raw.githubusercontent.com/filippogambarota/bib-database/main/references.bib\"\ntoc-depth: 1\n---\n\n\n\n\n\n\n## Notation\n\n$$\n\\mathbf{y}_{N \\times 1} = \\mathbf{X}_{N \\times p} \\boldsymbol{\\beta}_{p \\times 1} + \\sum_{i=1}^{m} \\mathbf{Z}_i^{(N \\times q_i)} \\mathbf{b}_i^{(q_i \\times 1)} + \\boldsymbol{\\varepsilon}_{N \\times 1}\n$$\n\nWhere $N$ is the number of observations, $p$ is the number of predictors, $q$ is the number of clusters (e.g., participants) and $m$ is the number of random effects (e.g., nested or crossed).\n\n## Visualizing the $\\mathbf{Z}$ matrix\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat <- sleepstudy\nZ <- get_Z_matrix(~ Days + (1|Subject), dat)\nrownames(Z) <- NULL\ncolnames(Z) <- NULL\n\nreshape2::melt(Z) |>\n  ggplot(aes(x = Var1, y = Var2, fill = factor(value), color = factor(value))) +\n  geom_tile(show.legend = FALSE) +\n  scale_fill_manual(values = c(\"transparent\", scales::alpha(\"black\", 0.5))) +\n  scale_color_manual(values = c(\"transparent\", \"black\")) +\n  theme_bw(20) +\n  theme(panel.grid = element_blank(),\n        aspect.ratio = 1) +\n  ylab(latex2exp::TeX(\"$Cluster_q$\")) +\n  xlab(latex2exp::TeX(\"$Observation_i$\"))\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Why clustered data in Psychology?\n\nIn Psychology and Neuroscience we (almost) always have clustered data. For example:\n\n- Childrens nested within classrooms (maybe nested within schools)\n- Trials of a cognitive experiments nested within participants\n- ...\n\nThe main point is that, clustered observations are not independent and we want to take into account the correlation.\n\n## Example with `lme4::sleepstudy`\n\nA very simple example is the `lme4::sleepstudy` where participants reaction times where evaluated under sleep deprivation.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat <- lme4::sleepstudy\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   Reaction Days Subject\n#> 1 249.5600    0     308\n#> 2 258.7047    1     308\n#> 3 250.8006    2     308\n#> 4 321.4398    3     308\n#> 5 356.8519    4     308\n#> 6 414.6901    5     308\n```\n\n\n:::\n:::\n\n\n\n## Overall model\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point(position = position_jitter(width = 0.1)) +\n  scale_x_continuous(breaks = unique(dat$Days)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  ggtitle(\"Linear Model (ignore dependency)\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## By-participant model\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point(position = position_jitter(width = 0.1)) +\n  facet_wrap(~Subject) +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## By-participant model\n\nFrom the by-participant models, we see a clear dependency. Observations within the same participant are more similar compared to observations across participant.\n\nIn addition, at Day 0, some participants have higher/lower reaction times compared to the overall trend. Similarly, some participants have higher/lower slopes.\n\n**Individual differences are the core of Psychology** and we want to explictly model them!\n\n## Individual differences\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = Days, y = Reaction, group = Subject)) +\n  geom_point(position = position_jitter(width = 0.1)) +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Are the observations clustered?\n\nWe can start assessing the clustering structure by fitting a mixed-model with only the random-intercepts and calculating the intraclass-correlation.\n\n$$\ny_{ij} = \\beta_0 + \\beta_{0_i} + \\epsilon_{ij}\n$$\n\n## Are the observations clustered?\n\nThe model can be fitted with the `lme4::lmer()` function:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit0 <- lmer(Reaction ~ 1 + (1|Subject), data = dat)\nsummary(fit0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: Reaction ~ 1 + (1 | Subject)\n#>    Data: dat\n#> \n#> REML criterion at convergence: 1904.3\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -2.4983 -0.5501 -0.1476  0.5123  3.3446 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  Subject  (Intercept) 1278     35.75   \n#>  Residual             1959     44.26   \n#> Number of obs: 180, groups:  Subject, 18\n#> \n#> Fixed effects:\n#>             Estimate Std. Error t value\n#> (Intercept)   298.51       9.05   32.98\n```\n\n\n:::\n:::\n\n\n\n## Are the observations clustered?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# using the insight::get_variance() function\nvv <- insight::get_variance(fit0)\nvv$var.intercept / (vv$var.intercept + vv$var.residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   Subject \n#> 0.3948896\n```\n\n\n:::\n\n```{.r .cell-code}\n# or directly with performance::icc()\nperformance::icc(fit0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # Intraclass Correlation Coefficient\n#> \n#>     Adjusted ICC: 0.395\n#>   Unadjusted ICC: 0.395\n```\n\n\n:::\n:::\n\n\n\nThus roughly 39% of the variance is explained by the clustering structure.\n\n## Are the observations clustered?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = Subject, y = Reaction)) +\n  #geom_point(position = position_jitter(width = 0.1))\n  geom_boxplot(fill = \"dodgerblue\") +\n  ggtitle(\"ICC = 39%\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Are the observations clustered\n\nWe can remove the subject-specific effect.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  mutate(Reaction_cmc = cmc(Reaction, Subject)) |> \n  ggplot(aes(x = Subject, y = Reaction_cmc)) +\n  #geom_point(position = position_jitter(width = 0.1))\n  geom_boxplot(fill = \"dodgerblue\") +\n  ylab(\"Reaction (cluster-mean centered)\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## How different ICCs appear...\n\nWe can simulate some datasets with different ICC:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nicc <- c(0, 0.3, 0.8)\nsb0 <- sqrt(icc)\n\nb0 <- 0\n\nns <- 10\nnt <- 10\n\nid <- rep(1:ns, each = nt)\n\ny <- vector(mode = \"list\", length = length(icc))\n\nfor(i in 1:length(icc)){\n  b0i <- rnorm(ns, 0, sb0[i])\n  y[[i]] <- b0 + b0i[id] + rnorm(ns * nt, 0, 1 - sb0[i])\n}\n\ndd <- data.frame(\n  id = rep(id, length(sb0)),\n  sb0 = rep(sb0, each = ns * nt),\n  icc = rep(icc, each = ns * nt),\n  y = unlist(y)\n)\n\ndd |> \n  mutate(icc = sprintf(\"ICC = %s\", icc),\n         icc = factor(icc, levels = c(\"ICC = 0\", \"ICC = 0.3\",\"ICC = 0.8\"))) |> \n  ggplot(aes(x = factor(id), y = y)) +\n  geom_boxplot() +\n  facet_wrap(~icc) +\n  xlab(\"Cluster\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Psychological interpretation of random intercepts\n\nThe random-intercepts are intepreted as baseline variation in the experiment. For example:\n\n- variability at time 0\n- variability pre treatment\n- variability for the reference condition\n- ...\n\nFurthemore, the ICC (that is related to the random-intercepts variance) affects the statistical power of the model.\n\n## Statistical power and ICC\n\nWe will see how to simulate data in a meaningful way, but here just an example on the impact of ICC on the statistical power.\n\nI estimate the statistical power (for an intercept-only model) using the analytical method by @Hedges2001-ra.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npowerICC <- function(nc, ns, d, icc, alpha = 0.05){\n  tau2 <- icc\n  vi <- 1/ns + 1/ns\n  v <- (vi + tau2)/nc\n  z <- d / sqrt(v)\n  zc <- abs(qnorm(alpha/2))\n  1 - pnorm(zc - z) + pnorm(-zc - z)\n}\n```\n:::\n\n\n\n## Statistical power and ICC\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim <- expand.grid(\n  nc = c(1, seq(5, 50, 10)),\n  ns = 20,\n  d = 0.3,\n  icc = c(0, 0.3, 0.5, 0.8)\n) \n\nsim$power <- with(sim, powerICC(nc, ns, d, icc))\n\nsim |> \n  ggplot(aes(x = icc, y = power, color = factor(nc))) +\n  geom_line() +\n  labs(color = \"Clusters\") +\n  xlab(\"ICC\") +\n  ylab(\"Power\") +\n  ggtitle(\"N = 20 (per cluster), d = 0.3\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## ICC in Psychology^[This is just an approximation to estimate the impact of the ICC]\n\nIn Psychology it is common to collect clustered data and the data collection is usually time expensive. In addition, sample sizes are usually lower than the optimal level according to power calculation. @Rao1992-nl defined the concept of *effective sample size* for the reduction in the sample size (and thus power) according to the ICC in clustered data.\n\n$$\nN_{\\text{eff}} = \\frac{N}{1 + (\\bar k - 1) \\rho}\n$$\nWhere $\\bar k$ is the average number of observations per cluster.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nneff <- function(N, khat, icc){\n  N / (1 + (khat - 1) * icc)\n}\n```\n:::\n\n\n\n## ICC in Psychology\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nN <- 100\nkhat <- 10\nicc <- seq(0, 1, 0.01)\n\nqplot(icc, neff(N, khat, icc), type = \"l\", lwd = 1)\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Adding the fixed effect\n\nThen we can add the fixed effect of `Days`. The variable is numeric starting with 0 up to 5 days. We can just add the variable as it is.\n\n\n\n\n \\small\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit1 <- lmer(Reaction ~ Days + (1|Subject), data = dat)\n# equivalent to\n# fit1 <- update(fit0, . ~ . + Days)\n\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: Reaction ~ Days + (1 | Subject)\n#>    Data: dat\n#> \n#> REML criterion at convergence: 1786.5\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.2257 -0.5529  0.0109  0.5188  4.2506 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  Subject  (Intercept) 1378.2   37.12   \n#>  Residual              960.5   30.99   \n#> Number of obs: 180, groups:  Subject, 18\n#> \n#> Fixed effects:\n#>             Estimate Std. Error t value\n#> (Intercept) 251.4051     9.7467   25.79\n#> Days         10.4673     0.8042   13.02\n#> \n#> Correlation of Fixed Effects:\n#>      (Intr)\n#> Days -0.371\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n## Adding the fixed effect\n\nThis means that for each day we have an expected increase in reaction times of 10.47 milliseconds (or 0.01 seconds).\n\nWe have only the random intercept for subjects, thus we are assuming that each subject has the same sleep deprivation effect but can have different baseline reaction times.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(coef(fit1)$Subject)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>     (Intercept)     Days\n#> 308    292.1888 10.46729\n#> 309    173.5556 10.46729\n#> 310    188.2965 10.46729\n#> 330    255.8115 10.46729\n#> 331    261.6213 10.46729\n#> 332    259.6263 10.46729\n```\n\n\n:::\n\n```{.r .cell-code}\n# equivalent to fixef(fit1)[\"(Intercept)\"] + ranef(fit1)$Subject for the random intercept\n```\n:::\n\n\n\n## Adding the fixed effect\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoef(fit1)$Subject |> \n  mutate(Subject = 1:n()) |> \n  mutate(b0 = fixef(fit1)[1],\n         b1 = fixef(fit1)[2]) |> \n  rename(\"b0i\" = `(Intercept)`,\n         \"b1i\" = `Days`) |> \n  expand_grid(Days = unique(dat$Days)) |> \n  mutate(pi = b0i + b1i * Days,\n         p = b0 + b1 * Days) |> \n  ggplot(aes(x = Days, y = pi)) +\n  geom_line(aes(group = Subject),\n            alpha = 0.5) +\n  geom_line(aes(x = Days, y = p),\n            lwd = 1.5,\n            col = \"firebrick\") +\n  ylab(\"Reaction\")\n```\n:::\n\n\n\n## Adding the fixed effect\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoef(fit1)$Subject |> \n  mutate(Subject = 1:n()) |> \n  mutate(b0 = fixef(fit1)[1],\n         b1 = fixef(fit1)[2]) |> \n  rename(\"b0i\" = `(Intercept)`,\n         \"b1i\" = `Days`) |> \n  expand_grid(Days = unique(dat$Days)) |> \n  mutate(pi = b0i + b1i * Days,\n         p = b0 + b1 * Days) |> \n  ggplot(aes(x = Days, y = pi)) +\n  geom_line(aes(group = Subject),\n            alpha = 0.5) +\n  geom_line(aes(x = Days, y = p),\n            lwd = 1.5,\n            col = \"firebrick\") +\n  ylab(\"Reaction\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Adding the fixed effect\n\nThe same can be achieved using:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nDD <- expand_grid(\n  Subject = unique(dat$Subject),\n  Days = unique(dat$Days)\n)\n\nDD$pi <- predict(fit1, newdata = DD)\n\nhead(DD)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 3\n#>   Subject  Days    pi\n#>   <fct>   <dbl> <dbl>\n#> 1 308         0  292.\n#> 2 308         1  303.\n#> 3 308         2  313.\n#> 4 308         3  324.\n#> 5 308         4  334.\n#> 6 308         5  345.\n```\n\n\n:::\n:::\n\n\n\n## Is the fixed effect enough?\n\nA big problem with mixed models is that effects can be included both as fixed and random and the choice is not always easy. From the plots at the beginning there is a clear variability in slopes that the model is ignoring.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit2 <- lmer(Reaction ~ Days + (Days|Subject), data = dat)\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: Reaction ~ Days + (Days | Subject)\n#>    Data: dat\n#> \n#> REML criterion at convergence: 1743.6\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.9536 -0.4634  0.0231  0.4634  5.1793 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev. Corr\n#>  Subject  (Intercept) 612.10   24.741       \n#>           Days         35.07    5.922   0.07\n#>  Residual             654.94   25.592       \n#> Number of obs: 180, groups:  Subject, 18\n#> \n#> Fixed effects:\n#>             Estimate Std. Error t value\n#> (Intercept)  251.405      6.825  36.838\n#> Days          10.467      1.546   6.771\n#> \n#> Correlation of Fixed Effects:\n#>      (Intr)\n#> Days -0.138\n```\n\n\n:::\n:::\n\n\n\n## Is the fixed effect enough?\n\nThe first important part is the estimation of the random part. Clearly the random slopes variance is not zero.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfilter_output(summary(fit2), c(\"^Random effects|^Number of obs\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Random effects:\n#>  Groups   Name        Variance Std.Dev. Corr\n#>  Subject  (Intercept) 612.10   24.741       \n#>           Days         35.07    5.922   0.07\n#>  Residual             654.94   25.592       \n#> Number of obs: 180, groups:  Subject, 18\n```\n\n\n:::\n:::\n\n\n\n## Is the fixed effect enough?\n\nOne of the most important part is that the standard error of the fixed coefficients is affected by the inclusion of the random slopes. Omitting the slopes was underestimating the standard error.\n\nqualcosa su barr, keep it maximal, etc.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncar::compareCoefs(fit1, fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Calls:\n#> 1: lmer(formula = Reaction ~ Days + (1 | Subject), data = dat)\n#> 2: lmer(formula = Reaction ~ Days + (Days | Subject), data = dat)\n#> \n#>             Model 1 Model 2\n#> (Intercept)  251.41  251.41\n#> SE             9.75    6.82\n#>                            \n#> Days         10.467  10.467\n#> SE            0.804   1.546\n#> \n```\n\n\n:::\n:::\n\n\n\n## Is the fixed effect enough?\n\nWe can formally compare the models using a Likelihood Ratio Test (LRT)^[Note that the `anova()` function is refitting models using Maximum Likelihood (and not REML). This is required to compare models using LRT]:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova(fit1, fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Data: dat\n#> Models:\n#> fit1: Reaction ~ Days + (1 | Subject)\n#> fit2: Reaction ~ Days + (Days | Subject)\n#>      npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(>Chisq)    \n#> fit1    4 1802.1 1814.8 -897.04    1794.1                         \n#> fit2    6 1763.9 1783.1 -875.97    1751.9 42.139  2  7.072e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# or ranova(refitML(fit2))\n```\n:::\n\n\n\n<!-- TODO vedi se ranova richiede di fare refit ml -->\n\n## Plotting the random slopes\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nDD <- expand_grid(\n  Subject = unique(dat$Subject),\n  Days = unique(dat$Days)\n)\n\nDD$pi <- predict(fit2, newdata = DD)\n\nDD |> \n  ggplot(aes(x = Days, y = pi, group = Subject)) +\n  geom_line() +\n  xlab(\"Days\") +\n  ylab(\"Reaction\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-24-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Shrinkage!\n\nWe can compare the fit of the multilevel model with linear models for each cluster. We can use the `fit_by_cluster()` function tha takes a formula, a grouping factor and fit a model for each cluster.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfitl <- fit_by_cluster(Reaction ~ Days | Subject,\n                       dat,\n                       model = lm)\n```\n:::\n\n\n\n## Shrinkage!\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfitl <- fit_by_cluster(Reaction ~ Days | Subject,\n                       dat,\n                       model = lm)\n\nDD <- expand_grid(\n  Subject = unique(dat$Subject),\n  Days = unique(dat$Days)\n)\n\nDD$lmer <- predict(fit2, DD)\nDD$Subject <- as.numeric(as.character(DD$Subject))\n\nsubjs <- unique(DD$Subject)\nDD$lm <- NA\n\nfor(i in 1:length(fitl)){\n  pp <- predict(fitl[[i]], DD[DD$Subject == subjs[i], ])\n  DD$lm[DD$Subject == subjs[i]] <- pp\n}\n\nDD |> \n  pivot_longer(c(lmer, lm)) |> \n  ggplot(aes(x = Days, y = value, color = name)) +\n  geom_line(lwd = 1) +\n  facet_wrap(~Subject) +\n  ylab(\"Reaction\") +\n  labs(color = \"Method\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-26-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n# Let's simulate a multilevel model!\n\n## Let's try to simulate the previous model\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nN <- 10\nDAYS <- 10 # max number of days\nb00 <- 300 # grand-mean of reaction times at time 0\nb1 <- 20  # increase in reaction time for each day\nsb0 <- 30 # standard deviation intercepts\nsb1 <- 10 # standard deviaton slopes\ns <- 100 # residual standard deviation\n\n# we are simulating an ICC of\nsb0^2 / (sb0^2 + s^2)\n#> [1] 0.08256881\n\nsim <- expand_grid(\n  id = 1:N,\n  days = 0:(DAYS - 1)\n)\n\n# random intercepts and slopes, rho = 0\n\nR <- 0 + diag(1 - 0, 2)\nVCOV <- diag(c(sb0, sb1)) %*% R %*% diag(c(sb0, sb1))\n\nRE <- MASS::mvrnorm(N, c(0, 0), VCOV)\n\nb0i <- RE[, 1]\nb1i <- RE[, 2]\n\n# linear predictor\nsim$lp <- with(sim, b00 + b0i[id] + (b1 + b1i[id]) * days)\nsim$rt <- rnorm(nrow(sim), sim$lp, s)\n```\n:::\n\n \\normalsize\n\n\n\n## Let's try to simulate the previous model\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim |> \n  ggplot(aes(x = rt)) +\n  geom_histogram(color = \"black\",\n                 fill = \"dodgerblue\") +\n  xlab(\"Reaction Times\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Let's try to simulate the previous model\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim |> \n  ggplot(aes(x = days, y = rt)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:DAYS) +\n  xlab(\"Days\") +\n  ylab(\"Reaction Times\") +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              aes(group = id))\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Let's try to simulate the previous model\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim |> \n  ggplot(aes(x = days, y = rt)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 0:DAYS) +\n  xlab(\"Days\") +\n  ylab(\"Reaction Times\") +\n  facet_wrap(~id, ncol = 4) +\n  geom_smooth(method = \"lm\",\n              se = FALSE)\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Let's try to simulate the previous model\n\nA more realistic scenario could be to have an heterogeneous number of observations for each cluster.\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# minimum 3 observations\nndays_per_subject <- sample(3:(DAYS - 1), size = N, replace = TRUE)\nsiml <- split(sim, sim$id)\n\nfor(i in 1:length(siml)){\n  siml[[i]] <- siml[[i]][1:ndays_per_subject[i], ]\n}\n\nsim_missing <- do.call(rbind, siml)\n\n# number of observations\ntapply(sim_missing$rt, sim_missing$id, length)\n#>  1  2  3  4  5  6  7  8  9 10 \n#>  8  3  3  6  9  5  4  5  7  4\n```\n:::\n\n \\normalsize\n\n\n\n## Let's try to simulate the previous model\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim_missing |> \n  ggplot(aes(x = days, y = rt)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 0:DAYS) +\n  xlab(\"Days\") +\n  ylab(\"Reaction Times\") +\n  facet_wrap(~id, ncol = 4) +\n  geom_smooth(method = \"lm\",\n              se = FALSE)\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Let's fit the model\n\nHere the random-intercepts and slopes model:\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- lmer(rt ~ days + (days|id), data = sim_missing)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: rt ~ days + (days | id)\n#>    Data: sim_missing\n#> \n#> REML criterion at convergence: 650\n#> \n#> Scaled residuals: \n#>      Min       1Q   Median       3Q      Max \n#> -2.10251 -0.66077  0.00732  0.63516  2.01985 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev. Corr \n#>  id       (Intercept)  3543.5   59.53        \n#>           days          451.5   21.25   -0.77\n#>  Residual             10994.3  104.85        \n#> Number of obs: 54, groups:  id, 10\n#> \n#> Fixed effects:\n#>             Estimate Std. Error t value\n#> (Intercept)   313.18      30.20  10.369\n#> days           12.14      11.03   1.101\n#> \n#> Correlation of Fixed Effects:\n#>      (Intr)\n#> days -0.761\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n# Can the multilevel model be misleading?\n\n## What do you see in this plot?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 10 # number of clusters/units\nn <- 100 # number of participants within each unit\nN <- n * k # total sample size\nunit <- rep(1:k, each = n)\n\nage <- rnorm(N, 60 - 3 * (unit - 1), 5) # age equation\ny <- 0 + rnorm(k, 0, 0.1)[unit] + 0.01 * age + 0.1 * (unit-1) + rnorm(N, 0, 0.1) # response equation\ndat <- data.frame(unit, age, y)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = age, y = y)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-35-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n. . .\n\nThere is a clear negative relationship between `age` and the response variable `y`!\n\n## What do you see in this plot?\n\nLet's add the cluster information. What about now?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = age, y = y)) +\n  geom_point(aes(color = factor(unit))) +\n  labs(color = \"Cluster\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-36-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Simpson's paradox\n\n- We have clustered data, and the relationship `y ~ age` seems to be different within and between clusters.\n- This phenomenon is called **Simpson's paradox** and can be a serious problem in multilevel models.\n- Clearly this is a problem only for variables at the observation level (not at the cluster level).\n- For example, if clusters are schools and the observations are children. `age` is a variable at the children level (or aggregated at the school level). On the other side, the prestige of the school is a variable at the school level (the same for each child)\n\n## Simpson's paradox\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = age, y = y)) +\n  geom_point(aes(color = factor(unit))) +\n  labs(color = \"Cluster\") + \n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              col = \"black\") +\n  geom_smooth(method = \"lm\",\n              aes(group = unit,\n                  color = factor(unit)),\n              se = FALSE)\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Simpson's paradox, what to do?\n\nThe main strategy to deal with the Simpson's paradox is centering the variables. @Enders2007-dk provide a clear overview of the strategy.\n\n## Simpson's paradox, what to do?\n\nWe can define some centering functions and see how we can model the multilevel structure.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfilor::print_fun(c(funs$cm, funs$cmc, funs$gmc))\n```\n\n```r\ncm <- function(x, cluster){\n  cm <- tapply(x, cluster, mean)\n  cm[cluster]\n}\ncmc <- function(x, cluster){\n  x - cm(x, cluster)\n}\ngmc <- function(x){\n  x - mean(x)\n}\n```\n:::\n\n\n\n## Simpson's paradox, between-clusters effect\n\nFirsly we can calculate the clusters mean. This remove the within-effect and a linear model will capture only the between-effect.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  mutate(age_cm = cm(age, unit),\n         y_cm = cm(y, unit)) |> \n  ggplot(aes(x = age, y = y)) +\n  geom_point(alpha = 0.2,\n             aes(color = factor(unit))) +\n  geom_point(aes(x = age_cm, y = y_cm,\n                 color = factor(unit)),\n             alpha = 1,\n             size = 4) +\n  labs(color = \"Unit\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-39-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Simpson's paradox, between-clusters effect\n\nWe have a negative `age` effect between clusters, as expected.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat_cm <- dat |> \n  group_by(unit) |> \n  summarise(y_cm = mean(y),\n            age_cm = mean(age))\n\nlm(y_cm ~ age_cm, data = dat_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = y_cm ~ age_cm, data = dat_cm)\n#> \n#> Coefficients:\n#> (Intercept)       age_cm  \n#>     1.92527     -0.02175\n```\n\n\n:::\n:::\n\n\n\n## Simpson's paradox, within-clusters effect\n\nWe can substract (i.e., centering) from each observation, the cluster mean.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  mutate(age_cm = cm(age, unit),\n         age_cmc = cmc(age, unit),\n         y_cm = cm(y, unit)) |> \n  ggplot(aes(x = age_cmc, y = y)) +\n  geom_point(alpha = 0.2,\n             aes(color = factor(unit))) +\n  xlab(latex2exp::TeX(\"$age - \\\\; \\\\bar{age_k}$\")) +\n  geom_point(aes(x = 0, y = y_cm,\n                 color = factor(unit))) +\n  geom_smooth(method = \"lm\",\n              aes(group = unit,\n                  color = factor(unit)),\n              se = FALSE) +\n  labs(color = \"Unit\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-41-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Simpson's paradox, within-clusters effect\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$age_cmc <- cmc(dat$age, dat$unit)\n\nfitl_cmc <- fit_by_cluster(\n  y ~ age_cmc | unit,\n  data = dat,\n  model = lm\n)\n```\n:::\n\n\n\n## Simpson's paradox, within-clusters effect\n\nAll slopes are similar but positive (compared to the between-effect). Thus estimating only the between (or within) effect is completely misleading.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfitl_cmc |> \n  lapply(broom::tidy, conf.int = TRUE) |> \n  bind_rows(.id = \"unit\") |> \n  mutate(unit = as.numeric(unit)) |> \n  filter(term == \"age_cmc\") |> \n  ggplot(aes(x = estimate, y = unit)) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0,\n             lty = \"dashed\",\n             col = \"firebrick\") +\n  xlab(latex2exp::TeX(\"$\\\\beta_{age}$\"))\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/unnamed-chunk-43-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## What about the multilevel model?\n\nWe can fit a multilevel model on the full dataset. What is the `age` slope? within or between?\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- lmer(y ~ age + (1|unit), data = dat)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: y ~ age + (1 | unit)\n#>    Data: dat\n#> \n#> REML criterion at convergence: -1710.7\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.7439 -0.6848 -0.0165  0.7191  3.0376 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  unit     (Intercept) 0.091429 0.30237 \n#>  Residual             0.009747 0.09872 \n#> Number of obs: 1000, groups:  unit, 10\n#> \n#> Fixed effects:\n#>              Estimate Std. Error t value\n#> (Intercept) 0.4471843  0.0997411   4.483\n#> age         0.0100761  0.0006074  16.590\n#> \n#> Correlation of Fixed Effects:\n#>     (Intr)\n#> age -0.283\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n## What about the multilevel model?\n\nThe estimated effect is a sort of weighted average of the within and between effect. This is usually not interesting, especially when the two effects are different. We want to isolate the between and within effects.\n\nWe need to include two version of the `age` variable, one centered on the clusters and the other representing the clusters means.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$age_cmc <- cmc(dat$age, dat$unit)\ndat$age_cm <- cm(dat$age, dat$unit)\n\nfit_bw <- lmer(y ~ age_cmc + age_cm + (1|unit), data = dat)\n```\n:::\n\n\n\n## What about the multilevel model?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(fit_bw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: y ~ age_cmc + age_cm + (1 | unit)\n#>    Data: dat\n#> \n#> REML criterion at convergence: -1724.4\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.7518 -0.6848 -0.0136  0.7126  3.0452 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  unit     (Intercept) 0.007530 0.08678 \n#>  Residual             0.009746 0.09872 \n#> Number of obs: 1000, groups:  unit, 10\n#> \n#> Fixed effects:\n#>               Estimate Std. Error t value\n#> (Intercept)  1.9252710  0.1503998  12.801\n#> age_cmc      0.0101730  0.0006083  16.724\n#> age_cm      -0.0217498  0.0031833  -6.832\n#> \n#> Correlation of Fixed Effects:\n#>         (Intr) ag_cmc\n#> age_cmc  0.000       \n#> age_cm  -0.983  0.000\n```\n\n\n:::\n:::\n\n\n\n## What about the multilevel model?\n\nThe within-clusters effect can be also included as random slope^[Of course, including the clusters means as random-slopes is not possible. Note that data are simulated without random slopes, thus the model estimate parameters at the boundaries]. Basically we allows not only the within effect to be different compared to the between effect but also that each cluster has a different sloope.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_bw2 <- lmer(y ~ age_cmc + age_cm + (age_cmc|unit), data = dat)\nsummary(fit_bw2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: y ~ age_cmc + age_cm + (age_cmc | unit)\n#>    Data: dat\n#> \n#> REML criterion at convergence: -1724.5\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.7516 -0.6911 -0.0114  0.7116  3.0458 \n#> \n#> Random effects:\n#>  Groups   Name        Variance  Std.Dev.  Corr\n#>  unit     (Intercept) 7.528e-03 0.0867634     \n#>           age_cmc     2.053e-08 0.0001433 1.00\n#>  Residual             9.746e-03 0.0987213     \n#> Number of obs: 1000, groups:  unit, 10\n#> \n#> Fixed effects:\n#>              Estimate Std. Error t value\n#> (Intercept)  1.933188   0.149989  12.889\n#> age_cmc      0.010176   0.000610  16.683\n#> age_cm      -0.021920   0.003174  -6.905\n#> \n#> Correlation of Fixed Effects:\n#>         (Intr) ag_cmc\n#> age_cmc  0.020       \n#> age_cm  -0.983 -0.006\n#> optimizer (nloptwrap) convergence code: 0 (OK)\n#> boundary (singular) fit: see help('isSingular')\n```\n\n\n:::\n:::\n\n\n\n## More on the Simpson's Paradox\n\n@Kievit2013-mt describe the SP problem in Psychology with methods to detect it.\n\n![](img/kievit2013.png)\n\nWhat do you think?\n\n## Practical session\n\n- simulate clustered data with some predictors\n- simulate the same dataset but with a some between-within differences\n\n# Diagnostics\n\n## The `performance` package\n\nThe `performance` package has a series of nice functions to visualize and assess the models fit.\n\nLet's go back to our reaction times:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- lmer(Reaction ~ Days + (Days|Subject), data = sleepstudy)\n\nperformance::check_model(fit)\n```\n:::\n\n\n\n## The `performance` package\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- lmer(Reaction ~ Days + (Days|Subject), data = sleepstudy)\n\nperformance::check_model(fit)\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-html/performance-plot-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Influence measures\n\n@Nieuwenhuis2012-uv describe the `Influence.ME` package that computes the standard influence measures (Cook's distance, DFbeta, etc.) for `lme4` models.\n\nAlso the `lme4:::influence.merMod()` compute all the measures. You can provide the `groups = ` argument to specify at which level performing the leave-one-out procedure.\n\n## Influence measures, small exercise\n\nBoth `lme4:::influence.merMod()` and `Influence.ME` do not provide (good enough) plotting tools. Write a set of functions that takes a `lme4` model in input, calculate the influence measures, organize everything in a data.frame and plot the influence measures results with `ggplot2`.\n\n# Effect sizes\n\n## $R^2$ for multilevel models\n\nThe $R^2$ for multilevel models is not computed as for standard regression models. @Nakagawa2017-hb describe how to calculate the $R^2$. They described the *marginal* (only fixed-effects) and the *conditional* (fixed and random-effects).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# or performance::r2()\nperformance::r2_nakagawa(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # R2 for Mixed Models\n#> \n#>   Conditional R2: 0.799\n#>      Marginal R2: 0.279\n```\n\n\n:::\n:::\n\n\n\nClearly the *conditional* is always greater or equal to the *marginal* one.\n\n# References\n",
    "supporting": [
      "glmer_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}