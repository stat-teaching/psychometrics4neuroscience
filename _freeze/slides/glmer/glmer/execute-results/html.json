{
  "hash": "31736664a8d38618dd0b532edf52c423",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Generalized Linear Mixed-Effects Models\"\nformat: beamer\nscript: \"[glmer.R](glmer.R)\"\n---\n\n\n\n\n\n\n\n\n\n## Notation\n\n$$\n\\mathbf{y}_{N \\times 1} = \\mathbf{X}_{N \\times p} \\boldsymbol{\\beta}_{p \\times 1} + \\sum_{i=1}^{m} \\mathbf{Z}_i^{(N \\times q_i)} \\mathbf{b}_i^{(q_i \\times 1)} + \\boldsymbol{\\varepsilon}_{N \\times 1}\n$$\n\nWhere $N$ is the number of observations, $p$ is the number of predictors, $q$ is the number of clusters (e.g., participants) and $m$ is the number of random effects (e.g., nested or crossed).\n\n## Visualizing the $\\mathbf{Z}$ matrix\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- sleepstudy\nZ <- get_Z_matrix(~ Days + (1|Subject), dat)\nrownames(Z) <- NULL\ncolnames(Z) <- NULL\n\nreshape2::melt(Z) |>\n  ggplot(aes(x = Var1, y = Var2, fill = factor(value), color = factor(value))) +\n  geom_tile(show.legend = FALSE) +\n  scale_fill_manual(values = c(\"transparent\", scales::alpha(\"black\", 0.5))) +\n  scale_color_manual(values = c(\"transparent\", \"black\")) +\n  theme_bw(20) +\n  theme(panel.grid = element_blank(),\n        aspect.ratio = 1) +\n  ylab(latex2exp::TeX(\"$Cluster_q$\")) +\n  xlab(latex2exp::TeX(\"$Observation_i$\"))\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n## Why clustered data in Psychology?\n\nIn Psychology and Neuroscience we (almost) always have clustered data. For example:\n\n- Childrens nested within classrooms (maybe nested within schools)\n- Trials of a cognitive experiments nested within participants\n- ...\n\nThe main point is that, clustered observations are not independent and we want to take into account the correlation.\n\n## Examples of clustered data\n\nfigure here\n\n## Example with `lme4::sleepstudy`\n\nA very simple example is the `lme4::sleepstudy` where participants reaction times where evaluated under sleep deprivation.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- lme4::sleepstudy\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Reaction Days Subject\n1 249.5600    0     308\n2 258.7047    1     308\n3 250.8006    2     308\n4 321.4398    3     308\n5 356.8519    4     308\n6 414.6901    5     308\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Overall model\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point(position = position_jitter(width = 0.1)) +\n  scale_x_continuous(breaks = unique(dat$Days)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  ggtitle(\"Linear Model (ignore dependency)\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n## By-participant model\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point(position = position_jitter(width = 0.1)) +\n  facet_wrap(~Subject) +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## By-participant model\n\nFrom the by-participant models, we see a clear dependency. Observations within the same participant are more similar compared to observations across participant.\n\nIn addition, at Day 0, some participants have higher/lower reaction times compared to the overall trend. Similarly, some participants have higher/lower slopes.\n\n**Individual differences are the core of Psychology** and we want to explictly model them!\n\n## Individual differences\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = Days, y = Reaction, group = Subject)) +\n  geom_point(position = position_jitter(width = 0.1)) +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n## Are the observations clustered?\n\nWe can start assessing the clustering structure by fitting a mixed-model with only the random-intercepts and calculating the intraclass-correlation.\n\n$$\ny_{ij} = \\beta_0 + \\beta_{0_i} + \\epsilon_{ij}\n$$\n\n## Are the observations clustered?\n\nThe model can be fitted with the `lme4::lmer()` function:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit0 <- lmer(Reaction ~ 1 + (1|Subject), data = dat)\nsummary(fit0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ 1 + (1 | Subject)\n   Data: dat\n\nREML criterion at convergence: 1904.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.4983 -0.5501 -0.1476  0.5123  3.3446 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 1278     35.75   \n Residual             1959     44.26   \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   298.51       9.05   32.98\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Are the observations clustered?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# using the insight::get_variance() function\nvv <- insight::get_variance(fit0)\nvv$var.intercept / (vv$var.intercept + vv$var.residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Subject \n0.3948896 \n```\n\n\n:::\n\n```{.r .cell-code}\n# or directly with performance::icc()\nperformance::icc(fit0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.395\n  Unadjusted ICC: 0.395\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThus roughly 39% of the variance is explained by the clustering structure.\n\n## Are the observations clustered?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  ggplot(aes(x = Subject, y = Reaction)) +\n  #geom_point(position = position_jitter(width = 0.1))\n  geom_boxplot(fill = \"dodgerblue\") +\n  ggtitle(\"ICC = 39%\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n## Are the observations clustered\n\nWe can remove the subject-specific effect.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  mutate(Reaction_cmc = cmc(Reaction, Subject)) |> \n  ggplot(aes(x = Subject, y = Reaction_cmc)) +\n  #geom_point(position = position_jitter(width = 0.1))\n  geom_boxplot(fill = \"dodgerblue\") +\n  ylab(\"Reaction (cluster-mean centered)\")\n```\n\n::: {.cell-output-display}\n![](glmer_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n## How different ICCs appear...\n\nWe can simulate some datasets with different ICC:\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](glmer_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n## Psychological interpretation of random intercepts\n\nThe random-intercepts are intepreted as baseline variation in the experiment. For example:\n\n- variability at time 0\n- variability pre treatment\n- variability for the reference condition\n- ...\n\nFurthemore, the ICC (that is related to the random-intercepts variance) affects the statistical power of the model.\n\n## Statistical power and ICC\n\nWe will see how to simulate data in a meaningful way, but here just an example on the impact of ICC on the statistical power.\n\nI estimate the statistical power (for an intercept-only model) using the analytical method by @Hedges2001-ra.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npowerICC <- function(nc, ns, d, icc, alpha = 0.05){\n  tau2 <- icc\n  vi <- 1/ns + 1/ns\n  v <- (vi + tau2)/nc\n  z <- d / sqrt(v)\n  zc <- abs(qnorm(alpha/2))\n  1 - pnorm(zc - z) + pnorm(-zc - z)\n}\n```\n:::\n\n\n\n\n\n\n## Statistical power and ICC\n\nClearly, as the ICC increases the power decreases. Furthermore as the ICC increases, the effective sample sizes decreases.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](glmer_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n## Effective sample size\n\nThis is crucial in Psychology because we need to collect data and colleting data is time-consuming, expensive and usually our sample sizes are lower than optimal according to power calculation.\n\n$$\nN_{\\mbox{eff}} = \\frac{N}{1 + (m - 1)\\rho}\n$$\n\n\n\n## References",
    "supporting": [
      "glmer_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}