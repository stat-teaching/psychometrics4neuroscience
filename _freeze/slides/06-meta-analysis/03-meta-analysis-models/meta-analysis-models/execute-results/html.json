{
  "hash": "efcab4dced1c367ec8b6271dcac41ae6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Meta-analysis Models\nexecute: \n  echo: true\nknitr:\n  opts_chunk: \n    comment: \"#>\"\n---\n\n\n\n\n\n\n\n# Simulation setup {.section}\n\n## Notation {.smaller}\n\nMeta-analysis notation is a little bit inconsistent in textbooks and papers. We define here some rules to simplify the work.\n\n- $k$ is the number of studies\n- $n_j$ is the sample size of the group $j$ within a study\n- $y_i$ are the observed effect size included in the meta-analysis\n- $\\sigma_i^2$ are the observed sampling variance of studies and $\\epsilon_i$ are the sampling errors\n- $\\theta$ is the equal-effects parameter (see @eq-ee1)\n- $\\delta_i$ is the random-effect (see @eq-re-mod2)\n- $\\mu_\\theta$ is the average effect of a random-effects model (see @eq-re-mod1)\n- $w_i$ are the meta-analysis weights\n- $\\tau^2$ is the heterogeneity (see @eq-re-mod2)\n- $\\Delta$ is the (generic) population effect size\n- $s_j^2$ is the variance of the group $j$ within a study\n\n## Simulation setup\n\nGiven the introduction to effect sizes, from now we will simulate data using UMD and the individual-level data. \n\nBasically we are simulating an effect size $D$ coming from the comparison of two independent groups $G_1$ and $G_2$.\n\nEach group is composed by $n$ participants measured on a numerical outcome (e.g., reaction times)\n\n## Simulation setup\n\nA more general, clear and realistic approach to simulate data is by generating $k$ studies with same/different sample sizes and (later) true effect sizes.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 10 # number of studies\nn1 <- n2 <- 10 + rpois(k, 30 - 10) # sample size from poisson distribution with lambda 40 and minimum 10\nD <- 0.5 # effect size\n\nyi <- rep(NA, k)\nvi <- rep(NA, k)\n  \nfor(i in 1:k){\n  g1 <- rnorm(n1[i], 0, 1)\n  g2 <- rnorm(n2[i], D, 1)\n  yi[i] <- mean(g2) - mean(g1)\n  vi[i] <- var(g1)/n1[i] + var(g2)/n2[i]\n}\n  \nsim <- data.frame(id = 1:k, yi, vi)\n\nhead(sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   id         yi         vi\n#> 1  1 0.59106047 0.05288966\n#> 2  2 0.41798386 0.07324075\n#> 3  3 0.19122091 0.05204159\n#> 4  4 0.46807990 0.08673606\n#> 5  5 0.08452244 0.10221239\n#> 6  6 0.72401769 0.05089307\n```\n\n\n:::\n:::\n\n\n\n## Simulation setup\n\nWe can again put everything within a function:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim_studies <- function(k, es, n1, n2 = NULL){\n  if(length(n1) == 1) n1 <- rep(n1, k)\n  if(is.null(n2)) n2 <- n1\n  if(length(es) == 1) es <- rep(es, k)\n  \n  yi <- rep(NA, k)\n  vi <- rep(NA, k)\n  \n  for(i in 1:k){\n    g1 <- rnorm(n1[i], 0, 1)\n    g2 <- rnorm(n2[i], es[i], 1)\n    yi[i] <- mean(g2) - mean(g1)\n    vi[i] <- var(g1)/n1[i] + var(g2)/n2[i]\n  }\n  \n  sim <- data.frame(id = 1:k, yi, vi, n1 = n1, n2 = n2)\n  \n  # convert to escalc for using metafor methods\n  sim <- metafor::escalc(yi = yi, vi = vi, data = sim)\n  \n  return(sim)\n}\n```\n:::\n\n\n\n## Simulation setup - Disclaimer\n\nThe proposed simulation approach using a `for` loop and separated vectors. For the purpose of the workshop this is the best option. In real-world meta-analysis simulations you can choose a more functional approach starting from a simulation grid as `data.frame` and mapping the simulation functions.\n\nFor some examples see:\n\n- @Gambarota2024-lp\n- [www.jepusto.com/simulating-correlated-smds](https://www.jepusto.com/simulating-correlated-smds)\n\n## Simulation setup - Disclaimer\n\nFor a more extended overview of the simulation setup we have an entire paper. Supplementary materials ([github.com/shared-research/simulating-meta-analysis](https://github.com/shared-research/simulating-meta-analysis)) contains also more examples for complex (multilevel and multivariate models.)\n\n![](img/gambarota2023.png){fig-align=\"center\"}\n\n# Combining studies {.section}\n\n## Combining studies\n\nLet's imagine to have $k = 10$ studies, a $D = 0.5$ and heterogeneous sample sizes in each study.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 10\nD <- 0.5\nn <- 10 + rpois(k, lambda = 20) \ndat <- sim_studies(k = k, es = D, n1 = n)\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>   id     yi     vi n1 n2 \n#> 1  1 0.2966 0.0586 25 25 \n#> 2  2 0.5997 0.0594 33 33 \n#> 3  3 0.6338 0.0928 29 29 \n#> 4  4 0.8950 0.0772 26 26 \n#> 5  5 0.7392 0.0482 33 33 \n#> 6  6 0.2714 0.0473 30 30\n```\n\n\n:::\n:::\n\n\n\n. . .\n\nWhat is the best way to combine the studies?\n\n## Combining studies\n\nWe can take the average effect size and considering it as a huge study. This can be considered the best way to combine the effects.\n\n$$\n\\hat{D} = \\frac{\\sum^{k}_{i = 1} D_i}{k}\n$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(dat$yi)\n#> [1] 0.5610234\n```\n:::\n\n\n\n. . .\n\nIt is appropriate? What do you think? Are we missing something?\n\n## Weighting studies\n\nWe are not considering that some studies, despite providing a similar effect size could give more information. An higher sample size (or lower sampling variance) produce a more reliable estimation.\n\n. . .\n\nWould you trust more a study with $n = 100$ and $D = 0.5$ or a study with $n = 10$ and $D = 0.5$? The \"meta-analysis\" that we did before is completely ignoring this information.\n\n## Weighting studies\n\nWe need to find a value (called weight $w_i$) that allows assigning more trust to a study because it provide more information. \n\n. . .\n\nThe simplest weights are just the sample size, but in practice we use the so-called **inverse-variance weighting**. We use the (inverse) of the sampling variance of the effect size to weight each study. \n\n. . .\n\nThe basic version of a meta-analysis is just a **weighted average**:\n\n$$\n\\overline D_w = \\frac{\\sum^k_{i = 1}{w_iD_i}}{\\sum^k_{i = 1}{w_i}}\n$$\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwi <- 1/dat$vi\nsum(dat$yi * wi) / sum(wi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.5457273\n```\n\n\n:::\n\n```{.r .cell-code}\n# weighted.mean(dat$yi, wi)\n```\n:::\n\n\n\n## Weighting studies\n\nGraphically, the two models can be represented in this way:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndw <- weighted.mean(dat$yi, 1/dat$vi)\ndunw <- mean(dat$yi)\n\nunw_plot <- ggplot(dat, aes(x = yi, y = factor(id))) +\n  geom_point(size = 3) +\n  xlim(c(-0.5, 1.5)) +\n  geom_vline(xintercept = dunw) +\n  xlab(latex2exp::TeX(\"$y_i$\")) +\n  ylab(\"Study\") +\n  theme_minimal(15) +\n  annotate(\"label\", x = 0.5, y = k + 0.4, label = latex2exp::TeX(sprintf(\"$\\\\bar{D} = %.2f$\", dunw))) +\n  geom_label(aes(x = 1, y = id, label = paste0(\"n = \", n1)))\n\nw_plot <- ggplot(dat, aes(x = yi, y = factor(id))) +\n  geom_point(aes(size = 1/vi),\n             show.legend = FALSE) +\n  xlim(c(-0.5, 1.5)) +\n  geom_vline(xintercept = dw) +\n  xlab(latex2exp::TeX(\"$y_i$\")) +\n  ylab(\"Study\") +\n  theme_minimal(15) +\n  annotate(\"label\", x = 0.5, y = k + 0.4, label = latex2exp::TeX(sprintf(\"$\\\\bar{D}_w = %.2f$\", dw))) +\n  geom_label(aes(x = 1, y = id, label = paste0(\"n = \", n1)))\n\nunw_plot + w_plot\n```\n\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n# Equal-effects (EE) meta-analysis {.section}\n\n## EE meta-analysis\n\nWhat we did in the last example (the weighted mean) is the exactly a meta-analysis model called **equal-effects** (or less precisely fixed-effect). The assumptions are very simple:\n\n- there is a unique, true effect size to estimate $\\theta$\n- each study is a more or less precise estimate of $\\theta$\n- there is no TRUE variability among studies. The observed variability is due to studies that are imprecise (i.e., sampling error)\n- assuming that each study has a very large sample size, the observed variability is close to zero.\n\n## EE meta-analysis, formally\n\n$$\ny_i = \\theta + \\epsilon_i\n$$ {#eq-ee1}\n\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n$$ {#eq-ee2}\n\nWhere $\\sigma^2_i$ is the vector of sampling variabilities of $k$ studies. This is a standard linear model but with heterogeneous sampling variances.\n\n## EE meta-analysis\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Simulating an EE model\n\nWhat we were doing with the `sim_studies()` function so far was simulating an EE model. In fact, there were a single $\\theta$ parameter and the observed variability was a function of the `rnorm()` randomness.\n\nBased on previous assumptions and thinking a little bit, what could be the result of simulating studies with a very large $n$?\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nns <- c(10, 50, 100, 1000, 1e4)\nD <- 0.5\ndats <- lapply(ns, function(n) sim_studies(10, es = D, n1 = n))\n```\n:::\n\n\n\n## Simulating an EE modelm {#sec-ee-impact-n}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Simulating an EE model\n\nFormulating the model as a intercept-only regression (see Equations [@eq-ee1] and [@eq-ee2]) we can generate data directly:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nD <- 0.5\nn <- 30\nk <- 10\n\nyi <- D + rnorm(k, 0, sqrt(1/n + 1/n))\n# or equivalently\n# yi <- rnorm(k, D, sqrt(1/n + 1/n))\n```\n:::\n\n\n\nAs we did for the aggregated data approach. Clearly we need to simulate also the `vi` vector from the appropriate distribution. Given that we simulated data starting from the participant-level the uncertainty of `yi` and `vi` is already included.\n\n## Fitting an EE model\n\nThe model can be fitted using the `metafor::rma()` function, with `method = \"EE\"`^[There is a confusion about the *fixed-effects* vs *fixed-effect* (no *s*) and *equal-effects* models. See [https://wviechtb.github.io/metafor/reference/misc-models.html](https://wviechtb.github.io/metafor/reference/misc-models.html)].\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntheta <- 0.5\nk <- 15\nn <- 10 + rpois(k, 30 - 10)\ndat <- sim_studies(k = k, es = theta, n1 = n)\nfit <- rma(yi, vi, data = dat, method = \"EE\")\nsummary(fit)\n#> \n#> Equal-Effects Model (k = 15)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -4.5745   22.6448   11.1490   11.8570   11.4566   \n#> \n#> I^2 (total heterogeneity / total variability):   38.18%\n#> H^2 (total variability / sampling variability):  1.62\n#> \n#> Test for Heterogeneity:\n#> Q(df = 14) = 22.6448, p-val = 0.0663\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.5453  0.0650  8.3883  <.0001  0.4179  0.6727  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n## Interpreting an EE model\n\n- The first section (`logLik`, `deviance`, etc.) presents some general model statistics and information criteria\n- The $I^2$ and $H^2$ are statistics evaluating the observed heterogeneity (see next slides)\n- The `Test of Heterogeneity` section presents the test of the $Q$ statistics for the observed heterogeneity (see next slides)\n- The `Model Results` section presents the estimation of the $\\theta$ parameter along with the standard error and the Wald $z$ test ($H_0: \\theta = 0$)\n\nThe `metafor` package has a several well documented functions to calculate and plot model results, residuals analysis etc.\n\n## Interpreting an EE model\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(fit) # general plots\n```\n\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Interpreting an EE Model\n\nThe main function for plotting model results is the `forest()` function that produce the forest plot.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nforest(fit)\n```\n\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Interpreting an EE Model\n\nWe did not introduced the concept of heterogeneity, but the $I^2$, $H^2$ and $Q$ statistics basically evaluate if the observed heterogeneity should be attributed to **sampling variability** (uncertainty in estimating $\\theta$ because we have a limited $k$ and $n$) or **sampling variability** plus other sources of heterogeneity.\n\n## EE model as a weighted Average\n\nFormally $\\theta$ is estimated as [see @Borenstein2009-mo, p. 66]\n\n$$\n\\hat{\\theta} = \\frac{\\sum^k_{i = 1}{w_iy_i}}{\\sum^k_{i = 1}{w_i}}; \\;\\;\\; w_i = \\frac{1}{\\sigma^2_i}\n$$\n\n$$\nSE_{\\theta} = \\frac{1}{\\sum^k_{i = 1}{w_i}}\n$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwi <- 1/dat$vi\ntheta_hat <- with(dat, sum(yi * wi)/sum(wi))\nse_theta_hat <- sqrt(1/sum(wi))\nc(theta = theta_hat, se = se_theta_hat, z = theta_hat / se_theta_hat)\n#>    theta       se        z \n#> 0.545264 0.065003 8.388289\n```\n:::\n\n\n\n# Random-effects (RE) meta-analysis {.section}\n\n## Are the EE assumptions realistic?\n\nThe EE model is appropriate if our studies are somehow **exact replications** of the exact same effect. We are assuming that there is **no real variability**.\n\n. . .\n\nHowever, meta-analysis rarely report the results of $k$ exact replicates. It is more common to include **studies answering the same research question** but with different methods, participants, etc.\n\n. . .\n\n- people with different ages or other participant-level differences\n- different methodology\n- ...\n\n## Are the EE assumptions realistic?\n\n. . .\n\nIf we relax the previous assumption we are able to combine studies that are not exact replications. \n\n. . .\n\nThus the real effect $\\theta$ is no longer a single **true** value but can be larger or smaller in some conditions.\n\n. . .\n\nIn other terms we are assuming that there could be some variability (i.e., **heterogeneity**) among studies that is independent from the sample size. Even with studies with $\\lim_{n\\to\\infty}$ the observed variability is not zero.\n\n## Random-effects model (RE)\n\nWe can extend the EE model including another source of variability, $\\tau^2$. $\\tau^2$ is the true heterogeneity among studies caused by methdological differences or intrisic variability in the phenomenon.\n\nFormally we can extend @eq-ee1 as:\n$$\ny_i = \\mu_{\\theta} + \\delta_i + \\epsilon_i\n$$ {#eq-re-mod1}\n\n$$\n\\delta_i \\sim \\mathcal{N}(0, \\tau^2)\n$$ {#eq-re-mod2}\n\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n$$\n\nWhere $\\mu_{\\theta}$ is the average effect size and $\\delta_i$ is the study-specific deviation from the average effect (regulated by $\\tau^2$). Clearly each study specific effect is $\\theta_i = \\mu_{\\theta} + \\delta_i$.\n\n## RE model\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n## RE model estimation\n\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also $\\tau^2$.\n\n$$\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n$$ {#eq-re1}\n\n$$\nw^*_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$ {#eq-re2}\n\nThe weights are different compared to the EE model. Extremely precise/imprecise studies will have less impact in the RE model.\n\n## RE vs EE model\n\nThe crucial difference with the EE model is that even with large $n$, only the $\\mu_{\\theta} + \\delta_i$ are estimated (almost) without error. As long $\\tau^2 \\neq 0$ there will be variability in the effect sizes.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Simulating a RE Model\n\nTo simulate the RE model we simply need to include $\\tau^2$ in the EE model simulation.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 15 # number of studies\nmu <- 0.5 # average effect\ntau2 <- 0.1 # heterogeneity\nn <- 10 + rpois(k, 30 - 10) # sample size\ndeltai <- rnorm(k, 0, sqrt(tau2)) # random-effects\nthetai <- mu + deltai # true study effect\n\ndat <- sim_studies(k = k, es = thetai, n1 = n)\n\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>   id      yi     vi n1 n2 \n#> 1  1  0.7622 0.0715 35 35 \n#> 2  2 -0.1831 0.0617 25 25 \n#> 3  3  1.2130 0.0812 30 30 \n#> 4  4  0.5455 0.0753 30 30 \n#> 5  5  0.1203 0.0596 32 32 \n#> 6  6  0.5873 0.0701 28 28\n```\n\n\n:::\n:::\n\n\n\n## Simulating a RE model\n\nAgain, we can put everything within a function expanding the previous `sim_studies()` by including $\\tau^2$:\n\n\n\n\n\n\n\n```r\nsim_studies <- function(k, es, tau2 = 0, n1, n2 = NULL, add = NULL){\n  if(length(n1) == 1) n1 <- rep(n1, k)\n  if(is.null(n2)) n2 <- n1\n  if(length(es) == 1) es <- rep(es, k)\n  \n  yi <- rep(NA, k)\n  vi <- rep(NA, k)\n  \n  # random effects\n  deltai <- rnorm(k, 0, sqrt(tau2))\n  \n  for(i in 1:k){\n    g1 <- rnorm(n1[i], 0, 1)\n    g2 <- rnorm(n2[i], es[i] + deltai[i], 1)\n    yi[i] <- mean(g2) - mean(g1)\n    vi[i] <- var(g1)/n1[i] + var(g2)/n2[i]\n  }\n  \n  sim <- data.frame(id = 1:k, yi, vi, n1 = n1, n2 = n2)\n  \n  if(!is.null(add)){\n    sim <- cbind(sim, add)\n  }\n  \n  # convert to escalc for using metafor methods\n  sim <- metafor::escalc(yi = yi, vi = vi, data = sim)\n  \n  return(sim)\n}\n```\n\n## Simulating a RE model\n\nThe data are similar to the EE simulation but we have an extra source of heterogeneity.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat |>\n  summary() |>\n  qforest()\n```\n:::\n\n\n\n## Simulating a RE model\n\nTo see the actual impact of $\\tau^2$ we can follow the same approach of @sec-ee-impact-n thus using a large $n$. The sampling variance `vi` of each study is basically 0.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ... other parameters as before\nn <- 1e4\ndeltai <- rnorm(k, 0, sqrt(tau2)) # random-effects\nthetai <- mu + deltai # true study effect\ndat <- sim_studies(k = k, es = thetai, n1 = n)\n# or equivalently \n# dat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\n\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>   id     yi     vi    n1    n2 \n#> 1  1 0.3103 0.0002 10000 10000 \n#> 2  2 0.6719 0.0002 10000 10000 \n#> 3  3 0.3730 0.0002 10000 10000 \n#> 4  4 0.5965 0.0002 10000 10000 \n#> 5  5 0.4026 0.0002 10000 10000 \n#> 6  6 0.2482 0.0002 10000 10000\n```\n\n\n:::\n:::\n\n\n\n## Simulating a RE Model\n\nClearly, compared to @sec-ee-impact-n, even with large $n$ the variability is not reduced because $\\tau^2 \\neq 0$. As $\\tau^2$ approach zero the EE and RE models are similar.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat |>\n  summary() |>\n  qforest()\n```\n\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## RE model estimation\n\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also $\\tau^2$.\n\n$$\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n$$ {#eq-re1}\n\n$$\nw^*_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$ {#eq-re2}\n\nThe weights are different compared to the EE model. Extremely precise/imprecise studies will have less impact in the RE model.\n\n## Fitting a RE model\n\nIn R we can use the `metafor::rma()` function using the `method = \"REML\"`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- rma(yi, vi, data = dat, method = \"REML\")\nsummary(fit)\n#> \n#> Random-Effects Model (k = 15; tau^2 estimator: REML)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -0.2474    0.4947    4.4947    5.7729    5.5856   \n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.0605 (SE = 0.0229)\n#> tau (square root of estimated tau^2 value):      0.2459\n#> I^2 (total heterogeneity / total variability):   99.67%\n#> H^2 (total variability / sampling variability):  302.54\n#> \n#> Test for Heterogeneity:\n#> Q(df = 14) = 4212.0410, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.4729  0.0636  7.4366  <.0001  0.3483  0.5975  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n## Intepreting the RE model\n\nThe model output is quite similar to the EE model and also the intepretation is similar.\n\nThe only extra section is `tau^2/tau` that is the estimation of the between-study heterogeneity.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Random-Effects Model (k = 15; tau^2 estimator: REML)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -0.2474    0.4947    4.4947    5.7729    5.5856   \n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.0605 (SE = 0.0229)\n#> tau (square root of estimated tau^2 value):      0.2459\n#> I^2 (total heterogeneity / total variability):   99.67%\n#> H^2 (total variability / sampling variability):  302.54\n#> \n#> Test for Heterogeneity:\n#> Q(df = 14) = 4212.0410, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.4729  0.0636  7.4366  <.0001  0.3483  0.5975  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n## Estimating $\\tau^2$\n\n\n![](img/langan2017.png){fig-align=\"center\" width=80%}\n\n## Estimating $\\tau^2$\n\nThe **Restricted Maximum Likelihood** (REML) estimator is considered one of the best. We can compare the results using the `all_rma()` custom function that tests all the estimators^[The `filor::compare_rma()` function is similar to the `car::compareCoefs()` function].\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfitl <- all_rma(fit)\nround(filor::compare_rma(fitlist = fitl), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>           DL     HE     HS    HSk     SJ     ML   REML     EB     PM    PMM\n#> b      0.473  0.473  0.473  0.473  0.473  0.473  0.473  0.473  0.473  0.473\n#> se     0.063  0.064  0.061  0.063  0.064  0.061  0.064  0.064  0.064  0.065\n#> zval   7.457  7.436  7.719  7.457  7.437  7.698  7.437  7.437  7.435  7.258\n#> pval   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n#> ci.lb  0.349  0.348  0.353  0.349  0.348  0.352  0.348  0.348  0.348  0.345\n#> ci.ub  0.597  0.598  0.593  0.597  0.598  0.593  0.598  0.598  0.598  0.601\n#> I2    99.668 99.669 99.644 99.668 99.669 99.646 99.669 99.669 99.670 99.685\n#> tau2   0.060  0.060  0.056  0.060  0.060  0.056  0.060  0.060  0.060  0.063\n```\n\n\n:::\n:::\n\n\n\n## Intepreting heterogeneity $\\tau^2$\n\nLooking at @eq-re-mod2, $\\tau^2$ is essentially the variance of the random-effect. This means that we can intepret it as the variability (or the standard deviation) of the true effect size distribution.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ntau2s <- c(0.01, 0.05, 0.1, 0.2)\ntau2s_t <- latex2exp::TeX(sprintf(\"$\\\\tau^2 = %.2f$\", tau2s))\n\npar(mfrow = c(1, 3))\nhist(rnorm(1e4, 0, sqrt(tau2s[1])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[1], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")\nhist(rnorm(1e4, 0, sqrt(tau2s[2])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[2], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")\n#hist(rnorm(1e4, 0, sqrt(tau2s[3])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[3], probability = TRUE, ylim = c(0, 5))\nhist(rnorm(1e4, 0, sqrt(tau2s[4])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[4], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")\n```\n\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-25-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Intepreting $\\tau^2$\n\nAs in the previus plot we can assume $n = \\infty$ and generate true effects from @eq-re-mod2. In this way we understand the impact of assuming (or estimating) a certain $\\tau^2$.\n\nFor example, a $\\tau = 0.2$ and a $\\mu_{\\theta} = 0.5$, 50% of the true effects ranged between:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nD <- 0.5\nyis <- D + rnorm(1e5, 0, 0.2)\nquantile(yis, c(0.75, 0.25))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       75%       25% \n#> 0.6346211 0.3650564\n```\n\n\n:::\n:::\n\n\n\n## The $Q$ Statistics^[See @Harrer2021-go (Chapter 5) and @Hedges2019-ry for an overview about the Q statistics]\n\nThe Q statistics is used to make inference on the heterogeneity. Can be considered as a weighted sum of squares:\n\n$$\nQ = \\sum^k_{i = 1}w_i(y_i - \\hat \\mu)^2\n$$\n\nWhere $\\hat \\mu$ is EE estimation (regardless if $\\tau^2 \\neq 0$) and $w_i$ are the inverse-variance weights. Note that in the case of $w_1 = w_2 ... = w_i$, Q is just a standard sum of squares (or deviance).\n\n## The $Q$ Statistics\n\n- Given that we are summing up squared distances, they should be approximately $\\chi^2$ with $df = k - 1$. In case of no heterogeneity ($\\tau^2 = 0$) the observed variability is only caused by sampling error and the expectd value of the $\\chi^2$ is just the degrees of freedom ($df = k - 1$).\n- In case of $\\tau^2 \\neq 0$, the expected value is $k - 1 + \\lambda$ where $\\lambda$ is a non-centrality parameter.\n- In other terms, if the expected value of $Q$ exceed the expected value assuming no heterogeneity, we have evidence that $\\tau^2 \\neq 0$.\n\n## The $Q$ Statistics\n\nLet's try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nget_Q <- function(yi, vi){\n  wi <- 1/vi\n  theta_ee <- weighted.mean(yi, wi)\n  sum(wi*(yi - theta_ee)^2)\n}\n\nk <- 30\nn <- 30\ntau2 <- 0.1\nnsim <- 1e4\n\nQs_tau2_0 <- rep(0, nsim)\nQs_tau2 <- rep(0, nsim)\nres2_tau2_0 <- vector(\"list\", nsim)\nres2_tau2 <- vector(\"list\", nsim)\n\nfor(i in 1:nsim){\n  dat_tau2_0 <- sim_studies(k = 30, es = 0.5, tau2 = 0, n1 = n)\n  dat_tau2 <- sim_studies(k = 30, es = 0.5, tau2 = tau2, n1 = n)\n  \n  theta_ee_tau2_0 <- weighted.mean(dat_tau2_0$yi, 1/dat_tau2_0$vi)\n  theta_ee <- weighted.mean(dat_tau2$yi, 1/dat_tau2$vi)\n  \n  res2_tau2_0[[i]] <- dat_tau2_0$yi - theta_ee_tau2_0\n  res2_tau2[[i]] <- dat_tau2$yi - theta_ee\n  \n  Qs_tau2_0[i] <- get_Q(dat_tau2_0$yi, dat_tau2_0$vi)\n  Qs_tau2[i] <- get_Q(dat_tau2$yi, dat_tau2$vi)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-28-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## The $Q$ Statistics\n\nLet's try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\n. . .\n\n- clearly, in the presence of heterogeneity, the expected value of the Q statistics is higher (due to $\\lambda \\neq 0$) and also residuals are larger (the $\\chi^2$ is just a sum of squared weighted residuals)\n\n. . .\n\n- we can calculate a p-value for deviation from the $\\tau^2 = 0$ case as evidence agaist the absence of heterogeneity\n\n## $I^2$ [@Higgins2002-fh]\n\nWe have two sources of variability in a random-effects meta-analysis, the sampling variability $\\sigma_i^2$ and true heterogeneity $\\tau^2$. We can use the $I^2$ to express the interplay between the two.\n$$\nI^2 = 100\\% \\times \\frac{\\hat{\\tau}^2}{\\hat{\\tau}^2 + \\tilde{v}}\n$${#eq-i2}\n\n$$\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2},\n$$\n\nWhere $\\tilde{v}$ is the typical sampling variability. $I^2$ is intepreted as the proportion of total variability due to real heterogeneity (i.e., $\\tau^2$)\n\n## $I^2$ [@Higgins2002-fh]^[see [https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf](https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf)]\n\nNote that we can have the same $I^2$ in two completely different meta-analysis. An high $I^2$ does not represent high heterogeneity. Let's assume to have two meta-analysis with $k$ studies and small ($n = 30$) vs large ($n = 500$) sample sizes. \n\nLet's solve @eq-i2 for $\\tau^2$ (using `filor::tau2_from_I2()`) and we found that the same $I^2$ can be obtained with two completely different $\\tau^2$ values:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_1 <- 30\nvi_1 <- 1/n_1 + 1/n_1\ntau2_1 <- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#> [1] 0.2666667\n\nn_2 <- 500\nvi_2 <- 1/n_2 + 1/n_2\ntau2_2 <- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#> [1] 0.016\n```\n:::\n\n\n\n## $I^2$ [@Higgins2002-fh]\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_1 <- 30\nvi_1 <- 1/n_1 + 1/n_1\ntau2_1 <- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#> [1] 0.2666667\n\nn_2 <- 500\nvi_2 <- 1/n_2 + 1/n_2\ntau2_2 <- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#> [1] 0.016\n```\n:::\n\n\n\n. . .\n\nIn other terms, the $I^2$ can be considered a good index of heterogeneity only when the total variance ($\\tilde{v} + \\tau^2$) is similar.\n\n## What about $\\tilde{v}$?\n\n$\\tilde{v}$ is considered the \"typical\" within-study variability (see [https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate](https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate)). There are different estimators but @eq-tildev [@Higgins2002-fh] is the most common.\n\n$$\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2}\n$$ {#eq-tildev}\n\n## What about $\\tilde{v}$?\n\nIn the hypothetical case where $\\sigma^2_1 = \\dots = \\sigma^2_k$, $\\tilde{v}$ is just $\\sigma^2$. This fact is commonly used to calculate the statistical power analytically [@Borenstein2009-mo, Chapter 29].\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvtilde <- function(wi){\n  k <- length(wi)\n  (k - 1) * sum(wi) / (sum(wi)^2 - sum(wi^2))\n}\n\nk <- 20\n\n# same vi\nvi <- rep((1/30 + 1/30), k)\nhead(vi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n```\n\n\n:::\n\n```{.r .cell-code}\nvtilde(1/vi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.06666667\n```\n\n\n:::\n\n```{.r .cell-code}\n# heterogeneous vi\nn <- 10 + rpois(k, 30 - 10)\nvi <- sim_vi(k = k, n1 = n)\nvtilde(1/vi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.0606214\n```\n\n\n:::\n:::\n\n\n\n## What about $\\tilde{v}$?\n\nUsing simulations we can see that $\\tilde{v}$ with heterogenenous variances (i.e., sample sizes in this case) can be approximated by the central tendency of the sample size distribution. Note that we are fixing $\\sigma^2 = 1$ thus we are not including uncertainty.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nk <- 100 # number of studies\nn <- 30 # sample size\n\nvti <- rep(NA, 1e5)\n\nfor(i in 1:1e5){\n  ni <- rpois(k, n)\n  vi <- 1/ni + 1/ni\n  vti[i] <- vtilde(1/vi)\n}\n\n# vtilde calculated from lambda\nvt <- 1/n + 1/n\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-33-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## $H^2$\n\nThe $H^2$ is an alternative index of heterogeneity. Is calculated as:\n\n$$\nH^2 = \\frac{Q}{k - 1}\n$$\n\nWe defined $Q$ as the weighted sum of squares representing the total variability. $k - 1$ is the expected value of the $\\chi^2$ statistics (i.e., sum of squares) when $\\tau^2 = 0$ (or $\\lambda = 0$). \n\nThus $H^2$ is the ratio between total heterogeneity and sampling variability. Higher $H^2$ is associated with higher heterogeneity **relative** to the sampling variability. $H^2$ is not a measure of absolute heterogeneity.\n\n## $H^2$\n\nWhen we are fitting a RE model, the $I^2$ and $H^2$ equations are slightly different [@Higgins2002-fh]. See also the `metafor` [source code](https://github.com/cran/metafor/blob/994d26a65455fac90760ad6a004ec1eaca5856b1/R/rma.uni.r#L2459C30-L2459C30).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 100\nmu <- 0.5\ntau2 <- 0.1\nn <- 30\n\ndat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\nfit_re <- rma(yi, vi, data = dat, method = \"REML\")\nfit_ee <- rma(yi, vi, data = dat, method = \"EE\")\n\n# H2 with EE model\n\ntheta_ee <- fit_ee$b[[1]] # weighted.mean(dat$yi, 1/dat$vi)\nwi <- 1/dat$vi\nQ <- with(dat, sum((1/vi)*(yi - theta_ee)^2))\nc(Q, fit_ee$QE) # same\n#> [1] 256.9859 256.9859\n\nc(H2 = fit_ee$QE / (fit_ee$k - fit_ee$p), H2_model = fit_ee$H2) # same\n#>       H2 H2_model \n#> 2.595817 2.595817\n\n# H2 with RE model\n\nvt <- vtilde(1/dat$vi)\nc(H2 = fit_re$tau2 / vt + 1, H2_model = fit_re$H2) # same\n#>       H2 H2_model \n#> 2.591544 2.591544\n```\n:::\n\n\n\n## Confidence Intervals\n\nWhat is reported in the model summary as `ci.lb` and `ci.ub` refers to the 95% confidence interval representing the uncertainty in estimating the effect (or a meta-regression parameter).\n\nWithout looking at the equations, let's try to implement this idea using simulations.\n\n- choose $k$, $\\tau^2$ and $n$\n- simulate data (several times) accordingly and fit the RE model\n- extract the estimated effect size\n- compare the simulated sampling distribution with the analytical result\n\n## Confidence Intervals\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 30\nn <- 30\ntau2 <- 0.05\nmu <- 0.5\nnsim <- 5e3\n\n# true parameters (see Borenstein, 2009; Chapter 29)\nvt <- 1/n + 1/n\nvs <- (vt + tau2)/ k\nse <- sqrt(vs)\n\nmui <- rep(NA, nsim)\n\nfor(i in 1:nsim){\n  dat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\n  fit <- rma(yi, vi, data = dat)\n  mui[i] <- coef(fit)[1]\n}\n\n# standard error\nc(simulated = sd(mui), analytical = fit$se)\n#>  simulated analytical \n#> 0.06164146 0.06009689\n\n# confidence interval\nrbind(\n  \"simulated\"  = quantile(mui, c(0.05, 0.975)),\n  \"analytical\" = c(\"2.5%\" = fit$ci.lb, \"97.5%\" = fit$ci.ub)\n)\n#>                   5%     97.5%\n#> simulated  0.3973701 0.6207606\n#> analytical 0.4507569 0.6863324\n```\n:::\n\n\n\n## Confidence Intervals\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(mui, breaks = 50, freq = FALSE, main = \"Sampling Distribution\", xlab = latex2exp::TeX(\"$\\\\mu_{\\\\theta}$\"))\ncurve(dnorm(x, mu, se), add = TRUE, col = \"firebrick\", lwd = 1.5)\n```\n\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-36-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Confidence Intervals\n\nNow the equation for the 95% confidence interval should be more clear. The standard error is a function of the within study sampling variances (depending mainly on $n$), $\\tau^2$ and $k$. As we increase $k$ the standard error tends towards zero. \n\n$$\nCI = \\hat \\mu_{\\theta} \\pm z SE_{\\mu_{\\theta}}\n$$\n\n$$\nSE_{\\mu_{\\theta}} = \\sqrt{\\frac{1}{\\sum^{k}_{i = 1}w^{\\star}_i}}\n$$\n\n$$\nw^{\\star}_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$\n\n## Confidence Intervals\n\nWe can also see it analytically, there is a huge impact of $k$.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# true parameters (see Borenstein, 2009; Chapter 29)\nvt <- 1/n + 1/n\nvs <- (vt + tau2)/ k\nse <- sqrt(vs)\n\nk <- c(10, 50, 100, 500, 1000, 5000)\nn <- c(10, 50, 100, 500, 1000, 5000)\ntau2 <- c(0, 0.05, 0.1, 0.2)\n\ndd <- expand.grid(k = k, n = n, tau2 = tau2)\n\ndd$vt <- with(dd, 1/n + 1/n)\ndd$vs <- with(dd, (vt + tau2)/ k)\ndd$se <- sqrt(dd$vs)\n\ndd$k <- as_tex_label(dd$k, \"$k = %s$\")\n\nggplot(dd, aes(x = n, y = se, color = factor(tau2))) +\n  geom_line() +\n  facet_wrap(~k, labeller = label_parsed) +\n  labs(color = latex2exp::TeX(\"\\\\tau^2\")) +\n  xlab(\"Sample Size (n)\") +\n  ylab(latex2exp::TeX(\"$SE_{\\\\mu_{\\\\theta}}$\"))\n```\n\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-37-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Prediction intervals (PI)\n\nWe could say that the CI is not completely taking into account the between-study heterogeneity ($\\tau^2$). After a meta-analysis we would like to know how confident we are in the parameters estimation BUT also **what would be the expected effect running a new experiment tomorrow?**.\n\nThe **prediction interval** [@IntHout2016-sz; @Riley2011-hp] is exactly the range of effects that I expect in predicting a new study.\n\n## PI for a sample mean\n\nTo understand the concept, let's assume to have a sample $X$ of size $n$ and we estimate the mean $\\overline X$. The PI is calculated as^[Notice that the equation, in particular the usage of $t$ vs $z$ depends on assuming $s_x$ to be known or estimated. See [https://online.stat.psu.edu/stat501/lesson/3/3.3](https://online.stat.psu.edu/stat501/lesson/3/3.3), [https://en.wikipedia.org/wiki/Prediction_interval](https://en.wikipedia.org/wiki/Prediction_interval) and [https://www.bryanshalloway.com/2021/03/18/intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals/](https://www.bryanshalloway.com/2021/03/18/intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals/)]:\n\n$$\nPI = \\overline X \\pm t_{\\alpha/2} s_x \\sqrt{1 + \\frac{1}{n}}\n$$\n\nWhere $s$ is the sample standard deviation. Basically we are combining the uncertainty in estimating $\\overline X$ (i.e, $\\frac{s_x}{n}$) with the standard deviation of the data $s_x$. Compare it with the confidence interval containing only $\\frac{s_x}{n}$.\n\n## PI in meta-analysis\n\nFor meta-analysis the equation^[When a $t$ distribution is assumed, the quantiles are calculated using $k - 2$ degrees of freedom] is conceptually similar but with different quantities.\n\n$$\nPI = \\hat \\mu_{\\theta} \\pm z \\sqrt{\\tau^2 + SE_{\\mu_{\\theta}}}\n$$\n\nBasically we are combining all the sources of uncertainty. As long as $\\tau^2 \\neq 0$ the PI is greater than the CI (in the EE model they are the same). Thus even with very precise $\\mu_{\\theta}$ estimation, large $\\tau^2$ leads to uncertain predictions.\n\n## PI in meta-analysis\n\nIn R the PI can be calculated using `predict()`. By default the model assume a standard normal distribution thus using $z$ scores. To use the @Riley2011-hp approach ($t$ distribution) the model need to be fitted using `test = \"t\"`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 100\ndat <- sim_studies(k = k, es = 0.5, tau2 = 0.1, n1 = 30)\nfit_z <- rma(yi, vi, data = dat, test = \"z\") # test = \"z\" is the default\npredict(fit_z) # notice pi.ub/pi.lb vs ci.ub/ci.lb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>    pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n#>  0.4926 0.0358 0.4225 0.5627 -0.0044 0.9897\n```\n\n\n:::\n\n```{.r .cell-code}\n# manually\nfit_z$b[[1]] + qnorm(c(0.025, 0.975)) * sqrt(fit_z$se^2 + fit_z$tau2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] -0.004426046  0.989669736\n```\n\n\n:::\n\n```{.r .cell-code}\nfit_t <- rma(yi, vi, data = dat, test = \"t\")\npredict(fit_t) # notice pi.ub/pi.lb vs ci.ub/ci.lb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>    pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n#>  0.4926 0.0358 0.4217 0.5636 -0.0106 0.9958\n```\n\n\n:::\n\n```{.r .cell-code}\n# manually\nfit_z$b[[1]] + qt(c(0.025, 0.975), k - 2) * sqrt(fit_t$se^2 + fit_t$tau2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] -0.01064014  0.99588383\n```\n\n\n:::\n:::\n\n\n\n# Aggregated vs raw data\n\n## Aggregated vs raw data\n\nMeta-analysis is about data aggregation (calculating the effect size at level-2) before modelling. What about using raw data instead? Is aggregating harmful or harmless?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 30\ntau2 <- 0.1\nes <- 0.3\nn0 <- 20 + rpois(k, 50 - 20)\nn1 <- 20 + rpois(k, 50 - 20)\n\nout <- vector(mode = \"list\", length = k) \n\ndeltai <- rnorm(k, 0, sqrt(tau2))\n\nfor(i in 1:k){\n    g0 <- rnorm(n0[i], 0, 1)\n    g1 <- rnorm(n1[i], es + deltai[i], 1)\n    di <- data.frame(\n        study = i,\n        y = c(g0, g1),\n        x = rep(c(\"g0\", \"g1\"), c(n0[i], n1[i]))\n    )\n    out[[i]] <- di\n}\n\nsim <- do.call(rbind, out)\n\nhead(sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   study          y  x\n#> 1     1  1.9616302 g0\n#> 2     1  0.9597321 g0\n#> 3     1 -0.3604109 g0\n#> 4     1 -1.9132715 g0\n#> 5     1  1.1356660 g0\n#> 6     1  1.1486092 g0\n```\n\n\n:::\n:::\n\n\n\n## Aggregated vs raw data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsimMeta <- sim |> \n    group_by(study, x) |> \n    summarise(m = mean(y),\n              sd = sd(y),\n              n = n()) |> \n    pivot_wider(names_from = x,\n                values_from = c(m, sd, n))\n\nsimMeta <- escalc(\"SMD\", \n       m1i = m_g1, m2i = m_g0, \n       sd1i = sd_g1, sd2i = sd_g0,\n       n1i = n_g1, n2i = n_g0,\n       data = simMeta)\n\nhead(simMeta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>   study        m_g0       m_g1     sd_g0     sd_g1 n_g0 n_g1     yi     vi \n#> 1     1  0.02305585 0.58993131 1.1005215 0.8131273   37   45 0.5891 0.0514 \n#> 2     2 -0.02637065 0.05104601 0.9221935 1.1308823   56   48 0.0751 0.0387 \n#> 3     3 -0.09902593 0.76891804 0.8546093 0.9294818   45   55 0.9606 0.0450 \n#> 4     4 -0.24153727 0.54860072 1.0136325 0.9724782   41   50 0.7904 0.0478 \n#> 5     5  0.06405333 0.47681065 1.0347475 0.9262632   39   50 0.4196 0.0466 \n#> 6     6 -0.01258543 0.33268296 0.9921219 1.0323516   47   36 0.3388 0.0497\n```\n\n\n:::\n:::\n\n\n\n## Aggregated vs raw data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_lme4 <- lmer(y ~ x + (x|study), data = sim)\nfit_rma <- rma(yi, vi, data = simMeta)\n\nsummary(fit_lme4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: y ~ x + (x | study)\n#>    Data: sim\n#> \n#> REML criterion at convergence: 8280.9\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.1571 -0.6656 -0.0059  0.6832  3.4559 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev. Corr\n#>  study    (Intercept) 0.002384 0.04882      \n#>           xg1         0.066497 0.25787  0.19\n#>  Residual             0.983705 0.99182      \n#> Number of obs: 2915, groups:  study, 30\n#> \n#> Fixed effects:\n#>              Estimate Std. Error t value\n#> (Intercept) -0.000455   0.027593  -0.016\n#> xg1          0.382756   0.059787   6.402\n#> \n#> Correlation of Fixed Effects:\n#>     (Intr)\n#> xg1 -0.364\n```\n\n\n:::\n:::\n\n\n\n## Aggregated vs raw data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(fit_rma)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Random-Effects Model (k = 30; tau^2 estimator: REML)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -9.0677   18.1353   22.1353   24.8699   22.5969   \n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.0665 (SE = 0.0287)\n#> tau (square root of estimated tau^2 value):      0.2578\n#> I^2 (total heterogeneity / total variability):   60.88%\n#> H^2 (total variability / sampling variability):  2.56\n#> \n#> Test for Heterogeneity:\n#> Q(df = 29) = 74.0770, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.3804  0.0604  6.2976  <.0001  0.2620  0.4988  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n## References {.refs}\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}