{
  "hash": "3be669e7637aac2240d400a810c00e6d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Publication Bias\"\nexecute: \n  echo: true\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n\n\n\n# Publication Bias (PB) {.section}\n\n# What do you think about PB? What do you know? Causes? Remedies?  {.question .smaller}\n\n## Publication Bias (PB)\n\nThe PB is a very critical **most problematic aspects** of meta-analysis. Essentially **the probability of publishing a paper** (~and thus including into the meta-analysis) [is not the same regardless of the result]{.imp}.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n\n## Publication Bias Disclaimer!\n\n**We cannot (completely) solve the PB using statistical tools**. The PB is a problem related to the publishing process and publishing incentives\n\n. . .\n\n- **pre-registrations**, **multi-lab studies**, etc. can (almost) completely solve the problem filling the literature with unbiased studies\n\n. . .\n\n- there are **statistical tools to detect, estimate and correct** for the publication bias. As every statistical method, they are influenced by statistical assumptions, number of studies and sample size, heterogeneity, etc.\n\n## Publication Bias (PB) - The Big Picture^[Thanks to the Wolfgang Viechtbauer's course [https://www.wvbauer.com/doku.php/course_ma](https://www.wvbauer.com/doku.php/course_ma)]\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](img/big-picture.svg)\n:::\n:::\n\n\n\n\n## PB under an EE model\n\nThe easiest way to understand the PB is by simulating what happen without the PB. Let's simulate a lot of studies (under a EE model) keeping all the results without selection (the ideal world).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\nk <- 1e3\nn <- round(runif(k, 10, 100))\ntheta <- 0.3\ndat <- sim_studies(k = k, es = theta, tau2 = 0, n1 = n)\ndat <- summary(dat)\n# compute 1 tail pvalue\ndat$pval1 <- 1 - pnorm(dat$zi)\nht(dat)\n#> \n#>        id      yi     vi n1 n2    sei      zi   pval   ci.lb  ci.ub       pval1 \n#> 1       1  0.3483 0.0406 52 52 0.2015  1.7287 0.0839 -0.0466 0.7431 0.041927744 \n#> 2       2  0.3762 0.0605 40 40 0.2460  1.5291 0.1262 -0.1060 0.8585 0.063119366 \n#> 3       3  0.0634 0.0911 25 25 0.3018  0.2101 0.8336 -0.5281 0.6549 0.416800171 \n#> 4       4  0.4101 0.0487 46 46 0.2206  1.8588 0.0631 -0.0223 0.8426 0.031530757 \n#> 5       5 -0.0476 0.1160 13 13 0.3405 -0.1398 0.8888 -0.7151 0.6199 0.555581669 \n#> 995   995  0.3584 0.0950 24 24 0.3083  1.1625 0.2450 -0.2458 0.9625 0.122511729 \n#> 996   996  0.3939 0.1697 17 17 0.4120  0.9562 0.3390 -0.4135 1.2014 0.169484321 \n#> 997   997  0.5205 0.0486 35 35 0.2204  2.3610 0.0182  0.0884 0.9525 0.009112230 \n#> 998   998  0.4206 0.0815 29 29 0.2854  1.4737 0.1406 -0.1388 0.9801 0.070283435 \n#> 999   999  0.0625 0.0244 83 83 0.1562  0.3999 0.6893 -0.2438 0.3687 0.344631179 \n#> 1000 1000  0.5547 0.0416 37 37 0.2039  2.7201 0.0065  0.1550 0.9544 0.003263493\n```\n:::\n\n\n\n\n## PB under an EE model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 3))\nhist(dat$yi, breaks = 50, col = \"dodgerblue\", main = \"Effect Size\")\nplot(dat$yi, dat$pval1, pch = 19, col = ifelse(dat$pval1 <= 0.05, scales::alpha(\"firebrick\", 0.5), scales::alpha(\"black\", 0.5)),\n     main = \"P value (one tail) ~ Effect size\")\nabline(h = 0.05)\nplot(dat$yi, dat$pval, pch = 19, col = ifelse(dat$pval <= 0.05, scales::alpha(\"firebrick\", 0.5), scales::alpha(\"black\", 0.5)),\n     main = \"P value (two tails) ~ Effect size\")\nabline(h = 0.05)\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n\n## PB under an EE model\n\nThen, let's assume that our publishing system is very strict (extreme). You can publish only if $p \\leq 0.05$ on the expected direction. Then the true population of effect sizes will be truncated. Essentially we are assuming that $P(1|p \\leq 0.05) = 1$ and $P(1|p \\leq 0.05) = 0$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# selecting\nsign <- dat$pval1 <= 0.05 & dat$zi > 0\ndat_pb <- dat[sign, ]\ndat_un <- dat[sample(1:nrow(dat), sum(sign)), ]\n\n# fitting EE model for the full vs selected (ignore k differences)\nfit <- rma(yi, vi, method = \"EE\", data = dat_un)\nfit_pb <- rma(yi, vi, method = \"EE\", data = dat_pb)\n```\n:::\n\n\n\n\n## PB under an EE model\n\nThen, let's assume that our publishing system is very strict (extreme). You can publish only if $p \\leq 0.05$ on the expected direction. Then the true population of effect sizes will be truncated. Essentially we are assuming that $P(1|p \\leq 0.05) = 1$ and $P(1|p \\leq 0.05) = 0$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(compare_rma(fit, fit_pb), 3)\n#> fit: rma(yi = yi, vi = vi, data = dat_un, method = \"EE\")\n#> fit_pb: rma(yi = yi, vi = vi, data = dat_pb, method = \"EE\")\n#>                fit fit_pb\n#> b (intrcpt)  0.289  0.436\n#> se           0.009  0.008\n#> zval        32.003 51.811\n#> pval         0.000  0.000\n#> ci.lb        0.271  0.420\n#> ci.ub        0.307  0.453\n#> I2           0.955  0.000\n#> tau2         0.000  0.000\n```\n:::\n\n\n\n\n## PB under an EE model\n\nThe situation is even worse when we simulate a null effect. This strict selection results in committing type-1 error:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\nk <- 1e3\nn <- round(runif(k, 10, 100))\ndat0 <- sim_studies(k = k, es = 0, tau2 = 0, n1 = n)\ndat0 <- summary(dat0)\n# compute 1 tail pvalue\ndat0$pval1 <- 1 - pnorm(dat0$zi)\n# selecting\nsign <- dat0$pval1 <= 0.05 & dat0$zi > 0\ndat_pb0 <- dat0[sign, ]\ndat_un0 <- dat0[sample(1:nrow(dat0), sum(sign)), ]\n\n# fitting EE model for the full vs selected (ignore k differences)\nfit0 <- rma(yi, vi, method = \"EE\", data = dat_un0)\nfit_pb0 <- rma(yi, vi, method = \"EE\", data = dat_pb0)\nround(compare_rma(fit0, fit_pb0), 3)\n#> fit0: rma(yi = yi, vi = vi, data = dat_un0, method = \"EE\")\n#> fit_pb0: rma(yi = yi, vi = vi, data = dat_pb0, method = \"EE\")\n#>               fit0 fit_pb0\n#> b (intrcpt) -0.006   0.363\n#> se           0.027   0.026\n#> zval        -0.220  14.015\n#> pval         0.826   0.000\n#> ci.lb       -0.059   0.312\n#> ci.ub        0.047   0.414\n#> I2           0.000   0.000\n#> tau2         0.000   0.000\n```\n:::\n\n\n\n\n## PB under an EE model\n\nAssuming to pick a very precise ($n = 1000$) and a very unprecise ($n = 20$) study, which one is more likely to have an effect size close to the true value?\n\n. . .\n\nThe precise study has a lower $\\epsilon_i$ thus is closer to $\\theta$. This relationship create a very insightful visual representation.\n\n. . .\n\nWhat could be the shape of the plot when plotting the precision (e.g., the sample size or the inverse of the variance) as a function of the effect size?\n\n## PB under an EE model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|code-fold: true\nplot(dat$yi, sqrt(dat$vi), ylim=rev(range(dat$vi)), pch = 19, col = scales::alpha(\"black\", 0.5), cex = 1.5)\nabline(v = theta, lwd = 3, col = \"firebrick\")\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n\n## Publication Bias (PB) - Funnel Plot\n\nWe created a **funnel plot**. This is a **visual tool** to check the presence of asymmetry that could be caused by publication bias. If meta-analysis assumptions are respected, and there is no publication bias:\n\n- effects should be normally distributed around the average effect\n- more precise studies should be closer to the average effect\n- less precise studies could be equally distributed around the average effect\n\n## Publication Bias (PB) - Funnel Plot\n\nThe plot assume the typical funnel shape and there are not missing spots on the at the bottom. The presence of missing spots is a potential index of publication bias.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfit <- rma(yi, vi, method = \"EE\", data = dat)\ndat$pb <- dat$pval <= 0.05\n\nwith(dat[dat$pb, ],\n     plot(yi, sei,\n          ylim = rev(range(dat$sei)),\n          xlab = latex2exp::TeX(\"$y_i$\"),\n          ylab = latex2exp::TeX(\"$\\\\sqrt{\\\\sigma_i^2}$\"),\n          xlim = range(dat$yi),\n          pch = 19,\n          col = scales::alpha(\"firebrick\", 0.5))\n)\n\nwith(dat[!dat$pb, ],\n     points(yi, sei, col = scales::alpha(\"black\", 0.5), pch = 19)\n)\n\nabline(v = fit$b[[1]], col = \"black\", lwd = 1.2)\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n\n## Publication Bias (PB) - Funnel Plot\n\nThe plot assume the typical funnel shape and there are not missing spots on the at the bottom. The presence of missing spots is a potential index of publication bias.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n\n## Robustness to PB - Fail-safe N\n\nThe Fail-safe N [@Rosenthal1979-yx] idea is very simple. Given a meta-analysis with a significant result (i.e., $p \\leq \\alpha$). How many null studies (i.e., $\\hat \\theta = 0$) do I need to obtain $p > \\alpha$?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetafor::fsn(yi, vi, data = dat)\n#> \n#> Fail-safe N Calculation Using the Rosenthal Approach\n#> \n#> Observed Significance Level: <.0001\n#> Target Significance Level:   0.05\n#> \n#> Fail-safe N: 832741\n```\n:::\n\n\n\n\n## Robustness to PB - Fail-safe N\n\nThere are several criticism to the Fail-safe N procedure:\n\n. . .\n\n- is not actually *detecting* the PB but suggesting the required PB size to remove the effect. A very large N suggest that even with PB, it is unlikely that the results could be completely changed by actually reporting null studies\n\n. . .\n\n- @Orwin1983-vu proposed a new method calculating the number of studies required to reduce the effect size to a given target\n\n. . .\n\n- @Rosenberg2005-ie proposed a method similar to Rosenthal [-@Rosenthal1979-yx] but combining the (weighted) effect sizes and not the p-values.\n\n\n## Detecting PB - Egger Regression\n\nA basic method to test the funnel plot asymmetry is using an the **Egger regression test**. Basically we calculate the relationship between $y_i$ and $\\sqrt{\\sigma^2_i}$. In the absence of asimmetry, the line slope should be not different from 0.\n\nWe can use the `metafor::regtest()` function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\negger <- regtest(fit)\negger\n#> \n#> Regression Test for Funnel Plot Asymmetry\n#> \n#> Model:     fixed-effects meta-regression model\n#> Predictor: standard error\n#> \n#> Test for Funnel Plot Asymmetry: z = -0.3621, p = 0.7173\n#> Limit Estimate (as sei -> 0):   b =  0.3061 (CI: 0.2601, 0.3520)\n```\n:::\n\n\n\n\n## Publication Bias (PB) - Egger Regression\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n\nThis is a standard (meta) regression thus the number of studies, the precision of each study and heterogeneity influence the reliability (power, type-1 error rate, etc.) of the procedure.\n\n## Correcting PB - Trim and Fill\n\nThe Trim and Fill method [@Duval2000-ym] is used to impute the hypothetical missing studies according to the funnel plot and recomputing the meta-analysis effect. Shi and Lin [@Shi2019-pj] provide an updated overview of the method with some criticisms.\n\n. . .\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\nk <- 100 # we increased k to better show the effect\ntheta <- 0.5\ntau2 <- 0.1\nn <- runif(k, 10, 100)\ndat <- sim_studies(k, theta, tau2, n)\ndat <- summary(dat)\ndatpb <- dat[dat$pval <= 0.1 & dat$zi > 0, ]\nfit <- rma(yi, vi, data = datpb, method = \"REML\")\n```\n:::\n\n\n\n\n## Correcting PB - Trim and Fill\n\nNow we can use the `metafor::trimfill()` function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaf <- metafor::trimfill(fit)\ntaf\n#> \n#> Estimated number of missing studies on the left side: 15 (SE = 5.4670)\n#> \n#> Random-Effects Model (k = 84; tau^2 estimator: REML)\n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.0562 (SE = 0.0146)\n#> tau (square root of estimated tau^2 value):      0.2371\n#> I^2 (total heterogeneity / total variability):   61.61%\n#> H^2 (total variability / sampling variability):  2.60\n#> \n#> Test for Heterogeneity:\n#> Q(df = 83) = 210.3501, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se     zval    pval   ci.lb   ci.ub      \n#>   0.5934  0.0339  17.5084  <.0001  0.5270  0.6598  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\nThe trim-and-fill estimates that 15 are missing. The new effect size after including the studies is reduced and closer to the simulated value (but in this case still significant).\n\n## Correcting PB - Trim and Fill\n\nWe can also visualize the funnel plot highligting the points that are included by the method.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfunnel(taf)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfunnel(taf)\negg <- regtest(fit)\negg_npb <- regtest(taf)\nse <- seq(0,1.8,length=100)\nlines(coef(egg$fit)[1] + coef(egg$fit)[2]*se, se, lwd=3, col = \"black\")\nlines(coef(egg_npb$fit)[1] + coef(egg_npb$fit)[2]*se, se, lwd=3, col = \"firebrick\")\nlegend(\"topleft\", legend = c(\"Original\", \"Corrected\"), fill = c(\"black\", \"firebrick\"))\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n\n## Why the funnel plot can be misleading?\n\nThis funnel plot show an evident asymmetry on the left side. Is there evidence of publication bias? What do you think?\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n\n## Why the funnel plot can be misleading?\n\nThe data are of course simulated and this is the code. What do you think now?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2024)\nk <- 50\nn1 <- round(runif(k, 10, 200))\nn2 <- round(runif(k, 10, 50))\ndat1 <- sim_studies(k, 0, 0, n1, add = list(x = 0))\ndat2 <- sim_studies(k, 0.5, 0.05, n2, add = list(x = 1))\ndat <- rbind(dat1, dat2)\nfit <- rma(yi, vi, dat = dat)\nfunnel(fit)\n```\n:::\n\n\n\n\n## Why the funnel plot can be misleading?\n\nIn fact, these are two **unbiased** population of effect sizes. Extra source of heterogeneity could create asymmetry not related to PB.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nfunnel(fit)\nfunnel(fit, col = dat$x + 1)\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n\n\n## Why the funnel plot can be misleading?\n\nAlso the methods to detect/correct for PB are committing a false alarm:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregtest(fit)\n#> \n#> Regression Test for Funnel Plot Asymmetry\n#> \n#> Model:     mixed-effects meta-regression model\n#> Predictor: standard error\n#> \n#> Test for Funnel Plot Asymmetry: z =  6.2569, p < .0001\n#> Limit Estimate (as sei -> 0):   b = -0.2067 (CI: -0.3460, -0.0675)\n```\n:::\n\n\n\n\n## Why the funnel plot can be misleading?\n\nAlso the methods to detect/correct for PB are committing a false alarm:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrimfill(fit)\n#> \n#> Estimated number of missing studies on the left side: 32 (SE = 6.3553)\n#> \n#> Random-Effects Model (k = 132; tau^2 estimator: REML)\n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.2183 (SE = 0.0337)\n#> tau (square root of estimated tau^2 value):      0.4672\n#> I^2 (total heterogeneity / total variability):   86.77%\n#> H^2 (total variability / sampling variability):  7.56\n#> \n#> Test for Heterogeneity:\n#> Q(df = 131) = 639.7238, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval    ci.lb   ci.ub    \n#>   0.0281  0.0457  0.6142  0.5391  -0.0615  0.1177    \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n## Why the funnel plot can be misleading?\n\nThe `regtest` can be applied also with moderators. The idea should be to take into account the moderators effects and then check for asymmetry.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitm <- rma(yi, vi, mods = ~x, data = dat)\nregtest(fitm)\n#> \n#> Regression Test for Funnel Plot Asymmetry\n#> \n#> Model:     mixed-effects meta-regression model\n#> Predictor: standard error\n#> \n#> Test for Funnel Plot Asymmetry: z = -1.0277, p = 0.3041\n```\n:::\n\n\n\n\n## Why the funnel plot can be misleading?\n\nIn fact, the funnel plot on the raw dataset and on residuals looks quite different because the asymmetry was caused by the moderator.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat$ri <- residuals(fitm)\nfitr <- rma(ri, vi, data = dat)\npar(mfrow = c(1, 2))\nplot_regtest(fit, main = \"Full model\")\nplot_regtest(fitr, main = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n\n\n## Correcting PB - Selection Models (SM)\n\nSM are more than a tool for correcting for the PB. SM are formal models of PB that can help us understanding and simulating the PB.\n\nThe SM are composed by two parts:\n\n1. **Effect size model**: the unbiased data generation process. In our case basically the `sim_studies()` function.\n2. **Selection model**: the assumed process generating the biased selection of effect sizes\n\n**Selection models** can be based on the p-value (e.g., *p-curve* or *p-uniform*) and/or the effect size and variance (*Copas* model). We will see only models based on the p-value.\n\n## Correcting PB - Selection Models (SM)\n\nFormally, the random-effect meta-analysis probability density function (PDF) can be written as [e.g., @Citkowicz2017-ox]:\n\n$$\nf\\left(y_i \\mid \\beta, \\tau^2 ; \\sigma_i^2\\right)=\\frac{\\phi\\left(\\frac{y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right)}{\\int_{-\\infty}^{\\infty}  \\phi\\left(\\frac{Y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right) d y_i}\n$$\n\nWithout going into details, this is the PDF without any selection process (i.e., the **effect sizes model**).\n\n## Correcting PB - Selection Models (SM)\n\nIf we have a function linking the p-value with the probability of publishing (a **weight function**) $w(p_i)$ we can include it in the previous PDF, creating a weighted PDF.\n\n$$\nf\\left(y_i \\mid \\beta, \\tau^2 ; \\sigma_i^2\\right)=\\frac{\\mathrm{w}\\left(p_i\\right) \\phi\\left(\\frac{y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right)}{\\int_{-\\infty}^{\\infty} \\mathrm{w}\\left(p_i\\right) \\phi\\left(\\frac{Y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right)d y_i}\n$$\n\nEssentially, this new model take into account the selection process (the **weight function**) to estimate a new meta-analysis. In case of no selection (all weigths are the same) the model is the standard random-effects meta-analysis.\n\n## SM - Weigth function\n\nThe weigth function is a simple function that links the p-value with the probability of publishing. The simple example at the beginning (publishing only significant p-values) is a step weigth function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- c(0, 0.05, 0.05, 1)\nw <- c(1, 1, 0, 0)\n\nplot(p, w, type = \"l\", xlab = \"p value\", ylab = \"Probability of Publishing\")\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n\n\n## SM - Weigth function\n\nWe can add more steps to express a more complex selection process:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- c(0.001, 0.01, 0.05, 0.1, 0.8, 1)\nw <- c(1, 1, 0.9, 0.5, 0.1, 0.1)\nplot(p, w, type = \"s\", xlab = \"p value\", ylab = \"Probability of Publishing\")\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n\n\n## SM - Weigth function\n\nOr we can draw a smooth function assuming certain parameters:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwbeta <- function(p, a = 1, b = 1) p^(a - 1) * (1 - p)^(b - 1)\npval <- seq(0, 1, 0.01)\n\nplot(pval, wbeta(pval, 1, 1), type = \"l\", ylim = c(0, 1), col = 1, lwd = 2,\n     xlab = \"p value\", ylab = \"Probability of Publishing\")\nlines(pval, wbeta(pval, 1, 2), col = 2, lwd = 2)\nlines(pval, wbeta(pval, 1, 5), col = 3, lwd = 2)\nlines(pval, wbeta(pval, 1, 50), col = 4, lwd = 2)\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n\n\n## SM - Weigth function\n\nWhatever the function, the SM estimate the parameters of the function and the meta-analysis parameters taking into account the weigths. \n\nClearly, in the presence of no bias the two models (with and without weights) are the same while with PB the estimation is different, probably reducing the effect size.\n\nIf the SM is correct (not possible in reality), the SM estimate the true effect even in the presence of bias. This is the strenght and elegance of the SM.\n\n## SM - Weigth functions\n\nThere are several weight functions:\n\n- the step model\n- the negative-exponential model\n- the beta model\n- ...\n\nFor an overview see the `metafor` documentation https://wviechtb.github.io/metafor/reference/selmodel.html\n\n## SM - Step model\n\nThe step model approximate the selection process with thresholds $\\alpha$ and the associated weight $w(p_i)$. For example:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsteps <- c(0.005, 0.01, 0.05, 0.10, 0.25, 0.35, 0.50, 0.65, 0.75, 0.90, 0.95, 0.99, 0.995, 1)\nmoderate_pb <- c(1, 0.99, 0.95, 0.80, 0.75, 0.65, 0.60, 0.55, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50)\nsevere_pb <- c(1, 0.99, 0.90, 0.75, 0.60, 0.50, 0.40, 0.35, 0.30, 0.25, 0.10, 0.10, 0.10, 0.10)\n\npar(mfrow = c(1, 2))\nplot(steps, moderate_pb, type = \"s\", xlab = \"p value\", ylab = \"Probability of Selection\", main = \"Moderate PB\", ylim = c(0, 1))\nabline(v = steps, col = \"grey\")\nplot(steps, severe_pb, type = \"s\", xlab = \"p value\", ylab = \"Probability of Selection\", main = \"Severe PB\", ylim = c(0, 1))\nabline(v = steps, col = \"grey\")\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n\n\n## SM - Negative-Exponential\n\nThe Negative-Exponential model is very simple and intuitive. The weight function is $e^{-\\delta p_i}$ thus the single parameter $\\delta$ is the amount of bias. When $\\delta = 0$ there is no bias.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwnegexp <- function(p, delta){\n  exp((-delta)*p)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(wnegexp(x, 0), ylim = c(0, 1), col = 1, lwd = 2, xlab = \"p value\", ylab = \"Probability of Selection\")\ncurve(wnegexp(x, 1), add = TRUE, col = 2, lwd = 2)\ncurve(wnegexp(x, 5), add = TRUE, col = 3, lwd = 2)\ncurve(wnegexp(x, 30), add = TRUE, col = 4, lwd = 2)\n\nlegend(\"topright\", legend = latex2exp::TeX(sprintf(\"$\\\\delta = %s$\", c(1, 2, 3, 4))), fill = 1:4)\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-30-1.png){width=960}\n:::\n:::\n\n\n\n\n## Simulating data with PB\n\nThe strategy to simulate biased data is to sample from the `sim_studies()` function but to keep the studies using a probabilistic sampling based on the weight function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2024)\n\nk <- 500 # high number to check the results\nes <- 0 # H0 true\ntau2 <- 0.1\ndelta <- 5\ndat <- vector(mode = \"list\", k)\n\ni <- 1\nwhile(i <= k){\n  # generate data\n  n <- runif(1, 10, 100)\n  d <- summary(sim_studies(1, es, tau2, n))\n  # get one-tail p-value\n  pi <- 1 - pnorm(d$zi)\n  # get wi\n  wpi <- wnegexp(pi, delta)\n  keep <- rbinom(1, 1, wpi) == 1\n  if(keep){\n    dat[[i]] <- d\n    i <- i + 1\n  }\n}\n\ndat <- do.call(rbind, dat)\nfit <- rma(yi, vi, data = dat)\n```\n:::\n\n\n\n\n## Simulating data with PB\n\nLet's see some plots:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(1, 3))\nhist(dat$yi, breaks = 50, col = \"dodgerblue\")\nhist(1 - pnorm(dat$zi), breaks = 50, col = \"dodgerblue\")\nplot(dat$yi, dat$sei, ylim = c(rev(range(dat$sei)[2]), 0), xlim = c(-1, 2))\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-32-1.png){width=960}\n:::\n:::\n\n\n\n\n## Simulating data with PB\n\nLet's see the model result:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- rma(yi, vi, data = dat)\n```\n:::\n\n\n\n\n\n\n## Simulating data with PB\n\nLet's see the Egger regression test and the trim-and-fill procedure:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregtest(fit)\n#> \n#> Regression Test for Funnel Plot Asymmetry\n#> \n#> Model:     mixed-effects meta-regression model\n#> Predictor: standard error\n#> \n#> Test for Funnel Plot Asymmetry: z = 4.4739, p < .0001\n#> Limit Estimate (as sei -> 0):   b = 0.2017 (CI: 0.1235, 0.2799)\ntrimfill(fit)\n#> \n#> Estimated number of missing studies on the left side: 106 (SE = 14.6182)\n#> \n#> Random-Effects Model (k = 606; tau^2 estimator: REML)\n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.0474 (SE = 0.0049)\n#> tau (square root of estimated tau^2 value):      0.2176\n#> I^2 (total heterogeneity / total variability):   56.94%\n#> H^2 (total variability / sampling variability):  2.32\n#> \n#> Test for Heterogeneity:\n#> Q(df = 605) = 1399.3991, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se     zval    pval   ci.lb   ci.ub      \n#>   0.2927  0.0121  24.1497  <.0001  0.2689  0.3164  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n## Simulating data with PB\n\nThe two methods are detecting the PB but not correcting it appropriately. Let's see the SM using a `negexp` method:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsel <- selmodel(fit, type = \"negexp\", alternative = \"greater\")\nsel\n#> \n#> Random-Effects Model (k = 500; tau^2 estimator: ML)\n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.0800 (SE = 0.0118)\n#> tau (square root of estimated tau^2 value):      0.2828\n#> \n#> Test for Heterogeneity:\n#> LRT(df = 1) = 115.9867, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval    ci.lb   ci.ub    \n#>   0.0519  0.0381  1.3609  0.1735  -0.0228  0.1266    \n#> \n#> Test for Selection Model Parameters:\n#> LRT(df = 1) = 46.6783, p-val < .0001\n#> \n#> Selection Model Results:\n#> \n#> estimate      se     zval    pval   ci.lb   ci.ub      \n#>   4.7526  0.3818  12.4494  <.0001  4.0044  5.5008  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n## Simulating data with PB\n\nWe can also plot the results:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sel)\n```\n\n::: {.cell-output-display}\n![](publication-bias_files/figure-revealjs/unnamed-chunk-37-1.png){width=960}\n:::\n:::\n\n\n\n\n## PB Sensitivity analysis\n\n- The SM is correctly detecting, estimating and correcting the PB. But we simulated a pretty strong bias with $k = 500$ studies. In reality meta-analyses have few studies.\n- @Vevea2005-xc proposed to fix the weight function parameters to certain values representing different degree of selection and check how the model changes.\n- If the model parameters are affected after taking into account the SM, this could be considered as an index of PB.\n- This approach is really interesting in general but especially when $k$ is too small for estimating the SM\n- see `?selmodel` for information about performing sensitivity analysis with pre-specified weight functions\n\n## More on SM and Publication Bias\n\n- The SM documentation of `metafor::selmodel()` [https://wviechtb.github.io/metafor/reference/selmodel.html](https://www.youtube.com/watch?v=ucmOCuyCk-c)\n- Wolfgang Viechtbauer overview of PB [https://www.youtube.com/watch?v=ucmOCuyCk-c](https://www.youtube.com/watch?v=ucmOCuyCk-c)\n- @Harrer2021-go - Doing Meta-analysis in R - [Chapter 9](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pub-bias.html)\n- @McShane2016-bk for a nice introduction about publication bias and SM\n- Another good overview by @Jin2015-ik\n- See also @Guan2016-kn, @Maier2023-js and @Bartos2022-im for Bayesian approaches to PB\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "publication-bias_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}