{
  "hash": "32d07e1121ad40fdd969058b41e6cf8a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Power analysis\"\nexecute: \n  echo: true\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n\n\n\n# Statistical Power {.section}\n\n## Power in a nutshell^[Thanks to https://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics]\n\nThe stastistical power is defined as the probability of correctly rejecting the null hypothesis $H_0$.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n\n## Power in a nutshell\n\nFor simple designs such as t-test, ANOVA, etc. the power can be computed analytically. For example, let's find the power of detecting an effect size of $d = 0.5$ with $n1 = n2 = 30$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- 0.5\nalpha <- 0.05\nn1 <- n2 <- 30\nsp <- 1\n\n# Calculate non-centrality parameter (delta)\ndelta <- d * sqrt(n1 * n2 / (n1 + n2))\n\n# Calculate degrees of freedom\ndf <- n1 + n2 - 2\n\n# Calculate critical t-value\ncritical_t <- qt(1 - alpha / 2, df)\n\n# Calculate non-central t-distribution value\nnon_central_t <- delta / sp\n\n# Calculate power\n1 - pt(critical_t - non_central_t, df)\n#> [1] 0.4741093\n```\n:::\n\n\n\n\n## Power in a nutshell\n\nThe same can be done using the `pwr` package:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower <- pwr::pwr.t.test(n = n1, d = 0.5)\npower\n#> \n#>      Two-sample t test power calculation \n#> \n#>               n = 30\n#>               d = 0.5\n#>       sig.level = 0.05\n#>           power = 0.4778965\n#>     alternative = two.sided\n#> \n#> NOTE: n is number in *each* group\n```\n:::\n\n\n\n\n## Power in a nutshell\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(power)\n```\n\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n\n## Power by simulations\n\nSometimes the analytical solution is not available or we can estimate the power for complex scenarios (missing data, unequal variances, etc.). The general workflow is:\n\n1. Generate data under the parametric assumptions\n2. Fit the appropriate model\n3. Extract the relevant metric (e.g., p-value)\n4. Repeat 1-3 several times (1000, 10000 or more)\n5. Summarise the results\n\nFor example, the power is the number of p-values lower than $\\alpha$ over the total number of simulations.\n\n## Power by simulations\n\nLet's see the previous example using simulations:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsim <- 5000\np <- rep(NA, nsim)\nfor(i in 1:nsim){\n    g1 <- rnorm(n1, 0, 1)\n    g2 <- rnorm(n2, d, 1)\n    p[i] <- t.test(g1, g2)$p.value\n}\nmean(p <= alpha)\n#> [1] 0.4842\n```\n:::\n\n\n\n\nThe estimated value is pretty close to the analytical value.\n\n## What about meta-analysis\n\nAlso for meta-analysis we have the two approaches analytical and simulation-based.\n\n- The analytical approach for (intercept-only) random-effects and equal-effects model can be found on @Borenstein2009-mo (Chapter 29). See also @Valentine2010-aj and @Hedges2001-ra\n- @Jackson2017-dv proposed a similar but improved approach\n\n## Analytical approach\n\nFor the analytical approach we need to make some assumptions:\n\n- $\\tau^2$ and $\\mu_{\\theta}$ (or $\\theta$) are estimated without errors\n- The $\\sigma^2_i$ (thus the sample size) of each $k$ study is the same\n\nUnder these assumptions the power is:\n\n$$\n(1 - \\Phi(c_{\\alpha} - \\lambda)) + \\Phi(-c_{\\alpha} - \\lambda)\n$$\n\nWhere $c_{\\alpha}$ is the critical $z$ value and $\\lambda$ is the observed statistics.\n\n## Analytical approach - EE model\n\nFor an EE model the only source of variability is the sampling variability, thus $\\lambda$:\n\n$$\n\\lambda_{EE} = \\frac{\\theta}{\\sqrt{\\sigma^2_{\\theta}}}\n$$\n\nAnd recalling previous assuptions where $\\sigma^2_1 = \\dots = \\sigma^2_k$:\n\n$$\n\\sigma^2_{\\theta} = \\frac{\\sigma^2}{k}\n$$\n\n## Analytical approach - EE model\n\nFor example, a meta-analysis of $k = 15$ studies where each study have a sample size of $n1 = n2 = 20$ (assuming again unstandardized mean difference as effect size):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 10\ntheta <- 0.3\nn1 <- n2 <- 25\nvt <- 1/n1 + 1/n2\nvtheta <- vt/k\nlambda <- theta/sqrt(vtheta)\nzcrit <- abs(qnorm(0.05/2))\n(1 - pnorm(zcrit - lambda)) + pnorm(-zcrit - lambda)\n#> [1] 0.9183621\n```\n:::\n\n\n\n\nBe careful that the EE model is assuming $\\tau^2 = 0$ thus is like having a huge study with $k \\times n_1$ participants per group.\n\n## Analytical approach - RE model\n\nFor the RE model we just need to include $\\tau^2$ in the $\\lambda$ calculation, thus:\n\n$$\n\\sigma^{2\\star}_{\\theta} = \\frac{\\sigma^2 + \\tau^2}{k}\n$$\n\nThe other calculations are the same as the EE model.\n\n## Analytical approach - RE model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 10\nmu <- 0.3\ntau2 <- 0.1\nn1 <- n2 <- 25\nvt <- 1/n1 + 1/n2\nvtheta <- (vt + tau2)/k\nlambda <- mu/sqrt(vtheta)\nzcrit <- abs(qnorm(0.05/2))\n(1 - pnorm(zcrit - lambda)) + pnorm(-zcrit - lambda)\n#> [1] 0.6087795\n```\n:::\n\n\n\n\nThe power is reduced because we are considering another source of heterogeneity. Clearly the maximal power of $k$ studies is achieved when $\\tau^2 = 0$. Hypothetically we can increase the power either increasing $k$ (the number of studies) or reducing $\\sigma^2_k$ (increasing the number of participants in each study).\n\n## Analytical approach - Power curves\n\nThe most informative approach is plotting the power curves for different values of $\\tau^2$, $\\sigma^2_k$ and $\\theta$ (or $\\mu_{\\theta}$).\n\nYou can use the `power_meta()` function:\n\n```r\npower_meta <- function(es, k, tau2 = 0, n1, n2 = NULL, alpha = 0.05){\n  if(is.null(n2)) n2 <- n1\n  zc <- qnorm(1 - alpha/2)\n  vt <- 1/n1 + 1/n2\n  ves <- (vt + tau2)/k\n  lambda <- es/sqrt(ves)\n  (1 - pnorm(zc - lambda)) + pnorm(-zc - lambda)\n}\n```\n\n## Analytical approach - Power curves\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nk <- c(5, 10, 30, 50, 100)\nes <- c(0.1, 0.3)\ntau2 <- c(0, 0.05, 0.1, 0.2)\nn <- c(10, 30, 50, 100, 1000)\n\npower <- expand_grid(es, k, tau2, n1 = n)\npower$power <- power_meta(power$es, power$k, power$tau2, power$n1)\n\npower$es <- factor(power$es, labels = latex2exp::TeX(sprintf(\"$\\\\mu_{\\\\theta} = %s$\", es)))\npower$tau2 <- factor(power$tau2, labels = latex2exp::TeX(sprintf(\"$\\\\tau^2 = %s$\", tau2)))\n\nggplot(power, aes(x = factor(k), y = power, color = factor(n1))) +\n  geom_point() +\n  geom_line(aes(group = factor(n1))) +\n  facet_grid(es~tau2, labeller = label_parsed) +\n  xlab(\"Number of Studies (k)\") +\n  ylab(\"Power\") +\n  labs(\n    color = latex2exp::TeX(\"$n_1 = n_2$\")\n  )\n```\n\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n\n## Analytical approach - Power curves\n\nWith the analytical approach we can (quickly) do interesting stuff. For example, we fix the total $N = n_1 + n_2$ for a series of $k$ and check the power.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# average meta k = 20, n = 30\nkavg <- 20\nnavg <- 30\nN <- kavg * (navg*2)\nes <- 0.3\ntau2 <- c(0, 0.05, 0.1, 0.2)\nk <- seq(10, 100, 10)\nn1 <- n2 <- round((N/k)/ 2)\n\nsim <- data.frame(es, k, n1, n2)\nsim <- expand_grid(sim, tau2 = tau2)\nsim$power <- power_meta(sim$es, sim$k, sim$tau2, sim$n1, sim$n2)\nsim$N <- with(sim, k * (n1 + n2))\n\nggplot(sim, aes(x = k, y = power, color = factor(tau2))) +\n  geom_line() +\n  ggtitle(latex2exp::TeX(\"Total N ($n_1 + n_2$) = 1200\")) +\n  labs(x = \"Number of Studies (k)\",\n       y = \"Power\",\n       color = latex2exp::TeX(\"$\\\\tau^2$\"))\n```\n\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n\nAs long as $\\tau^2 \\neq 0$ we need more studies (even if the total sample size is the same).\n\n## Simulation-based power\n\nWith simulations we can fix or relax the previous assumptions. For example, let's compute the power for an EE model:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 10\nes <- 0.3\ntau2 <- 0\nn1 <- n2 <- rep(25, k)\nnsim <- 1000 # more is better\npval <- rep(NA, nsim)\n\nfor(i in 1:nsim){\n  dat <- sim_studies(k, es, tau2, n1, n2)\n  fit <- rma(yi, vi, data = dat, method = \"EE\")\n  pval[i] <- fit$pval\n}\n\nmean(pval <= 0.05)\n#> [1] 0.924\n```\n:::\n\n\n\n\nThe value is similar to the analytical simulation. But we can improve it e.g. generating heterogeneous sample sizes.\n\n## Simulation-based power curve\n\nBy repeating the previous approach for a series of parameters we can easily draw a power curve:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- c(5, 10, 50, 100)\nes <- 0.1\ntau2 <- c(0, 0.05, 0.1, 0.2)\nnsim <- 1000\n\ngrid <- expand_grid(k, es, tau2)\npower <- rep(NA, nrow(grid))\n\nfor(i in 1:nrow(grid)){\n  pval <- rep(NA, nsim)\n  for(j in 1:nsim){\n    n <- rpois(grid$k[i], 40)\n    dat <- sim_studies(grid$k[i], grid$es[i], grid$tau2[i], n)\n    fit <- rma(yi, vi, data = dat)\n    pval[j] <- fit$pval\n  }\n  power[i] <- mean(pval <= 0.05)\n}\ngrid$power <- power\n```\n:::\n\n\n\n\n## Simulation-based power curve\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(grid, aes(x = factor(k), y = power, color = factor(tau2))) +\n  geom_point() +\n  geom_line(aes(group = factor(tau2))) +\n  labs(\n    y = \"Power\",\n    x = \"Number of studies (k)\",\n    color = latex2exp::TeX(\"$\\\\tau^2$\")\n  )\n```\n\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n\n## Power analysis for meta-regression\n\nThe power for a meta-regression can be easily computed by simulating the moderator effect. For example, let's simulate the effect of a binary predictor $x$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- seq(10, 100, 10)\nb0 <- 0.2 # average of group 1\nb1 <- 0.1 # difference between group 1 and 2\ntau2r <- 0.2 # residual tau2\nnsim <- 1000\npower <- rep(NA, length(k))\n\nfor(i in 1:length(k)){\n  es <- b0 + b1 * rep(0:1, each = k[i]/2)\n  pval <- rep(NA, nsim)\n  for(j in 1:nsim){\n    n <- round(runif(k[i], 10, 100))\n    dat <- sim_studies(k[i], es, tau2r, n)\n    fit <- rma(yi, vi, data = dat)\n    pval[j] <- fit$pval\n  }\n  power[i] <- mean(pval <= 0.05)\n}\n```\n:::\n\n\n\n\n## Power analysis for meta-regression\n\nThen we can plot the results:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npower <- data.frame(k = k, power = power)\nggplot(power, aes(x = k, y = power)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n\n# Multilab Studies {.section}\n\n## Multilab studies\n\nMultilab studies can be seen as a meta-analysis that is planned (a *prospective* meta-analysis) compared to standard *retrospective* meta-analysis.\n\nThe statistical approach is (roughly) the same with the difference that we have control both on $k$ (the number of experimental units) and $n$ the sample size within each unit.\n\nIn multilab studies we have also the raw data (i.e., participant-level data) thus we can do more complex multilevel modeling.\n\n## Meta-analysis as multilevel model\n\nAssuming that we have $k$ studies with raw data available there is no need to aggregate, calculate the effect size and variances and then use an EE or RE model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 50\nes <- 0.4\ntau2 <- 0.1\nn <- round(runif(k, 10, 100))\ndat <- vector(mode = \"list\", k)\nthetai <- rnorm(k, 0, sqrt(tau2))\n\nfor(i in 1:k){\n  g1 <- rnorm(n[i], 0, 1)\n  g2 <- rnorm(n[i], es + thetai[i], 1)\n  d <- data.frame(id = 1:(n[i]*2), unit = i, y = c(g1, g2), group = rep(c(0, 1), each = n[i]))\n  dat[[i]] <- d\n}\n\ndat <- do.call(rbind, dat)\nht(dat)\n#>      id unit          y group\n#> 1     1    1 -0.4359058     0\n#> 2     2    1  1.5799758     0\n#> 3     3    1  0.7015053     0\n#> 4     4    1  0.1376932     0\n#> 5     5    1 -1.7018263     0\n#> 5257 63   50  1.0430772     1\n#> 5258 64   50 -0.7878105     1\n#> 5259 65   50 -0.4609004     1\n#> 5260 66   50  1.5076526     1\n#> 5261 67   50  0.9963187     1\n#> 5262 68   50 -0.2518844     1\n```\n:::\n\n\n\n\n## Meta-analysis as multilevel model\n\nThis is a simple **multilevel model** (pupils within classrooms or trials within participants). We can fit the model using `lme4::lmer()`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(lme4)\nfit_lme <- lmer(y ~ group + (1|unit), data = dat)\nsummary(fit_lme)\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: y ~ group + (1 | unit)\n#>    Data: dat\n#> \n#> REML criterion at convergence: 15282.1\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.6473 -0.6764  0.0169  0.6663  3.5499 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  unit     (Intercept) 0.03605  0.1899  \n#>  Residual             1.05226  1.0258  \n#> Number of obs: 5262, groups:  unit, 50\n#> \n#> Fixed effects:\n#>              Estimate Std. Error t value\n#> (Intercept) 0.0006038  0.0341201   0.018\n#> group       0.3992543  0.0282823  14.117\n#> \n#> Correlation of Fixed Effects:\n#>       (Intr)\n#> group -0.414\n```\n:::\n\n\n\n\n## Meta-analysis as multilevel model\n\nLet's do the same as a meta-analysis. Firstly we compute the effect sizes for each unit:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatagg <- dat |> \n  group_by(unit, group) |> \n  summarise(m = mean(y),\n            sd = sd(y),\n            n = n()) |> \n  pivot_wider(names_from = group, values_from = c(m, sd, n), names_sep = \"\")\n\ndatagg <- escalc(\"MD\", m1i = m1, m2i = m0, sd1i = sd1, sd2i = sd0, n1i = n1, n2i = n0, data = datagg)\n```\n:::\n\n\n\n\n## Meta-analysis as multilevel model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nht(datagg)\n#> \n#>    unit           m0          m1       sd0       sd1 n0 n1      yi     vi \n#> 1     1 -0.067197098  0.40582612 0.9304467 1.0147085 42 42  0.4730 0.0451 \n#> 2     2 -0.091403369 -0.39447251 1.1524521 1.0145999 22 22 -0.3031 0.1072 \n#> 3     3 -0.003808028  0.16236294 0.9414775 0.7959424 20 20  0.1662 0.0760 \n#> 4     4  0.074297764  0.14426023 0.9656452 1.0081848 97 97  0.0700 0.0201 \n#> 5     5 -0.068813812  0.26518665 0.9798539 1.1045266 26 26  0.3340 0.0838 \n#> 45   45 -0.030328846  0.18885527 0.9490194 1.0119891 96 96  0.2192 0.0200 \n#> 46   46  0.076619304  0.24552547 1.0223705 0.8317036 37 37  0.1689 0.0469 \n#> 47   47 -0.123421255  0.80612492 0.8958121 1.2418829 41 41  0.9295 0.0572 \n#> 48   48 -0.012340409 -0.27712644 1.0127637 0.9681938 85 85 -0.2648 0.0231 \n#> 49   49  0.351899444 -0.08890696 0.9262625 1.0037785 46 46 -0.4408 0.0406 \n#> 50   50 -0.231551420  0.21903620 1.1221262 0.9182845 34 34  0.4506 0.0618\n```\n:::\n\n\n\n\n## Meta-analysis as multilevel model\n\nThen we can fit the model:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_rma <- rma(yi, vi, data = datagg)\nfit_rma\n#> \n#> Random-Effects Model (k = 50; tau^2 estimator: REML)\n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.1623 (SE = 0.0419)\n#> tau (square root of estimated tau^2 value):      0.4028\n#> I^2 (total heterogeneity / total variability):   81.04%\n#> H^2 (total variability / sampling variability):  5.28\n#> \n#> Test for Heterogeneity:\n#> Q(df = 49) = 260.7313, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.3711  0.0647  5.7356  <.0001  0.2443  0.4979  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n## Meta-analysis as multilevel modeling\n\nActually the results are very similar where the standard deviation of the intercepts of the `lme4` model is $\\approx \\tau$ and the `group` effect is the intercept of the `rma` model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\n  b = c(fixef(fit_lme)[2], fit_rma$b),\n  se = c(summary(fit_lme)$coefficients[2, 2], fit_rma$se),\n  tau2 = c(as.numeric(VarCorr(fit_lme)[[1]]), fit_rma$tau2),\n  model = c(\"lme4\", \"metafor\")\n)\n#>               b         se       tau2   model\n#> group 0.3992543 0.02828232 0.03604771    lme4\n#>       0.3710964 0.06470033 0.16226242 metafor\n```\n:::\n\n\n\n\nActually the two model are not exactly the same, especially when using only the aggregated data. See https://www.metafor-project.org/doku.php/tips:rma_vs_lm_lme_lmer.\n\n## Meta-analysis as multilevel modeling\n\nTo note, aggregating data and then computing a standard (non-weighted) model (sometimes this is done with trial-level data) is wrong and should be avoided. Using meta-analysis is clear that aggregating without taking into account the cluster (e.g., study or subject) precision is misleading.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataggl <- datagg |> \n  select(unit, m0, m1) |>\n  pivot_longer(c(m0, m1), values_to = \"y\", names_to = \"group\")\n\nsummary(lmer(y ~ group + (1|unit), data = dataggl))\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: y ~ group + (1 | unit)\n#>    Data: dataggl\n#> \n#> REML criterion at convergence: 63.5\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -2.5460 -0.5662 -0.1141  0.4252  4.0163 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  unit     (Intercept) 0.0000   0.0000  \n#>  Residual             0.1034   0.3215  \n#> Number of obs: 100, groups:  unit, 50\n#> \n#> Fixed effects:\n#>             Estimate Std. Error t value\n#> (Intercept) 0.009959   0.045473   0.219\n#> groupm1     0.365573   0.064309   5.685\n#> \n#> Correlation of Fixed Effects:\n#>         (Intr)\n#> groupm1 -0.707\n#> optimizer (nloptwrap) convergence code: 0 (OK)\n#> boundary (singular) fit: see help('isSingular')\n```\n:::\n\n\n\n\n## Mulitlab sample size vs unit\n\nWhen planning a multilab study there is an important decision between **increasing the sample size within each unit** (more effort for each lab) or **recruiting more units** with less participants per unit (more effort for the organization).\n\nWe could have the situation where the number of units $k$ is fixed and we can only increase the sample size.\n\nWe can also simulate scenarios where some units collect all data while others did not complete the data collection.\n\n## Fixed $k$, increasing $n$\n\nLet's assume that the maximum number of labs is $10$. How many participants are required assuming a certain amount of heterogeneity?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nes <- 0.2\nk <- 10\nn1 <- n2 <- seq(10, 500, 10)\ntau2 <- c(0.01, 0.05, 0.1, 0.2)\nsim <- expand_grid(k, es, tau2, n1)\nsim$n2 <- sim$n1\nsim$vt <- with(sim, 1/n1 + 1/n2)\nsim$I2 <- round(with(sim, tau2 / (tau2 + vt)) * 100, 3)\nsim$power <- power_meta(sim$es, sim$k, sim$tau2, sim$n1, sim$n2)\n\nht(sim)\n#> # A tibble: 11 Ã— 8\n#>        k    es  tau2    n1    n2      vt    I2 power\n#>    <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>\n#>  1    10   0.2  0.01    10    10 0.2      4.76 0.281\n#>  2    10   0.2  0.01    20    20 0.1      9.09 0.479\n#>  3    10   0.2  0.01    30    30 0.0667  13.0  0.627\n#>  4    10   0.2  0.01    40    40 0.05    16.7  0.733\n#>  5    10   0.2  0.01    50    50 0.04    20    0.807\n#>  6    10   0.2  0.2    450   450 0.00444 97.8  0.288\n#>  7    10   0.2  0.2    460   460 0.00435 97.9  0.288\n#>  8    10   0.2  0.2    470   470 0.00426 97.9  0.288\n#>  9    10   0.2  0.2    480   480 0.00417 98.0  0.288\n#> 10    10   0.2  0.2    490   490 0.00408 98    0.288\n#> 11    10   0.2  0.2    500   500 0.004   98.0  0.288\n```\n:::\n\n\n\n\n## Fixed $k$, increasing $n$\n\nWith a fixed $k$, we could reach a plateau even increasing $n$. This depends also on $\\mu_{\\theta}$ and $\\tau^2$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sim, aes(x = n1, y = power, color = factor(tau2))) +\n  geom_line() +\n  labs(\n    y = \"Power\",\n    x = \"N per group\",\n    color = latex2exp::TeX(\"$\\\\tau^2$\")\n  )\n```\n\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n\n\n## Multilab replication studies\n\nA special type of multilab studies are the replication projects. There are some paper discussing how to view replication studies as meta-analyses and how to plan them.\n\n- @Hedges2021-of\n- @Schauer2022-mj\n- @Schauer2020-tw\n- @Schauer2021-ja\n- @Schauer2023-yn\n- @Hedges2019-ry\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}